{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson Processes\n",
    "\n",
    "The preceding chapter discussed Poisson GLMs for neural spike counts. Changing notation slightly, we had,\n",
    "\n",
    "$$\n",
    "y_{i} \\sim \\mathrm{Po}(\\lambda_i \\Delta )\n",
    "$$\n",
    "\n",
    "where $y_i \\in \\mathbb{N}_0$ denotes the number of spikes in the $i$-th time bin, $\\lambda_i \\in \\mathbb{R}_+$ is the firing rate (in spikes/second), and $\\Delta \\in \\mathbb{R}_+$ is the bin width (in seconds). \n",
    "\n",
    "What happens if we take the bin width to zero? Then all but a finite number of bins will have zero counts, and we can instead represent the data as an unordered set of **spike times** $\\{t_n\\}_{n=1}^N \\subset [0, T)$. Instead of a vector of Poisson-distributed spike counts, we can model the spike times as a realization of a **Poisson process**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "A Poisson process is a **stochastic process** that generates sets of points, like the set of spike times above. It is defined by an intensity function or **firing rate**, $\\lambda(t): [0, T) \\mapsto \\mathbb{R}_+$. \n",
    "\n",
    "Let $N(\\mathcal{A}) = |\\{n: t_n \\in \\mathcal{A}\\}|$ denote the number of points in the set $\\mathcal{A} \\subseteq [0,T)$. For example, an interval $\\mathcal{A} = [a_0, a_1)$. Let $N(t)$ be shorthand for $N([0,t))$ that denotes the **counting function**, which specifies the number of points up to time $t$. Assume $N(0) = 0$.  \n",
    "\n",
    "A Poisson process has two defining properties:\n",
    "\n",
    "1. The number of points in an interval is **Poisson distributed** with expectation given by the integrated intensity function, \n",
    "\n",
    "    $$N(\\mathcal{A}) \\sim \\mathrm{Po}\\left( \\int_{\\mathcal{A}} \\lambda(t) \\, \\mathrm{d}t\\right).$$\n",
    "\n",
    "2. The number of points in $\\mathcal{A}$ is **independent** of the number of points in a disjoint interval $\\mathcal{A}'$,\n",
    "\n",
    "    $$N(\\mathcal{A}) \\perp \\!\\!\\! \\perp N(\\mathcal{A}')$$\n",
    "\n",
    "    if $\\mathcal{A} \\cap \\mathcal{A}' = \\varnothing$.\n",
    "\n",
    "A **homogeneous** Poisson process has a constant intensity function, $\\lambda(t) \\equiv \\lambda$. Otherwise, the process is called **inhomogeneous**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling a Poisson process\n",
    "\n",
    "The two properties above imply a method for sampling a Poisson process. First, sample the total number of points from its Poisson distribution,\n",
    "\n",
    "$$\n",
    "N \\sim \\mathrm{Po} \\left(\\int_0^T \\lambda(t) \\, \\mathrm{d}t \\right).\n",
    "$$\n",
    "\n",
    "Then sample the locations of the points by independently sampling from the normalized intensity,\n",
    "\n",
    "$$\n",
    "t_n \\overset{\\text{iid}}{\\sim} \\frac{\\lambda(t)}{\\int_0^T \\lambda(t) \\, \\mathrm{d} t},\n",
    "$$\n",
    "\n",
    "to obtain the unordered set $\\{t_n\\}_{n=1}^N$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interval distribution\n",
    "\n",
    "For a homogeneous Poisson process, the expected number of points is $\\mathbb{E}[N] = \\lambda T$, and the locations are independently sampled according to a uniform distribution over $[0, T)$.  The **intervals** between points, $\\delta_n = t_{(n)} - t_{(n-1)}$, where $t_{(n)}$ is the $n$-th spike time in the _ordered_ set, are **exponential random variables**,\n",
    "\n",
    "$$\\delta_{n} \\overset{\\text{iid}}{\\sim} \\mathrm{Exp}(\\lambda).$$\n",
    "\n",
    "This suggests another means of sampling a homogeneous Poisson process: sample intervals independently until the total elapsed time exceeds $T$. \n",
    "\n",
    "As a sanity check, note that the expected value of the intervals is $\\mathbb{E}[\\delta_n] = \\frac{1}{\\lambda}$, so we should get about $\\lambda T$ points for large $T$.\n",
    "\n",
    "The exponential distribution is **memoryless**: the distribution of time until the next point arrives is independent of how much time has elapsed since the last point. More formally,\n",
    "\n",
    "$$\n",
    "\\Pr(\\delta_n > a + b \\mid \\delta_n > a) = \\Pr(\\delta_n > b).\n",
    "$$\n",
    "\n",
    ":::{warning}\n",
    "\n",
    "The memoryless property of Poisson processes should give you pause. Don't neurons have a refractory period, which sets a lower bound on the interval between spikes? Incorporating these types of dependencies will require us to move beyond Poisson processes, as discussed below.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson process likelihood\n",
    "\n",
    "From the two-step sampling procedure above, we can derive the likelihood of a set of points under a Poisson process,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\{t_n\\}_{n=1}^N; \\lambda)\n",
    "&= \\exp\\left\\{-\\int_0^T \\lambda(t) \\, \\mathrm{d}t \\right\\} \\prod_{n=1}^N \\lambda(t_n)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    ":::{admonition} Derivation\n",
    "\n",
    "The sampling procedure has two steps: sample the number of points, then sample their location. Thus,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\{t_n\\}_{n=1}^N; \\lambda)\n",
    "&= p(N) \\times \\left[ \\prod_{n=1}^N p(t_n) \\right] \\times N!\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where does that lagging $N!$ come from? The likelihood is defined over **unordered sets of points** $\\{t_n\\}_{n=1}^N$. The product over $n$ implicitly assumes an ordering. Since we could obtain the same output from any of the $N!$ permutations, the likelihood needs to be multiplied by a factor of $N!$.\n",
    "\n",
    "Now expand the equation above and simplify,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\{t_n\\}_{n=1}^N; \\lambda)\n",
    "&= \\mathrm{Po}\\left(N; \\int_0^T \\lambda(t) \\, \\mathrm{d}t \\right) \\times \\left[ \\prod_{n=1}^N \\frac{\\lambda(t)}{\\int_0^T \\lambda(t) \\, \\mathrm{d} t} \\right]  \\times N! \\\\\n",
    "&= \\frac{1}{N!} \\left(\\int_0^T \\lambda(t) \\, \\mathrm{d}t \\right)^N \\exp\\left\\{-\\int_0^T \\lambda(t) \\, \\mathrm{d}t \\right\\} \\times \\left[ \\prod_{n=1}^N \\frac{\\lambda(t)}{\\int_0^T \\lambda(t) \\, \\mathrm{d} t} \\right]  \\times N! \\\\\n",
    "&= \\exp\\left\\{-\\int_0^T \\lambda(t) \\, \\mathrm{d}t \\right\\} \\prod_{n=1}^N \\lambda(t_n)\n",
    "\\end{aligned}\n",
    "$$\n",
    ":::\n",
    "\n",
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "\n",
    "Derive the likelihood of the set of intervals $\\{\\delta_n\\}_{n=1}^N$ corresponding to the points $\\{t_n\\}_{n=1}^N$. You should get the same form as above.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation\n",
    "\n",
    "In practice, we often want to estimate the intensity function from data. For example, we may assume the intensity function has a parametric form, $\\lambda(t; \\theta)$, with parameters $\\theta$. We could estimate the parameters by maximum likelihood estimation, using the Poisson process likelihood above. Note that the likelihood is a concave function of $\\lambda$, making it amenable to optimization.\n",
    "\n",
    "The challenge is the integrated intensity in the exponent. For many models, there is no closed-form solution for the integral. For temporal point processes like the ones considered in this chapter, this is simply a one-dimensional integral. We can approximate it using numerical quadrature rules, as long as we can evaluate $\\lambda(t; \\theta)$ for any $t$. \n",
    "\n",
    "Some classes of models do afford closed-form integration. For example, consider a model of the intensity as a weighted sum of basis functions,\n",
    "\n",
    "$$\n",
    "\\lambda(t; \\theta) = \\sum_{b=1}^B w_b \\phi_b(t)\n",
    "$$\n",
    "\n",
    "where $\\theta = (w_1, \\ldots, w_B)^\\top \\in \\mathbb{R}_+^B$ are non-negative weights and $\\phi_b: [0,T) \\mapsto \\mathbb{R}_+$ are non-negative **basis functions**. Then,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\int \\lambda(t; \\theta) \\, \\mathrm{d} t \n",
    "&= \\sum_{b=1}^B w_b C_b.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $C_b = \\int \\phi_b(t) \\, \\mathrm{d} t$. \n",
    "\n",
    "Since we get to choose the basis functions, we can simply choose functions that are easy to integrate, like continuous univariate probability densities. In the special case of **rectangular** basis functions of width $\\Delta$,\n",
    "\n",
    "$$\\phi_b(t) = \\mathbb{I}\\big[t \\in [b\\Delta, (b+1)\\Delta) \\big],$$\n",
    "\n",
    "we obtain a piecewise-constant model for the intensity function. This brings us back to the discrete-time model for spike counts from the last chapter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit of the discrete-time model\n",
    "\n",
    "We can arrive at a Poisson process by taking a limit of the discrete-time model for spike counts. Intuitively, think of discrete time model as a Poisson process with **piecewise constant intensity**,\n",
    "\n",
    "$$\n",
    "\\lambda(t) = \\lambda_{i(t)},\n",
    "$$\n",
    "\n",
    "where $i(t) = \\lfloor \\frac{t}{\\Delta} \\rfloor$ is the index of the time bin corresponding to continuous time $t$, and $\\lambda_{i(t)}$ is the constant intensity in that bin. The discrete-time model ignored the precise timing of spikes within each bin and simply focused on the spike counts. \n",
    "\n",
    "Sampling a Poisson process with piecewise constant intensity is straightforward: each bin is an independent, homogenous Poisson process with its own intensity. More precisely, sample\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_i &\\sim \\mathrm{Po}(\\lambda_{i(t)} \\Delta) & \\text{for } i&=1,\\ldots, \\lfloor \\tfrac{T}{\\Delta} \\rfloor \\\\\n",
    "t_{i,n} &\\overset{\\text{iid}}{\\sim} \\mathrm{Unif}([i \\Delta, (i+1)\\Delta)) & \\text{for } n&=1,\\ldots,y_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then return the union of all spikes, $\\cup_{i} \\{t_{i,n}\\}_{n=1}^{y_i}$. The total number of spikes is $N=\\sum_i y_i$.\n",
    "\n",
    "Now let's derive the likelihood,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\{t_n\\}_{n=1}^N; \\lambda)\n",
    "&= \\prod_{i=1}^{\\lfloor \\frac{T}{\\Delta} \\rfloor} p(\\{t_{i,n}\\}_{n=1}^{y_i}) \\\\\n",
    "&= \\prod_{i=1}^{\\lfloor \\frac{T}{\\Delta} \\rfloor} \\mathrm{Po}(y_i; \\lambda_{i(t)} \\Delta) \\left[ \\prod_{n=1}^{y_i} \\mathrm{Unif}(t_{i,n}; [i\\Delta, (i+1)\\Delta)) \\right] y_i! \\\\\n",
    "&= \\prod_{i=1}^{\\lfloor \\frac{T}{\\Delta} \\rfloor} \\frac{1}{y_i!} (\\lambda_{i(t)} \\Delta)^{y_i} e^{-\\lambda_{i(t)} \\Delta} \\left(\\tfrac{1}{\\Delta}\\right)^{y_i} y_i! \\\\\n",
    "&= \\prod_{i=1}^{\\lfloor \\frac{T}{\\Delta} \\rfloor} \\lambda_{i(t)}^{y_i} e^{-\\lambda_{i(t)} \\Delta} \\\\\n",
    "&= \\exp \\bigg\\{-\\sum_{i=1}^{\\lfloor \\frac{T}{\\Delta} \\rfloor} \\lambda_{i(t)} \\Delta \\bigg\\} \\prod_{i=1}^{\\lfloor \\frac{T}{\\Delta} \\rfloor} \\lambda_{i(t)}^{y_i} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now consider the limit as the bin width $\\Delta$ goes to zero. The sum in the exponent is a Riemann sum, and its limit is the integral of the intensity function. Moreover, as the bin width goes to zero, so does the probability of having more than one spike in a bin. In other words, $y_i$ is either zero or one. Since $\\lambda^0 = 1$, we can write the likelihood as a product of instantaneous intensities at the time of each spike,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\{t_n\\}_{n=1}^N; \\lambda)\n",
    "&= \\lim_{\\Delta \\to 0} \\; \\exp \\bigg\\{-\\sum_{i=1}^{\\lfloor \\frac{T}{\\Delta} \\rfloor} \\lambda_{i(t)} \\Delta \\bigg\\} \\prod_{i=1}^{\\lfloor \\frac{T}{\\Delta} \\rfloor} \\lambda_{i(t)}^{y_i} \\\\\n",
    "&= \\exp \\bigg\\{ -\\int_0^T \\lambda(t) \\, \\mathrm{d} t \\bigg\\} \\prod_{n=1}^N \\lambda(t_n)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    ":::{admonition} Simplicity and orderliness\n",
    ":class: dropdown\n",
    "\n",
    "A point process is **simple** if the probability of two spikes at the same time is zero. \n",
    "\n",
    "A point process is said to be **orderly** or **regular** if\n",
    "\n",
    "$$\n",
    "\\Pr(N([t,t+\\Delta)) > 1) = o(\\Delta)\n",
    "$$\n",
    "\n",
    "where $o(\\Delta)$ is _little-oh notation_ that means $\\lim_{\\Delta \\to 0} o(\\Delta)/\\Delta = 0$. Orderliness implies simplicity.\n",
    "\n",
    "As long as the intensity $\\lambda$ has no atoms, a Poisson process is orderly and simple. We used this property when we claimed that $y_i$ is either zero or one in the limit as $\\Delta$ goes to zero. To prove it more formally, note that in the piecewise constant model,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Pr(y_i = 0) &= e^{-\\lambda_{i(t)} \\Delta} \\\\\n",
    "\\Pr(y_i = 1) &= \\lambda_{i(t)} \\Delta e^{-\\lambda_{i(t)} \\Delta} \\\\\n",
    "\\Pr(y_i = 2) &= \\frac{(\\lambda_{i(t)} \\Delta)^2}{2} e^{-\\lambda_{i(t)} \\Delta}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For small $\\Delta$, we can use a Taylor approximation $e^{\\lambda_{i(t)} \\Delta} = 1 + \\lambda_{i(t)} \\Delta + o(\\Delta)$ where $o(\\Delta)$ is _little-oh notation_ that means $\\lim_{\\Delta \\to 0} o(\\Delta)/\\Delta = 0$. It captures the second and higher order terms in the Taylor approximation.  In this limit,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Pr(y_t = 0) &= 1 -\\lambda_{i(t)} \\Delta + o(\\Delta) \\\\\n",
    "\\Pr(y_t = 1) &= \\lambda_{i(t)} \\Delta + o(\\Delta) \\\\\n",
    "\\Pr(y_t \\geq 2) &= o(\\Delta) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The last line says that the probability of seeing more than one spike goes to zero faster than $\\Delta$ does, which justifies our claim that $y_i$ is binary in the limit of small bin sizes.\n",
    "\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renewal processes\n",
    "\n",
    "Let's return to the interval distribution above. As we saw, under a Poisson process the intervals are exponentially distributed. This isn't a very realistic model for neural spike trains because neurons have refractory periods. \n",
    "\n",
    "**Renewal processes** are a more general class of models that explicitly model the interval distribution. Again, let $\\{\\delta_n\\}_{n=1}^N$ denote the intervals between the points $\\{t_n\\}_{n=1}^N$. In a renewal process,\n",
    "\n",
    "$$\n",
    "\\delta_n \\overset{\\text{iid}}{\\sim} p(\\delta),\n",
    "$$\n",
    "\n",
    "where $p(\\delta)$ is an interval distribution with support on $\\mathbb{R}_+$. When $p(\\delta)$ is an exponential distribution we recover the Poisson process. To model neural spike trains, we could substitute a more general form like a gamma distribution or an [**inverse Gaussian distribution**](https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution). \n",
    "\n",
    "The inverse Gaussian distribution is appealing due to its connection to Brownian motion: it is the distribution of the _first passage time_ at which a particle undergoing Brownian motion reaches a fixed level. A very simple [**stochastic integrate-and-fire model**](https://en.wikipedia.org/wiki/Biological_neuron_model#Stochastic_models_of_membrane_voltage_and_spike_timing) of a neuron treats the spike times as the first passage times when the membrane potential reaches a firing threshold. If the membrane potential is modeled as Brownian motion with positive drift, then the inter-spike intervals are inverse Gaussian distributed.\n",
    "\n",
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "\n",
    "A defining property of a Poisson process is that the numbers of events in disjoint intervals are independent. Do renewal processes violate this assumption?\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional intensity functions\n",
    "\n",
    "So far we have treated the intensity as simply a function of time. Of course, the intensity could also be modeled as a function of external stimuli or covariates, $\\mathbf{x}(t)$. For example, the stimulus could be incorporated into the basis function model described above.\n",
    "\n",
    "To incorporate direct interactions between the points themselves, we need to generalize our model via a **conditional intensity function**,\n",
    "\n",
    "$$\n",
    "\\lambda(t \\mid \\mathcal{H}_t) \n",
    "$$\n",
    "\n",
    "where $\\mathcal{H}_t$ represents the **history** of points up to time $t$. In the language of stochastic processes, $\\mathcal{H}_t$ is a [**filtration**](https://en.wikipedia.org/wiki/Filtration_(probability_theory)).\n",
    "\n",
    "Given the history, the process behaves like a Poisson process. However, as soon as a new point occurs, the intensity function changes to account for it. Intuitively, this allows us to construct continuous time analogs of discrete time autoregressive models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hawkes processes\n",
    "\n",
    "Perhaps the canonical example of a point process with conditional dependencies is the Hawkes process {cite}`hawkes1971spectra`. It is defined by its conditional intensity function,\n",
    "\n",
    "$$\n",
    "\\lambda(t \\mid \\mathcal{H}_t) = \\lambda_0 + \\sum_{t_n \\in \\mathcal{H}_t} w \\, f(t - t_n),\n",
    "$$\n",
    "\n",
    "where $f : \\mathbb{R}_+ \\mapsto \\mathbb{R}_+$ is the **impulse response function** or, following the last chapter, the self-coupling filter. It specifies how past spikes affect the conditional intensity at time $t$. Without loss of generality, assume the impulse response is normalized so that $\\int_0^\\infty f(s) \\, \\mathrm{d}s = 1$. A common choice is is an exponential density with time-constant (inverse rate) $\\tau$,\n",
    "\n",
    "$$\n",
    "f(s) = \\mathrm{Exp}(s; \\tau^{-1}) = \\tau^{-1} e^{-s / \\tau}.\n",
    "$$\n",
    "\n",
    "This simple formulation of the model has two parameters, the baseline intensity $\\lambda_0 \\in \\mathbb{R}_+$ and the self-coupling weight $w \\in \\mathbb{R}_+$. \n",
    "\n",
    "Since $f$ is a positive function, a Hawkes process is **self-exciting**: past spikes can only increase the future firing rate. Self-excitation can easily become unstable, depending on the self-coupling weight. If $w \\geq 1$, the process becomes unstable because, intuitively, each spike induces more than one future spike in expectation. If $w < 1$, the stationary firing rate is,\n",
    "\n",
    "$$\n",
    "\\lambda_{\\infty} = \\frac{\\lambda_0}{1 - w}.\n",
    "$$\n",
    "\n",
    "When $w=0$, we recover the standard Poisson process with rate $\\lambda_0$, but as $w$ increases the self-excitation produces bursts of spikes and a larger stationary rate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Hawkes processes\n",
    "\n",
    "Now let's generalize the Hawkes process to multiple interacting point processes. Let $M$ denote the number of processes &mdash; e.g., the number of neurons in a multi-neuronal spike train recording &mdash; and let $\\lambda_m(t \\mid \\mathcal{H}_t)$ denote the conditional intensity function for the $m$-th process. Let $\\mathcal{H}_{t,m}$ denote the history of events on process $m$ and $\\mathcal{H}_t = \\{\\mathcal{H}_{t,m}\\}_{m=1}^M$ denote the combined history of all processes.\n",
    "\n",
    "The multivariate Hawkes process has conditional intensities,\n",
    "\n",
    "$$\n",
    "\\lambda_m(t \\mid \\mathcal{H}_t) = \\lambda_{0,m} + \\sum_{m'=1}^M \\sum_{t_n \\in \\mathcal{H}_{t,m'}} w_{m',m} \\, f(t - t_n),\n",
    "$$\n",
    "\n",
    "The weight $w_{m',m} \\geq 0$ specifies the influence that spikes on process $m'$ have on the future rate of process $m$. \n",
    "\n",
    "Let $\\mathbf{W} \\in \\mathbb{R}_+^{M \\times M}$ denote the matrix of weights. In the multivariate case, stability is determined by the eigenvalues of the weight matrix. The multivariate process is stable if the **spectral radius** of $\\mathbf{W}$ is less than one. Then, the stationary rates are,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\lambda}_\\infty = (\\mathbf{I} - \\mathbf{W})^{-1} \\boldsymbol{\\lambda}_0\n",
    "$$\n",
    "\n",
    ":::{admonition} Spectral radius and the Perron-Frobenius theorem\n",
    "\n",
    "The spectral radius of a matrix is the maximum absolute value of its eigenvalues. Since $\\mathbf{W}$ is a non-negative, real-valued matrix, the [**Perron-Frobenius theorem**](https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem) says that the maximum eigenvalue is real-valued and non-negative. \n",
    ":::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson superposition theorem\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Hawkes processes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
