

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Lab 7: Switching LDS &#8212; Machine Learning Methods for Neural Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'labs/07_slds';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lab 8: Sequential VAEs" href="08_lfads.html" />
    <link rel="prev" title="Lab 6: Autoregressive HMMs" href="06_arhmm.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    <p class="title logo__title">Machine Learning Methods for Neural Data Analysis</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Labs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="00_pytorch_primer.html">Lab 0: PyTorch Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_spike_sorting.html">Lab 1: Spike Sorting</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_calcium_imaging.html">Lab 2: Calcium Deconvolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_pose_tracking.html">Lab 3: Markerless Pose Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_glms.html">Lab 4: Generalized Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_decoding.html">Lab 5: Bayesian Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_arhmm.html">Lab 6: Autoregressive HMMs</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Lab 7: Switching LDS</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_lfads.html">Lab 8: Sequential VAEs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/02_probabilistic_modeling.html">Probabilistic Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/03_neurobio.html">Basic Neurobiology</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit I: Signal Extraction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/04_simple_spike_sorting.html">Simple Spike Sorting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/05_deconv_spike_sorting.html">Spike Sorting by Deconvolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/06_calcium_imaging.html">Demixing Calcium Imaging Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/07_pose_tracking.html">Markerless Pose Tracking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit II: Encoding &amp; Decoding</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/08_summary_stats.html">Summary Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/09_glm.html">Generalized Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/10_poisson_processes.html">Poisson Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/11_decoding.html">Decoding Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit III: Unsupervised Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/12_mixtures_em.html">Mixture Models and the EM Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/13_hmms.html">Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/poldrack_fmri_analysis.html">fMRI Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lectures/99_references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/slinderman/stats320/blob/winter2023/labs/07_slds.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/labs/07_slds.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lab 7: Switching LDS</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-0-build-the-generative-model">Part 0: Build the generative model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#make-a-linear-regression-distribution-object">Make a Linear Regression Distribution object</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#make-a-mixture-of-factor-analyzers-object">Make a mixture of factor analyzers object</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-data-from-the-generative-model">Sample data from the generative model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-coordinate-ascent-variational-inference-cavi">Part 1: Coordinate Ascent Variational Inference (CAVI)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1a-math-derive-the-expected-log-likelihood">Problem 1a [Math]: Derive the expected log likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1b-implement-the-discrete-state-update">Problem 1b: Implement the discrete state update</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1c-implement-the-continuous-state-update">Problem 1c: Implement the continuous state update</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1d-short-answer-intuition-for-the-continuous-updates">Problem 1d [Short Answer]: Intuition for the continuous updates</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1e-math-derive-the-evidence-lower-bound">Problem 1e [Math]: Derive the evidence lower bound</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1f-implement-the-elbo">Problem 1f: Implement the ELBO</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1g-math-derive-the-exact-marginal-likelihood">Problem 1g [Math]: Derive the exact marginal likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-the-exact-marginal-likelihood">Implement the exact marginal likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#run-cavi">Run CAVI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#re-examine-the-continuous-state-posterior-after-cavi">Re-examine the continuous state posterior after CAVI</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-variational-em-in-a-mixture-of-factor-analysis-models">Part 2: Variational EM in a mixture of factor analysis models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2a-compute-the-expected-sufficient-statistics">Problem 2a: Compute the expected sufficient statistics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2b-implement-the-m-step-for-the-parameters-of-p-z-mid-theta">Problem 2b: Implement the M-step for the parameters of <span class="math notranslate nohighlight">\(p(z \mid \Theta)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2c-implement-the-m-step-for-parameters-of-p-x-mid-z-theta">Problem 2c: Implement the M-step for parameters of <span class="math notranslate nohighlight">\(p(x \mid z, \Theta)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2d-implement-the-m-step-for-parameters-of-p-y-mid-x-theta">Problem 2d: Implement the M-step for parameters of <span class="math notranslate nohighlight">\(p(y \mid x, \Theta)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#put-it-all-together">Put it all together</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2e-short-answer-interpret-the-results">Problem 2e [Short Answer]: Interpret the results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2f-cross-validation">Problem 2f: Cross validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2g-short-answer-interpret-the-results">Problem 2g [Short answer]: Interpret the results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3-apply-it-to-real-data">Part 3: Apply it to real data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-the-data">Load the data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perform-pca">Perform PCA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-data">Plot the data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-pca-trajectories">Plot the PCA trajectories</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3a-short-answer-interpret-the-pca-trajectories">Problem 3a [Short Answer]: Interpret the PCA trajectories</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fit-the-mixture-of-factor-analyzers">Fit the mixture of factor analyzers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-the-overlap-between-the-given-and-inferred-discrete-states">Compute the overlap between the given and inferred discrete states</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-inferred-segmentation-and-the-given-state-labels">Plot the inferred segmentation and the given state labels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bonus-problem-3b-compare-and-contrast">Bonus: Problem 3b: Compare and contrast</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bonus-problem-3c-assessing-variability-across-initializations">Bonus: Problem 3c: Assessing variability across initializations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-4-switching-linear-dynamical-systems-slds">Part 4: Switching Linear Dynamical Systems (SLDS)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#submission-instructions">Submission Instructions</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="lab-7-switching-lds">
<h1>Lab 7: Switching LDS<a class="headerlink" href="#lab-7-switching-lds" title="Permalink to this heading">#</a></h1>
<p><strong>STATS320: Machine Learning Methods for Neural Data Analysis</strong></p>
<p><em>Stanford University. Winter, 2023.</em></p>
<hr class="docutils" />
<p><strong>Team Name:</strong> <em>Your team name here</em></p>
<p><strong>Team Members:</strong> <em>Names of everyone on your team here</em></p>
<p><em>Due: 11:59pm Thursday, March 9, 2023 via GradeScope</em></p>
<hr class="docutils" />
<p><img alt="" src="https://els-jbs-prod-cdn.jbs.elsevierhealth.com/cms/attachment/09c7213d-d3ef-4c84-8321-07ab84a365af/fx1.jpg" /></p>
<p>In this lab, we will implement a variational expectation maximization algorithm to fit a latent variable model with both discrete and continuous states. We’ll use a mean field approximation, which we’ll fit using coordinate ascent variational inference (CAVI). Then we’ll test it out on neural activity traces extracted from calcium imaging of the worm <em>C. elegans</em> by Kato et al (2015), in their paper on low dimensional dynamics of whole brain activity.</p>
<p>We won’t implement variational EM for full-blown hierachical, recurrent, switching linear dynamical systems (Linderman et al, 2019). Instead, we’ll work on a simpler model without time dependencies, which reduces to a mixture of factor analysis models. Once we’ve done so, you’ll understand how the main fitting algorithms underlying <a class="reference external" href="https://github.com/lindermanlab/ssm">SSM</a> work under the hood!</p>
<p><strong>References</strong></p>
<p>Kato, Saul, Harris S. Kaplan, Tina Schrödel, Susanne Skora, Theodore H. Lindsay, Eviatar Yemini, Shawn Lockery, and Manuel Zimmer. 2015. “Global Brain Dynamics Embed the Motor Command Sequence of Caenorhabditis Elegans.” Cell 163 (3): 656–69.</p>
<p>Linderman, Scott W., Annika L. A. Nichols, David M. Blei, Manuel Zimmer, and Liam Paninski. 2019. “Hierarchical Recurrent State Space Models Reveal Discrete and Continuous Dynamics of Neural Activity in C. Elegans.” bioRxiv.</p>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.interpolate</span> <span class="kn">import</span> <span class="n">interp1d</span>
<span class="kn">from</span> <span class="nn">scipy.io</span> <span class="kn">import</span> <span class="n">loadmat</span>
<span class="kn">from</span> <span class="nn">scipy.ndimage</span> <span class="kn">import</span> <span class="n">gaussian_filter1d</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">trange</span>

<span class="c1"># PyTorch </span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span><span class="p">,</span> <span class="n">MultivariateNormal</span><span class="p">,</span> <span class="n">Normal</span><span class="p">,</span> \
    <span class="n">LowRankMultivariateNormal</span><span class="p">,</span> <span class="n">kl_divergence</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>

<span class="c1"># Helper function to convert between numpy arrays and tensors</span>
<span class="n">to_t</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">array</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">from_t</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Helper Functions (run this cell!)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">onp</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">LinearSegmentedColormap</span>

<span class="n">kato_files</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;TS20140715e_lite-1_punc-31_NLS3_2eggs_56um_1mMTet_basal_1080s.mat&quot;</span><span class="p">,</span>
              <span class="s2">&quot;TS20140715f_lite-1_punc-31_NLS3_3eggs_56um_1mMTet_basal_1080s.mat&quot;</span><span class="p">,</span>
              <span class="s2">&quot;TS20140905c_lite-1_punc-31_NLS3_AVHJ_0eggs_1mMTet_basal_1080s.mat&quot;</span><span class="p">,</span>
              <span class="s2">&quot;TS20140926d_lite-1_punc-31_NLS3_RIV_2eggs_1mMTet_basal_1080s.mat&quot;</span><span class="p">,</span>
              <span class="s2">&quot;TS20141221b_THK178_lite-1_punc-31_NLS3_6eggs_1mMTet_basal_1080s.mat&quot;</span><span class="p">]</span>

<span class="n">kato_dir</span> <span class="o">=</span> <span class="s1">&#39;.&#39;</span>

<span class="c1"># Set notebook plotting defaults</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;notebook&quot;</span><span class="p">)</span>

<span class="c1"># initialize a color palette for plotting</span>
<span class="n">palette</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">xkcd_palette</span><span class="p">([</span><span class="s2">&quot;light blue&quot;</span><span class="p">,</span>   <span class="c1"># forward</span>
                            <span class="s2">&quot;navy&quot;</span><span class="p">,</span>         <span class="c1"># slow</span>
                            <span class="s2">&quot;orange&quot;</span><span class="p">,</span>       <span class="c1"># dorsal turn</span>
                            <span class="s2">&quot;yellow&quot;</span><span class="p">,</span>       <span class="c1"># ventral turn</span>
                            <span class="s2">&quot;red&quot;</span><span class="p">,</span>          <span class="c1"># reversal 1</span>
                            <span class="s2">&quot;pink&quot;</span><span class="p">,</span>         <span class="c1"># reversal 2</span>
                            <span class="s2">&quot;green&quot;</span><span class="p">,</span>        <span class="c1"># sustained reversal</span>
                            <span class="s2">&quot;greyish&quot;</span><span class="p">])</span>     <span class="c1"># no state</span>

<span class="k">def</span> <span class="nf">load_kato_labels</span><span class="p">():</span>
    <span class="n">zimmer_state_labels</span> <span class="o">=</span> \
        <span class="n">loadmat</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">kato_dir</span><span class="p">,</span>
            <span class="s2">&quot;sevenStateColoring.mat&quot;</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">zimmer_state_labels</span>

<span class="k">def</span> <span class="nf">load_kato_key</span><span class="p">():</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">load_kato_labels</span><span class="p">()</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;sevenStateColoring&quot;</span><span class="p">][</span><span class="s2">&quot;key&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">key</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">k</span><span class="p">)[</span><span class="mi">2</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">key</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">key</span>


<span class="k">def</span> <span class="nf">_get_neuron_names</span><span class="p">(</span><span class="n">neuron_ids_1</span><span class="p">,</span> <span class="n">neuron_ids_2</span><span class="p">,</span> <span class="n">worm_name</span><span class="p">):</span>
    <span class="c1"># Remove the neurons that are not uniquely identified</span>
    <span class="k">def</span> <span class="nf">check_label</span><span class="p">(</span><span class="n">neuron_name</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">neuron_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">neuron_name</span> <span class="o">==</span> <span class="s2">&quot;---&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="n">neuron_index</span> <span class="o">=</span> <span class="n">onp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">neuron_ids_1</span> <span class="o">==</span> <span class="n">neuron_name</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">neuron_index</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">neuron_ids_2</span><span class="p">[</span><span class="n">neuron_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># Make sure it doesn&#39;t show up in the second neuron list</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">onp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">neuron_ids_2</span> <span class="o">==</span> <span class="n">neuron_name</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="k">return</span> <span class="kc">True</span>

    <span class="n">final_neuron_names</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">neuron_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">neuron_ids_1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">check_label</span><span class="p">(</span><span class="n">neuron_name</span><span class="p">):</span>
            <span class="n">final_neuron_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">neuron_name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">final_neuron_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_neuron</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">worm_name</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">final_neuron_names</span>


<span class="k">def</span> <span class="nf">load_kato</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">sample_rate</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;unnamed&quot;</span><span class="p">):</span>
    <span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">kato_dir</span><span class="p">,</span> <span class="n">kato_files</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
    <span class="n">zimmer_data</span> <span class="o">=</span> <span class="n">loadmat</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>

    <span class="c1"># Get the neuron names</span>
    <span class="n">neuron_ids</span> <span class="o">=</span> <span class="n">zimmer_data</span><span class="p">[</span><span class="s2">&quot;wbData&quot;</span><span class="p">][</span><span class="s1">&#39;NeuronIds&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">neuron_ids_1</span> <span class="o">=</span> <span class="n">onp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
        <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="kc">None</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span>
                            <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]),</span>
            <span class="n">neuron_ids</span><span class="p">)))</span>

    <span class="n">neuron_ids_2</span> <span class="o">=</span> <span class="n">onp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
        <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span>
                            <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]),</span>
            <span class="n">neuron_ids</span><span class="p">)))</span>

    <span class="n">all_neuron_names</span> <span class="o">=</span> <span class="n">_get_neuron_names</span><span class="p">(</span><span class="n">neuron_ids_1</span><span class="p">,</span> <span class="n">neuron_ids_2</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="c1"># Get the calcium trace (corrected for bleaching)</span>
    <span class="n">t_smpl</span> <span class="o">=</span> <span class="n">onp</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">zimmer_data</span><span class="p">[</span><span class="s2">&quot;wbData&quot;</span><span class="p">][</span><span class="s1">&#39;tv&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">t_start</span> <span class="o">=</span> <span class="n">t_smpl</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">t_stop</span> <span class="o">=</span> <span class="n">t_smpl</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">tt</span> <span class="o">=</span> <span class="n">onp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">t_start</span><span class="p">,</span> <span class="n">t_stop</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="n">sample_rate</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">interp_data</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">):</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">interp1d</span><span class="p">(</span><span class="n">t_smpl</span><span class="p">,</span> <span class="n">xx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="n">kind</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">tt</span><span class="p">)</span>
        <span class="c1"># return np.interp(tt, t_smpl, xx, axis=0)</span>

    <span class="n">dff</span> <span class="o">=</span> <span class="n">interp_data</span><span class="p">(</span><span class="n">zimmer_data</span><span class="p">[</span><span class="s2">&quot;wbData&quot;</span><span class="p">][</span><span class="s1">&#39;deltaFOverF&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">dff_bc</span> <span class="o">=</span> <span class="n">interp_data</span><span class="p">(</span><span class="n">zimmer_data</span><span class="p">[</span><span class="s2">&quot;wbData&quot;</span><span class="p">][</span><span class="s1">&#39;deltaFOverF_bc&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">dff_deriv</span> <span class="o">=</span> <span class="n">interp_data</span><span class="p">(</span><span class="n">zimmer_data</span><span class="p">[</span><span class="s2">&quot;wbData&quot;</span><span class="p">][</span><span class="s1">&#39;deltaFOverF_deriv&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Kato et al smoothed the derivative.  Let&#39;s just work with the first differences</span>
    <span class="c1"># of the bleaching corrected and normalized dF/F</span>
    <span class="n">dff_bc_zscored</span> <span class="o">=</span> <span class="p">(</span><span class="n">dff_bc</span> <span class="o">-</span> <span class="n">dff_bc</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">dff_bc</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">dff_diff</span> <span class="o">=</span> <span class="n">onp</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">onp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">dff_bc_zscored</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span>
                                <span class="n">onp</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">dff_bc_zscored</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>

    <span class="c1"># Get the state sequence as labeled in Kato et al</span>
    <span class="c1"># Interpolate to get at new time points</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">load_kato_labels</span><span class="p">()</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="s2">&quot;sevenStateColoring&quot;</span><span class="p">][</span><span class="s2">&quot;dataset&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">][</span><span class="s1">&#39;stateTimeSeries&#39;</span><span class="p">]</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">interp_data</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    
    <span class="c1"># Only keep the neurons with names</span>
    <span class="n">has_name</span> <span class="o">=</span> <span class="n">onp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="ow">not</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;unnamed&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">all_neuron_names</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">dff_bc</span><span class="p">[:,</span> <span class="n">has_name</span><span class="p">]</span>
    <span class="n">neuron_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">valid</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">all_neuron_names</span><span class="p">,</span> <span class="n">has_name</span><span class="p">)</span> <span class="k">if</span> <span class="n">valid</span><span class="p">]</span>

    <span class="c1"># Load the state names from Kato et al</span>
    <span class="n">state_names</span><span class="o">=</span><span class="n">load_kato_key</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">neuron_names</span><span class="o">=</span><span class="n">neuron_names</span><span class="p">,</span> 
                <span class="n">y</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> 
                <span class="n">z_kato</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">states</span><span class="p">),</span> 
                <span class="n">state_names</span><span class="o">=</span><span class="n">state_names</span><span class="p">,</span>
                <span class="n">fps</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">gradient_cmap</span><span class="p">(</span><span class="n">colors</span><span class="p">,</span> <span class="n">nsteps</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Make a colormap that interpolates between a set of colors</span>
    <span class="n">ncolors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">colors</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bounds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bounds</span> <span class="o">=</span> <span class="n">onp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">ncolors</span><span class="p">)</span>

    <span class="n">reds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">greens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">blues</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">alphas</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">b</span><span class="p">,</span><span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bounds</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
        <span class="n">reds</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">greens</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">blues</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
        <span class="n">alphas</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span> <span class="k">else</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">))</span>

    <span class="n">cdict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;red&#39;</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">reds</span><span class="p">),</span>
             <span class="s1">&#39;green&#39;</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">greens</span><span class="p">),</span>
             <span class="s1">&#39;blue&#39;</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">blues</span><span class="p">),</span>
             <span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">alphas</span><span class="p">)}</span>

    <span class="n">cmap</span> <span class="o">=</span> <span class="n">LinearSegmentedColormap</span><span class="p">(</span><span class="s1">&#39;grad_colormap&#39;</span><span class="p">,</span> <span class="n">cdict</span><span class="p">,</span> <span class="n">nsteps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cmap</span>


<span class="k">def</span> <span class="nf">states_to_changepoints</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">z</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">onp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">onp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">onp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">onp</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">z</span><span class="p">))[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">z</span><span class="o">.</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]))</span>


<span class="k">def</span> <span class="nf">plot_2d_continuous_states</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> 
                              <span class="n">colors</span><span class="o">=</span><span class="n">palette</span><span class="p">,</span>
                              <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">inds</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
                              <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">),</span>
                              <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

    <span class="n">cps</span> <span class="o">=</span> <span class="n">states_to_changepoints</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

    <span class="c1"># Color denotes our inferred latent discrete state</span>
    <span class="k">for</span> <span class="n">cp_start</span><span class="p">,</span> <span class="n">cp_stop</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cps</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">cps</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">cp_start</span><span class="p">:</span><span class="n">cp_stop</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inds</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
                <span class="n">x</span><span class="p">[</span><span class="n">cp_start</span><span class="p">:</span><span class="n">cp_stop</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inds</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
                 <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">z</span><span class="p">[</span><span class="n">cp_start</span><span class="p">]],</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">cmap</span> <span class="o">=</span> <span class="n">gradient_cmap</span><span class="p">(</span><span class="n">palette</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_elbos</span><span class="p">(</span><span class="n">avg_elbos</span><span class="p">,</span> <span class="n">marginal_ll</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">marginal_ll</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">marginal_ll</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">avg_elbos</span><span class="p">),</span> 
                <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\log p(y \mid \Theta)$&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">avg_elbos</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\mathcal</span><span class="si">{L}</span><span class="s2">[q, \Theta]$&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Iteration&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;ELBO&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">marginal_ll</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">marginal_ll</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">avg_elbos</span><span class="p">),</span> 
                <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\log p(y \mid \Theta)$&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">avg_elbos</span><span class="p">)),</span> <span class="n">avg_elbos</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> 
                <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\mathcal</span><span class="si">{L}</span><span class="s2">[q, \Theta]$&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Iteration&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;ELBO&quot;</span><span class="p">)</span>

    <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">avg_elbos</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">avg_elbos</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Iteration&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Change in ELBO&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">avg_elbos</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> 
                <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="part-0-build-the-generative-model">
<h2>Part 0: Build the generative model<a class="headerlink" href="#part-0-build-the-generative-model" title="Permalink to this heading">#</a></h2>
<p>To start, we’ll consider a model that has both discrete and continuous latent variables, just like a switching linear dynamical system, but we’ll get rid of the time dependencies. Let <span class="math notranslate nohighlight">\(z_t \in \{1,\ldots, K\}\)</span> denote a discrete latent state, <span class="math notranslate nohighlight">\(x_t \in \mathbb{R}^D\)</span> denote a continuous latent state, and <span class="math notranslate nohighlight">\(y_t \in \mathbb{R}^N\)</span> denote an observed data point. The model is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(z, x, y \mid \Theta) &amp;= \prod_{t=1}^T p(z_t \mid \Theta) \, p(x_t \mid z_t, \Theta) \, p(y_t \mid x_t, \Theta) \\
&amp;= \prod_{t=1}^T \mathrm{Cat}(z_t \mid \pi) \, \mathcal{N}(x_t \mid b_{z_t}, Q_{z_t}) \, \mathcal{N}(y_t \mid C x_t + d, R) 
\end{align*}
\end{split}\]</div>
<p>where the parameters <span class="math notranslate nohighlight">\(\Theta\)</span> consist of,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\pi \in \Delta_K\)</span>, a distribution on discrete states</p></li>
<li><p><span class="math notranslate nohighlight">\(b_k \in \mathbb{R}^D\)</span>, a mean for each discrete state</p></li>
<li><p><span class="math notranslate nohighlight">\(Q_k \in \mathbb{R}^{D \times D}\)</span>, a covariance for each discrete state</p></li>
<li><p><span class="math notranslate nohighlight">\(C \in \mathbb{R}^{N \times D}\)</span>, an <em>observation matrix</em></p></li>
<li><p><span class="math notranslate nohighlight">\(d \in \mathbb{R}^{N}\)</span>, an <em>observation bias</em></p></li>
<li><p><span class="math notranslate nohighlight">\(R = \mathrm{diag}([r_1^2, \ldots, r_N^2])\)</span>, a diagonal observation coariance matrix.</p></li>
</ul>
<p>This is called a <strong>mixture of factor analyzers</strong> since each <span class="math notranslate nohighlight">\(p(y, x \mid z, \Theta)\)</span> is a factor analysis model. We also recognize it as an analogue of the switching linear dynamical system without any temporal dependencies.</p>
<section id="make-a-linear-regression-distribution-object">
<h3>Make a Linear Regression Distribution object<a class="headerlink" href="#make-a-linear-regression-distribution-object" title="Permalink to this heading">#</a></h3>
<p>We’ll be using PyTorch Distributions for this lab. PyTorch doesn’t include conditional distributions like <span class="math notranslate nohighlight">\(p(y \mid x)\)</span>, so we’ve written a lightweight object to encapsulate the parameters of the linear Gaussian observation model as well. We call it an <code class="docutils literal notranslate"><span class="pre">IndependentLinearRegression</span></code> because the observation covariance <span class="math notranslate nohighlight">\(R\)</span> is a diagonal matrix, which implies independent noise across each output dimension.  This is similar to what you wrote in <a class="reference internal" href="06_arhmm.html"><span class="doc std std-doc">Lab 6</span></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">IndependentLinearRegression</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An object that encapsulates the weights and covariance of a linear </span>
<span class="sd">    regression. It has an interface similar to that of PyTorch Distributions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">diag_covariance</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        weights: N x D tensor of regression weights</span>
<span class="sd">        bias: N tensor of regression bias</span>
<span class="sd">        diag_covariance: N tensor of non-negative variances</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">covariate_dim</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">assert</span> <span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_dim</span>
        <span class="k">assert</span> <span class="n">diag_covariance</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">diag_covariance</span> <span class="o">=</span> <span class="n">diag_covariance</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">covariates</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the log probability of the data given the covariates using the </span>
<span class="sd">        model parameters. Note that this function&#39;s signature is slightly </span>
<span class="sd">        different from what you implemented in Lab 7.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        data: a tensor with lagging dimension $N$, the dimension of the data.</span>
<span class="sd">        covariates: a tensor with lagging dimension $D$, the covariate dimension</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        lp: a tensor of log likelihoods for each data point and covariate pair.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="n">lp</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="c1">###</span>
        <span class="c1"># YOUR CODE BELOW</span>
        <span class="n">lp</span> <span class="o">+=</span> <span class="o">...</span>
        <span class="c1">###</span>
        <span class="k">return</span> <span class="n">lp</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">covariates</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sample data points given covariates.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;...d,nd-&gt;...n&#39;</span><span class="p">,</span> <span class="n">covariates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> 
        <span class="n">predictions</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">lkhd</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">diag_covariance</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">lkhd</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="make-a-mixture-of-factor-analyzers-object">
<h3>Make a mixture of factor analyzers object<a class="headerlink" href="#make-a-mixture-of-factor-analyzers-object" title="Permalink to this heading">#</a></h3>
<p>To get you started, we’ve written a <code class="docutils literal notranslate"><span class="pre">MixtureOfFactorAnalyzers</span></code> object that encapsulates the generative model. It’s built out of <code class="docutils literal notranslate"><span class="pre">torch.distributions.Distribution</span></code> objects, which represent the distributions in the generative model. You’re already familiar with the <code class="docutils literal notranslate"><span class="pre">MultivariateNormal</span></code> distribution object, which we will use to represent both <span class="math notranslate nohighlight">\(p(x \mid z)\)</span>. We also use the <code class="docutils literal notranslate"><span class="pre">Categorical</span></code> distribution object to represent <span class="math notranslate nohighlight">\(p(z)\)</span>. We’ll take advantage of the distribution objects’ broadcasting capability to combine all the conditional distributions <span class="math notranslate nohighlight">\(p(x \mid z=k)\)</span> into one object by using a batch of means and covariances.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MixtureOfFactorAnalyzers</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_states</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">data_dim</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_states</span> <span class="o">=</span> <span class="n">num_states</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span> <span class="o">=</span> <span class="n">latent_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_dim</span> <span class="o">=</span> <span class="n">data_dim</span>
        
        <span class="c1"># Initialize the discrete state prior p(z)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p_z</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_states</span><span class="p">))</span>

        <span class="c1"># Initialize the conditional distributions p(x | z)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p_x</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span>
            <span class="n">scale</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">),</span> 
            <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Initialize the observation model p(y | x)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p_y</span> <span class="o">=</span> <span class="n">IndependentLinearRegression</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">data_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">data_dim</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">data_dim</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Write property to get the parameters from the underlying objects</span>
    <span class="c1"># These variable names correspond to the math above.</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pi</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_z</span><span class="o">.</span><span class="n">probs</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">log_pi</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_z</span><span class="o">.</span><span class="n">logits</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">bs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_x</span><span class="o">.</span><span class="n">mean</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">Qs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_x</span><span class="o">.</span><span class="n">covariance_matrix</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">Js</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_x</span><span class="o">.</span><span class="n">precision_matrix</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">hs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># linear natural paramter h = Q^{-1} b = J b</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;kij,kj-&gt;ki&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Js</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">C</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_y</span><span class="o">.</span><span class="n">weights</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">d</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_y</span><span class="o">.</span><span class="n">bias</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">R_diag</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_y</span><span class="o">.</span><span class="n">diag_covariance</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,)):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Draw a sample of the latent variables and data under the MFA model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">###</span>
        <span class="c1"># YOUR CODE HERE</span>
        <span class="n">z</span> <span class="o">=</span> <span class="o">...</span>
        <span class="n">x</span> <span class="o">=</span> <span class="o">...</span>
        <span class="n">y</span> <span class="o">=</span> <span class="o">...</span>
        <span class="c1">###</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">spc</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="c1"># Unpack the arguments</span>
        <span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;z&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_states</span>
        <span class="n">N</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_dim</span>
        <span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        
        <span class="c1"># Plot the data</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">z</span> <span class="o">==</span> <span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">z</span> <span class="o">==</span> <span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">palette</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">mec</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;continuous latente dim 0&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;continuous latente dim 1&quot;</span><span class="p">)</span>

        <span class="c1"># Sort the data by their discrete states for nicer visualization</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">perm</span><span class="p">][</span><span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="o">-</span><span class="n">spc</span><span class="p">,</span> <span class="n">spc</span> <span class="o">*</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)),</span> 
                   <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">palette</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> 
                   <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span> <span class="o">+</span> <span class="n">spc</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="s1">&#39;wo&#39;</span><span class="p">,</span> <span class="n">mec</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">T</span><span class="p">],</span> <span class="p">[</span><span class="n">spc</span> <span class="o">*</span> <span class="n">n</span><span class="p">,</span> <span class="n">spc</span> <span class="o">*</span> <span class="n">n</span><span class="p">],</span> <span class="s1">&#39;:k&#39;</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;data index [sorted by discrete state]&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="n">spc</span><span class="p">,</span> <span class="n">spc</span> <span class="o">*</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">spc</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;data dimension&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="sample-data-from-the-generative-model">
<h3>Sample data from the generative model<a class="headerlink" href="#sample-data-from-the-generative-model" title="Permalink to this heading">#</a></h3>
<p>Now we will sample small training and testing datasets from an MFA model with random parameters. We plot the data in two ways: as points in the continuous latent space color coded by discrete label, and then as points in the data space. Don’t be fooled by the ordering of the second plot: the samples are arbitrarily ordered, but we’ve permuted their order to see how different states give rise to better see the distribution corresponding to each discrete state.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Construct a model instance.</span>
<span class="c1"># The scale keyword determines how separated the clusters are in the continuous</span>
<span class="c1"># latent space.</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">num_states</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">data_dim</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MixtureOfFactorAnalyzers</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">data_dim</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Sample from the model</span>
<span class="n">num_data</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_data</span><span class="p">,))</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_data</span><span class="p">,))</span>

<span class="c1"># Plot the data</span>
<span class="n">model</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/93ea99bb119cb293b3f234923fa8b9a5414605f4d0189cd9714ae62dc4eaab40.png" src="../_images/93ea99bb119cb293b3f234923fa8b9a5414605f4d0189cd9714ae62dc4eaab40.png" />
<img alt="../_images/077ae4d70024463820c66d22cd6218909a212bd08fe79018687784646d996f59.png" src="../_images/077ae4d70024463820c66d22cd6218909a212bd08fe79018687784646d996f59.png" />
</div>
</div>
</section>
</section>
<section id="part-1-coordinate-ascent-variational-inference-cavi">
<h2>Part 1: Coordinate Ascent Variational Inference (CAVI)<a class="headerlink" href="#part-1-coordinate-ascent-variational-inference-cavi" title="Permalink to this heading">#</a></h2>
<p>First, we’ll implement coordinate ascent variational inference (CAVI) for the mixture of factor analyzers model. We’ll use a mean field posterior approximation</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
p(z, x \mid y, \Theta) \approx \prod_{t=1}^T q(z_t) \, q(x_t)
\end{align*}
\]</div>
<p>such that <span class="math notranslate nohighlight">\(\mathrm{KL}\big( q(z)q(x) \, \| \, p(z, x \mid y, \Theta) \big)\)</span> is minimized. In class, we showed how to minimize the KL via coordinate ascent, iteratively optimizing <span class="math notranslate nohighlight">\(q(z)\)</span> and <span class="math notranslate nohighlight">\(q(x)\)</span>, holding the other fixed. Here we will implement that algorithm.</p>
<section id="problem-1a-math-derive-the-expected-log-likelihood">
<h3>Problem 1a [Math]: Derive the expected log likelihood<a class="headerlink" href="#problem-1a-math-derive-the-expected-log-likelihood" title="Permalink to this heading">#</a></h3>
<p>In class we derived the coordinate update for the discrete state factors,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\log q(z_t) &amp;= \mathbb{E}_{q(x_t)} \left[\log p(z_t, x_t, y \mid \Theta) \right] + \mathrm{c} \\
&amp;= \mathbb{E}_{q(x_t)} \left[\log p(z_t \mid \Theta) + \log p(x_t \mid z_t, \Theta) + \log p(y \mid x_t, \Theta) \right] + \mathrm{c} \\
&amp;= \log \mathrm{Cat}(z_t \mid \pi) + \mathbb{E}_{q(x_t)} \left[\log \mathcal{N}(x_t \mid b_{z_t}, Q_{z_t}) \right] + \mathrm{c} \\
&amp;= \sum_{k=1}^K \mathbb{I}[z_t=k] \left( \log \pi_k + \mathbb{E}_{q(x_t)} \left[\log \mathcal{N}(x_t \mid b_k, Q_k) \right] \right) + \mathrm{c} \\
&amp;= \log \mathrm{Cat}(z_t \mid \tilde{\pi}_t)
\end{align*}
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\log \tilde{\pi}_{tk} = \log \pi_k + \underbrace{\mathbb{E}_{q(x_t)} \left[\log \mathcal{N}(x_t \mid b_k, Q_k) \right]}_{\text{expected log likelihood}} + \mathrm{c}.
\end{align*}
\]</div>
<p>However, we did not simplify the expected log likelihood expression.</p>
<p>_Suppose <span class="math notranslate nohighlight">\(q(x_t) = \mathcal{N}(x_t \mid \tilde{\mu}_t, \tilde{\Sigma}_t)\)</span>. Show that the expected log likelihood is,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\mathbb{E}_{q(x_t)} \left[\log \mathcal{N}(x_t \mid b_k, Q_k) \right]
&amp;= \log \mathcal{N}(\tilde{\mu}_t \mid b_k, Q_k) - \tfrac{1}{2} \langle(Q_k^{-1}, \tilde{\Sigma}_t \rangle
\end{align*}
\]</div>
<p><em>Your answer here</em></p>
</section>
<section id="problem-1b-implement-the-discrete-state-update">
<h3>Problem 1b: Implement the discrete state update<a class="headerlink" href="#problem-1b-implement-the-discrete-state-update" title="Permalink to this heading">#</a></h3>
<p>We will use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Distribution</span></code> objects to represent the  approximate posterior distributions as well. We will use <code class="docutils literal notranslate"><span class="pre">MultivariateNormal</span></code> to represent <span class="math notranslate nohighlight">\(q(x)\)</span> and <code class="docutils literal notranslate"><span class="pre">Categorical</span></code> to represent <span class="math notranslate nohighlight">\(q(z)\)</span>. We’ll take advantage of the distribution objects’ broadcasting capability to represent the variational posteriors for all time steps at once.</p>
<p><em>Implement a CAVI update for the discrete states posterior that takes in the model and the continuous state posterior <code class="docutils literal notranslate"><span class="pre">q_x</span></code> and outputs the optimal <code class="docutils literal notranslate"><span class="pre">q_z</span></code>.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cavi_update_q_z</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">q_x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the optimal discrete state posterior given the generative model</span>
<span class="sd">    and the variational posterior on the continuous states.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    model: a MixtureOfFactorAnalyzers model instance.</span>
<span class="sd">    </span>
<span class="sd">    q_x: a `MultivariateNormal` object with a shape `TxD` parameter `mean` and a </span>
<span class="sd">        shape `TxDxD` parameter `covariance matrix` representing the means and </span>
<span class="sd">        covariances, respectively, for each data point under the variational </span>
<span class="sd">        posterior.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    q_z: a `Categorical` object with a shape `TxK` parameter `logits` </span>
<span class="sd">        representing the variational posterior on discrete states.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">num_states</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">q_x</span><span class="o">.</span><span class="n">mean</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
    <span class="c1">###</span>
    <span class="c1"># YOUR CODE BELOW</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="p">(</span><span class="n">bk</span><span class="p">,</span> <span class="n">Qk</span><span class="p">,</span> <span class="n">Jk</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">bs</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">Qs</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">Js</span><span class="p">)):</span>
        <span class="n">logits</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="o">...</span>
    <span class="c1">###</span>
    <span class="k">return</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_1b</span><span class="p">():</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">q_x</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">),</span>
                             <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">q_z</span> <span class="o">=</span> <span class="n">cavi_update_q_z</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">q_x</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">q_z</span><span class="o">.</span><span class="n">probs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">num_states</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">q_z</span><span class="o">.</span><span class="n">probs</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.2576</span><span class="p">),</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="n">test_1b</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="problem-1c-implement-the-continuous-state-update">
<h3>Problem 1c: Implement the continuous state update<a class="headerlink" href="#problem-1c-implement-the-continuous-state-update" title="Permalink to this heading">#</a></h3>
<p>In class we showed that the optimal continuous state posterior, holding the discrete posterior fixed, was a Gaussian distribution <span class="math notranslate nohighlight">\(q(x_t) = \mathcal{N}(\tilde{\mu}_t, \tilde{\Sigma}_t)\)</span> with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\tilde{\mu}_t &amp;= \tilde{J}_t^{-1} \tilde{h}_t &amp;
\tilde{\Sigma}_t &amp;= \tilde{J}_t^{-1} \\
\tilde{h}_t &amp;= \mathbb{E}_{q({z_t})}[Q_{z_t}^{-1} b_{z_t}] + C^\top R^{-1} (y_t-d) &amp;
\tilde{J}_t &amp;= \mathbb{E}_{q({z_t})}[Q_{z_t}^{-1}] + C^\top R^{-1} C \\
&amp;= \sum_{k=1}^K \left[ q({z_t}=k) Q_k^{-1} b_k \right] + C^\top R^{-1} (y_t-d) &amp;
&amp;= \sum_{k=1}^K \left[ q({z_t}=k) Q_k^{-1} \right] + C^\top R^{-1} C
\end{align*}
\end{split}\]</div>
<p><em>Implement a CAVI update for the continuous states posterior that takes in <code class="docutils literal notranslate"><span class="pre">p_x</span></code>, <code class="docutils literal notranslate"><span class="pre">p_y</span></code>, and <code class="docutils literal notranslate"><span class="pre">q_z</span></code> and outputs the optimal <code class="docutils literal notranslate"><span class="pre">q_x</span></code>.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cavi_update_q_x</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">q_z</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the optimal discrete state posterior given the generative model</span>
<span class="sd">    and the variational posterior on the continuous states.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data: a dictionary with a key `y` containing a `TxN` tensor of data.</span>

<span class="sd">    model: a MixtureOfFactorAnalyzers model instance.</span>

<span class="sd">    q_z: a `Categorical` object with a shape `TxK` parameters `logits` and </span>
<span class="sd">        `probs` representing the variational posterior on discrete states.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    q_x: a `MultivariateNormal` object with a shape `TxD` parameter `mean` and a </span>
<span class="sd">        shape `TxDxD` parameter `covariance matrix` representing the means and </span>
<span class="sd">        covariances, respectively, for each data point under the variational </span>
<span class="sd">        posterior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span>

    <span class="c1">###</span>
    <span class="c1"># YOUR CODE BELOW</span>
    <span class="n">q_x</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1">###</span>
    <span class="k">return</span> <span class="n">q_x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_1c</span><span class="p">():</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">q_z</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">num_states</span><span class="p">))</span>
    <span class="n">q_x</span> <span class="o">=</span> <span class="n">cavi_update_q_x</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">q_z</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">q_x</span><span class="o">.</span><span class="n">mean</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">q_x</span><span class="o">.</span><span class="n">covariance_matrix</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">q_x</span><span class="o">.</span><span class="n">mean</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7204</span><span class="p">),</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">q_x</span><span class="o">.</span><span class="n">mean</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.9253</span><span class="p">),</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">q_x</span><span class="o">.</span><span class="n">covariance_matrix</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0271</span><span class="p">),</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">q_x</span><span class="o">.</span><span class="n">covariance_matrix</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0623</span><span class="p">),</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="n">test_1c</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="problem-1d-short-answer-intuition-for-the-continuous-updates">
<h3>Problem 1d [Short Answer]: Intuition for the continuous updates<a class="headerlink" href="#problem-1d-short-answer-intuition-for-the-continuous-updates" title="Permalink to this heading">#</a></h3>
<p>Consider setting the discrete posterior <span class="math notranslate nohighlight">\(q(z)\)</span> to be uniform over the <span class="math notranslate nohighlight">\(K\)</span> states and then performing one update of the continuous states. The plot below shows the true values of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(z\)</span> as color coded dots in 2D, and then it shows the means of the continuous state posterior <span class="math notranslate nohighlight">\(q(x)\)</span> found using one step of CAVI. We see that the means of the continuous state posterior are all pulled toward the center. Why would you expect that to happen?</p>
<hr class="docutils" />
<p><em>Your answer here</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_data_and_q_x</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">q_x</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;z&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">num_states</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">z</span> <span class="o">==</span> <span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">z</span> <span class="o">==</span> <span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">palette</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">mec</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">q_x</span><span class="o">.</span><span class="n">mean</span><span class="p">[</span><span class="n">z</span> <span class="o">==</span> <span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">q_x</span><span class="o">.</span><span class="n">mean</span><span class="p">[</span><span class="n">z</span> <span class="o">==</span> <span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">palette</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">mfc</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">mec</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;continuous latente dim 0&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;continuous latente dim 1&quot;</span><span class="p">)</span>

<span class="n">q_z</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">num_states</span><span class="p">))</span>
<span class="n">q_x</span> <span class="o">=</span> <span class="n">cavi_update_q_x</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">q_z</span><span class="p">)</span>
<span class="n">plot_data_and_q_x</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">q_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a7885c7187f8957ffe9588c6ed0bca1e2bf7d58e46c8c2657edb26a8cb574a38.png" src="../_images/a7885c7187f8957ffe9588c6ed0bca1e2bf7d58e46c8c2657edb26a8cb574a38.png" />
</div>
</div>
</section>
<section id="problem-1e-math-derive-the-evidence-lower-bound">
<h3>Problem 1e [Math]: Derive the evidence lower bound<a class="headerlink" href="#problem-1e-math-derive-the-evidence-lower-bound" title="Permalink to this heading">#</a></h3>
<p>We will use the ELBO to track the convergence of our CAVI algorithm. In class we wrote the ELBO as,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\mathcal{L}(q, \Theta) &amp;= \mathbb{E}_{q(z)q(x)} \left[ \log p(z, x, y \mid \Theta) - \log q(z)q(x) \right] 
\end{align*}
\]</div>
<p>Show that this is equivalent to,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\mathcal{L}(q, \Theta)
&amp;= \mathbb{E}_{q(x)}\left[\log p(y \mid x, \Theta) \right] 
- \mathrm{KL}\big(q(z) \, \| \, p(z \mid \Theta) \big) - \mathbb{E}_{q(z)}\left[\mathrm{KL}\big( q(x) \, \| \, p(x \mid z, \Theta) \big) \right].
\end{align*}
\]</div>
<p>Then show that,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\mathbb{E}_{q(x)}\left[\log p(y \mid x, \Theta) \right] 
&amp;= \sum_{t=1}^T \log \mathcal{N}(y_t \mid C \tilde{\mu}_t + d, R) -\tfrac{1}{2} \langle C^\top R^{-1} C, \tilde{\Sigma}_t \rangle,
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{\mu}_t\)</span> and <span class="math notranslate nohighlight">\(\tilde{\Sigma}_t\)</span> are the parameters of the variational posterior <span class="math notranslate nohighlight">\(q(x_t)\)</span>, as above.</p>
<hr class="docutils" />
<p><em>Your answer here</em></p>
</section>
<section id="problem-1f-implement-the-elbo">
<h3>Problem 1f: Implement the ELBO<a class="headerlink" href="#problem-1f-implement-the-elbo" title="Permalink to this heading">#</a></h3>
<p>Use the <code class="docutils literal notranslate"><span class="pre">IndependentLinearRegression.log_prob</span></code> function and the <code class="docutils literal notranslate"><span class="pre">torch.distributions.kl_divergence</span></code> function imported at the top of the notebook to implement the ELBO calculation. Remember that the log probabilities and KL divergence functions broadcast nicely.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">elbo</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">variational_posterior</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the optimal discrete state posterior given the generative model</span>
<span class="sd">    and the variational posterior on the continuous states.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data: a dictionary with a key `y` containing a `TxN` tensor of data.</span>

<span class="sd">    model: a MixtureOfFactorAnalyzers model instance</span>
<span class="sd">    </span>
<span class="sd">    variational_posterior: a tuple (q_z, q_x) where</span>
<span class="sd">        q_z: a `Categorical` object with a shape `TxK` parameter `logits` </span>
<span class="sd">            representing the variational posterior on discrete states.</span>

<span class="sd">        q_x: a `MultivariateNormal` object with a shape `TxD` parameter `mean` </span>
<span class="sd">            and a shape `TxDxD` parameter `covariance matrix` representing the </span>
<span class="sd">            means and covariances, respectively, for each data point under the </span>
<span class="sd">            variational posterior.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    The evidence lower bound (ELBO) as derived above.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span>
    <span class="n">q_z</span><span class="p">,</span> <span class="n">q_x</span> <span class="o">=</span> <span class="n">variational_posterior</span>

    <span class="c1">###</span>
    <span class="c1"># YOUR CODE BELOW</span>
    <span class="n">elbo</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1">###</span>
    <span class="k">return</span> <span class="n">elbo</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_1f</span><span class="p">():</span>
    <span class="n">q_z</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_data</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">num_states</span><span class="p">))</span>
    <span class="n">q_x</span> <span class="o">=</span> <span class="n">cavi_update_q_x</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">q_z</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">elbo</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="n">q_z</span><span class="p">,</span> <span class="n">q_x</span><span class="p">))</span> <span class="o">/</span> <span class="n">num_data</span><span class="p">,</span>
                         <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">32.3214</span><span class="p">),</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="n">test_1f</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="problem-1g-math-derive-the-exact-marginal-likelihood">
<h3>Problem 1g [Math]: Derive the exact marginal likelihood<a class="headerlink" href="#problem-1g-math-derive-the-exact-marginal-likelihood" title="Permalink to this heading">#</a></h3>
<p>In this simple model, we can actually compute the marginal likelihood exactly. This gives us a way of seeing how tight the ELBO actually is. (Remember, the ELBO is a lower bound on the marginal likelihood!)</p>
<p>To compute the marginal likelihood, we need two key facts about Gaussian random variables:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
x \sim \mathcal{N}(b, Q) &amp;\implies C x + d \sim \mathcal{N}(Cb + d, CQ C^\top) \\
m \sim \mathcal{N}(\mu_1, \Sigma_1), 
\epsilon \sim \mathcal{N}(\mu_2, \Sigma_2) &amp;\implies 
m + \epsilon \sim \mathcal{N}(\mu_1 + \mu_2, \Sigma_1 + \Sigma_2)
\end{align*}
\end{split}\]</div>
<p>Use these two facts to show that</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
p(y_t \mid z_t, \Theta) &amp;= \mathcal{N}(C b_{z_t} + d, C Q_{z_t} C^\top + R).
\end{align*}
\]</div>
<p>Then show that</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\log p(y \mid \Theta) &amp;= \sum_{t=1}^T \log \left( \sum_{k=1}^K \pi_{k} \, \mathcal{N}(y_t \mid C b_{k} + d, C Q_{k} C^\top + R) \right).
\end{align*}
\]</div>
<hr class="docutils" />
<p><em>Your answer here</em></p>
</section>
<section id="implement-the-exact-marginal-likelihood">
<h3>Implement the exact marginal likelihood<a class="headerlink" href="#implement-the-exact-marginal-likelihood" title="Permalink to this heading">#</a></h3>
<p>The code below implements the exact marginal likelihood according to the formula above using PyTorch’s <code class="docutils literal notranslate"><span class="pre">LowRankMultivariateNormal</span></code> distribution. Note: this distribution takes in the square root of <span class="math notranslate nohighlight">\(C Q_k C^\top\)</span>, which is <span class="math notranslate nohighlight">\(C Q_k^{1/2}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">exact_marginal_lkhd</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the exact marginal likelihood. </span>
<span class="sd">    Normalize by the number of datapoints.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Compute the marginal distributions </span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">num_states</span>
    
    <span class="c1"># Compute the marginal likelihood under each discrete state assignment</span>
    <span class="n">lls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="p">(</span><span class="n">bk</span><span class="p">,</span> <span class="n">Qk</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">bs</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">Qs</span><span class="p">)):</span>
        <span class="c1"># log p(z = k)</span>
        <span class="n">lls</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">model</span><span class="o">.</span><span class="n">log_pi</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>

        <span class="c1"># logp(y | z = k) = log N(y | C b_k + d, C Q_k C^T + diag(R))</span>
        <span class="n">Qk_sqrt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">Qk</span><span class="p">)</span>
        <span class="n">p_yk</span> <span class="o">=</span> <span class="n">LowRankMultivariateNormal</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">C</span> <span class="o">@</span> <span class="n">bk</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> 
                                         <span class="n">model</span><span class="o">.</span><span class="n">C</span> <span class="o">@</span> <span class="n">Qk_sqrt</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">R_diag</span><span class="p">)</span>
        <span class="n">lls</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">p_yk</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">lls</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">marginal_ll</span> <span class="o">=</span> <span class="n">exact_marginal_lkhd</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_data</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="run-cavi">
<h3>Run CAVI<a class="headerlink" href="#run-cavi" title="Permalink to this heading">#</a></h3>
<p>That’s all we need for CAVI! The code below simply alternates between updating <span class="math notranslate nohighlight">\(q(z)\)</span> and <span class="math notranslate nohighlight">\(q(x)\)</span>. After each iteration, we compute the ELBO.We allow the user to pass in an initial posterior approximation (though only <span class="math notranslate nohighlight">\(q(z)\)</span> is used since <span class="math notranslate nohighlight">\(q(x)\)</span> is immediately updated).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cavi</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">initial_posterior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">pbar</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span>

    <span class="c1"># Initialize the discrete state posterior to uniform</span>
    <span class="k">if</span> <span class="n">initial_posterior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">q_z</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">model</span><span class="o">.</span><span class="n">num_states</span><span class="p">))</span>
        <span class="n">q_x</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">q_z</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">initial_posterior</span>

    <span class="c1"># Optional progress bar</span>
    <span class="k">if</span> <span class="n">pbar</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="n">pbar</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="c1"># Run CAVI</span>
    <span class="n">avg_elbos</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">pbar</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
        <span class="n">q_x</span> <span class="o">=</span> <span class="n">cavi_update_q_x</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">q_z</span><span class="p">)</span>
        <span class="n">avg_elbos</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elbo</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="n">q_z</span><span class="p">,</span> <span class="n">q_x</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="n">q_z</span> <span class="o">=</span> <span class="n">cavi_update_q_z</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">q_x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">avg_elbos</span><span class="p">),</span> <span class="p">(</span><span class="n">q_z</span><span class="p">,</span> <span class="n">q_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run CAVI and plot the ELBO over coordinate ascent iterations</span>
<span class="n">avg_elbos</span><span class="p">,</span> <span class="p">(</span><span class="n">q_z</span><span class="p">,</span> <span class="n">q_x</span><span class="p">)</span> <span class="o">=</span> <span class="n">cavi</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="n">plot_elbos</span><span class="p">(</span><span class="n">avg_elbos</span><span class="p">,</span> <span class="n">marginal_ll</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dbfbb12a41ab519809a907283b354fc8079f10978d87e66a221113688802842f.png" src="../_images/dbfbb12a41ab519809a907283b354fc8079f10978d87e66a221113688802842f.png" />
</div>
</div>
</section>
<section id="re-examine-the-continuous-state-posterior-after-cavi">
<h3>Re-examine the continuous state posterior after CAVI<a class="headerlink" href="#re-examine-the-continuous-state-posterior-after-cavi" title="Permalink to this heading">#</a></h3>
<p>Now let’s make the same plot from Problem 1d again. We should see that the continuous means are pulled toward their true values. Remember, these are inferences! The CAVI algorithm only sees the data <span class="math notranslate nohighlight">\(y\)</span> and the model parameters <span class="math notranslate nohighlight">\(\Theta\)</span>. After a few iterations (really, after about 2 iterations), it converges to a posterior approximation in which the mean of continuous latent states, <span class="math notranslate nohighlight">\(\mathbb{E}_{q(x_t)}[x_t]\)</span>, are close to their true values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_data_and_q_x</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">q_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4dd984f94c16a3d1bddd26d253360fdbcb56e8145442f7d9b516170e1c366406.png" src="../_images/4dd984f94c16a3d1bddd26d253360fdbcb56e8145442f7d9b516170e1c366406.png" />
</div>
</div>
</section>
</section>
<section id="part-2-variational-em-in-a-mixture-of-factor-analysis-models">
<h2>Part 2: Variational EM in a mixture of factor analysis models<a class="headerlink" href="#part-2-variational-em-in-a-mixture-of-factor-analysis-models" title="Permalink to this heading">#</a></h2>
<p>The CAVI algorithm we implemented in Part 1 will form the E step for variational EM. To complete the algorithm, we just need to compute the expected sufficient statistics under the variational posterior and use them to implement the M-step. Last week, in Lab 7, we derived the expected sufficient statistics needed to update the multivariate normal distribution and the weights of the linear regression. In this part, you’ll write similar functions to compute the expected sufficient statistics using the variational posterior.</p>
<section id="problem-2a-compute-the-expected-sufficient-statistics">
<h3>Problem 2a: Compute the expected sufficient statistics<a class="headerlink" href="#problem-2a-compute-the-expected-sufficient-statistics" title="Permalink to this heading">#</a></h3>
<p>The sufficient statistics of the model are (with zero-indexing for Python friendliness),</p>
<ol class="arabic simple" start="0">
<li><p><span class="math notranslate nohighlight">\(\sum_{t=1}^T \mathbb{I}[z_t=k]\)</span> for <span class="math notranslate nohighlight">\(k = 1, \ldots, K\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{t=1}^T \mathbb{I}[z_t=k] \, x_t\)</span> for <span class="math notranslate nohighlight">\(k = 1, \ldots, K\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{t=1}^T \mathbb{I}[z_t=k] \, x_t x_t^\top\)</span> for <span class="math notranslate nohighlight">\(k = 1, \ldots, K\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{t=1}^T x_t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{t=1}^T x_t x_t^\top\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{t=1}^T y_t x_t^\top\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{t=1}^T y_t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{t=1}^T y_t^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{t=1}^T 1 = T\)</span></p></li>
</ol>
<p>Write a function that computes the <em>expected</em> sufficient statistics <span class="math notranslate nohighlight">\(\mathbb{E}_{q(z)q(x)}[\cdot]\)</span> under the variational posterior distribution. In code, we’ll call these variables <code class="docutils literal notranslate"><span class="pre">E_*</span></code>, for example <code class="docutils literal notranslate"><span class="pre">E_z</span></code> represents the length <span class="math notranslate nohighlight">\(K\)</span> tensor for the sufficient statistic 0.</p>
<p><strong>Note:</strong> The expected outer product, <span class="math notranslate nohighlight">\(\mathbb{E}_{q(x_t)}[x_t x_t^\top]\)</span>, does <em>not</em> equal the covariance matrix unless <span class="math notranslate nohighlight">\(\mathbb{E}_{q(x_t)}[x_t]\)</span> is zero (and here, it’s not generally zero).</p>
<p><strong>Note:</strong> Statistics 3 and 1 are redundant, as are 4 and 2. We’ve split them out anyway, as they are used separately in updating the parameters of <code class="docutils literal notranslate"><span class="pre">p_x</span></code> and <code class="docutils literal notranslate"><span class="pre">p_y</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_expected_suffstats</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">posterior</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the expected sufficient statistics of the data </span>
<span class="sd">    under the variational posterior</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data: a dictionary with a key `y` containing a `TxN` tensor of data.</span>
<span class="sd">    posterior: a tuple (q_z, q_x) representing the variational posterior, as </span>
<span class="sd">        computed in part 1.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    A tuple of the 9 expected sufficient statistics in the order listed above.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span>
    <span class="n">q_z</span><span class="p">,</span> <span class="n">q_x</span> <span class="o">=</span> <span class="n">posterior</span>
    
    <span class="n">E_z</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">E_zx</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">E_zxxT</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">E_x</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">E_xxT</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">E_yxT</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">E_y</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">E_ysq</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">T</span> <span class="o">=</span> <span class="o">...</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">E_z</span><span class="p">,</span> <span class="n">E_zx</span><span class="p">,</span> <span class="n">E_zxxT</span><span class="p">,</span> <span class="n">E_x</span><span class="p">,</span> <span class="n">E_xxT</span><span class="p">,</span> <span class="n">E_yxT</span><span class="p">,</span> <span class="n">E_y</span><span class="p">,</span> <span class="n">E_ysq</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_2a</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;This test only checks the shapes, not the values!&quot;</span><span class="p">)</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">compute_expected_suffstats</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="p">(</span><span class="n">q_z</span><span class="p">,</span> <span class="n">q_x</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span> <span class="o">==</span> <span class="mi">9</span>
    <span class="n">E_z</span><span class="p">,</span> <span class="n">E_zx</span><span class="p">,</span> <span class="n">E_zxxT</span><span class="p">,</span> <span class="n">E_x</span><span class="p">,</span> <span class="n">E_xxT</span><span class="p">,</span> <span class="n">E_yxT</span><span class="p">,</span> <span class="n">E_y</span><span class="p">,</span> <span class="n">E_ysq</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">stats</span>
    <span class="k">assert</span> <span class="n">E_z</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_states</span><span class="p">,)</span>
    <span class="k">assert</span> <span class="n">E_zx</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">E_zxxT</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">E_x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">latent_dim</span><span class="p">,)</span>
    <span class="k">assert</span> <span class="n">E_xxT</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">E_yxT</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">data_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">E_y</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">data_dim</span><span class="p">,)</span>
    <span class="k">assert</span> <span class="n">E_ysq</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">data_dim</span><span class="p">,)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span>

<span class="n">test_2a</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>This test only checks the shapes, not the values!
</pre></div>
</div>
</div>
</div>
</section>
<section id="problem-2b-implement-the-m-step-for-the-parameters-of-p-z-mid-theta">
<h3>Problem 2b: Implement the M-step for the parameters of <span class="math notranslate nohighlight">\(p(z \mid \Theta)\)</span><a class="headerlink" href="#problem-2b-implement-the-m-step-for-the-parameters-of-p-z-mid-theta" title="Permalink to this heading">#</a></h3>
<p>Write a function to update the prior distribution on discrete states, <span class="math notranslate nohighlight">\(p(z \mid \Theta)\)</span>, using the expected sufficient statistics. This is part of the M-step for variational EM.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_p_z</span><span class="p">(</span><span class="n">stats</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the parameters $\pi$ of the $p(z \mid \Theta)$ and pack them into </span>
<span class="sd">    a new Categorical distribution object.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    stats: a tuple of the 9 sufficient statistics computed above</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    A new Categorical object for p_z with a length K tensor of cluster </span>
<span class="sd">        probabilities.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">E_z</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1">### </span>
    <span class="c1"># YOUR CODE BELOW</span>
    <span class="n">p_z</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1">###</span>
    <span class="k">return</span> <span class="n">p_z</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="problem-2c-implement-the-m-step-for-parameters-of-p-x-mid-z-theta">
<h3>Problem 2c: Implement the M-step for parameters of <span class="math notranslate nohighlight">\(p(x \mid z, \Theta)\)</span><a class="headerlink" href="#problem-2c-implement-the-m-step-for-parameters-of-p-x-mid-z-theta" title="Permalink to this heading">#</a></h3>
<p>Perform an M-step on the parameters of <span class="math notranslate nohighlight">\(p(x \mid z, \Theta)\)</span> using the expected sufficient statistics. As before, add a little to the diagonal of the covariance to ensure positive definiteness.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_p_x</span><span class="p">(</span><span class="n">stats</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the parameters $\{b_k, Q_k\}$ of the $p(x \mid z, \Theta)$ and pack </span>
<span class="sd">    them into a new MultivariateNormal distribution object.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    stats: a tuple of the 9 sufficient statistics computed above</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    A new MultivariateNormal object with KxD mean and KxDxD covariance matrix.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">E_z</span><span class="p">,</span> <span class="n">E_zx</span><span class="p">,</span> <span class="n">E_zxxT</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">K</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">E_zx</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1">###</span>
    <span class="c1"># YOUR CODE BELOW</span>
    <span class="n">p_x</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1">#</span>
    <span class="c1">###</span>
    <span class="k">return</span> <span class="n">p_x</span> 
</pre></div>
</div>
</div>
</div>
</section>
<section id="problem-2d-implement-the-m-step-for-parameters-of-p-y-mid-x-theta">
<h3>Problem 2d: Implement the M-step for parameters of <span class="math notranslate nohighlight">\(p(y \mid x, \Theta)\)</span><a class="headerlink" href="#problem-2d-implement-the-m-step-for-parameters-of-p-y-mid-x-theta" title="Permalink to this heading">#</a></h3>
<p>Following Lab 7, let <span class="math notranslate nohighlight">\(\phi_t = (x_t, 1)\)</span> denote the covariates that go into the linear model for data point <span class="math notranslate nohighlight">\(y_t\)</span>. Specifically,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
p(y_t \mid x_t, \Theta) &amp;= \mathcal{N}(y_t \mid W \phi_t, R),
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(W = (C, d) \in \mathbb{R}^{N \times D+1}\)</span> is an array containing both the weights and the bias of the linear regression model.</p>
<p>To update the linear regression, we need the expected sufficient statistics:</p>
<ul class="simple">
<li><p>The expected outer product of the data and covariates,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\mathbb{E}_{q(x_t)}[ y \phi_t^\top] = \mathbb{E}_{q(x_t)}[ y (x_t, 1)^\top] 
= \begin{bmatrix} \mathbb{E}_{q(x_t)}[ y x_t^\top], &amp; y \end{bmatrix} 
\end{align*}
\]</div>
<ul class="simple">
<li><p>The expected outer product of the covariates with themselves,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbb{E}_{q(x_t)}[ \phi_t \phi_t^\top] = \mathbb{E}_{q(x_t)}[ (x_t, 1) (x_t, 1)^\top] 
= \begin{bmatrix} \mathbb{E}_{q(x_t)}[ x_t x_t^\top], &amp;  \mathbb{E}_{q(x_t)}[ x_t] \\
\mathbb{E}_{q(x_t)}[x_t^\top], &amp; T 
\end{bmatrix} 
\end{align*}
\end{split}\]</div>
<p>These are <span class="math notranslate nohighlight">\(N \times (D+1)\)</span> and <span class="math notranslate nohighlight">\((D+1) \times (D+1)\)</span> tensors, respectively.</p>
<p>Since we are assuming a diagonal covariance matrix, we only need <span class="math notranslate nohighlight">\(y_{tn}^2\)</span> instead of the full outer product <span class="math notranslate nohighlight">\(y_t y_t^\top\)</span>.  As before, add a bit to the diagonal to ensure positive definiteness.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_p_y</span><span class="p">(</span><span class="n">stats</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the linear regression parameters given the expected </span>
<span class="sd">    sufficient statistics.</span>

<span class="sd">    Note: add a little bit to the diagonal of each covariance </span>
<span class="sd">        matrix to ensure that the result is positive definite.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    stats: a tuple of the 8 sufficient statistics computed above</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    A new IndependentLinearRegression object for p_y</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">E_x</span><span class="p">,</span> <span class="n">E_xxT</span><span class="p">,</span> <span class="n">E_yxT</span><span class="p">,</span> <span class="n">E_y</span><span class="p">,</span> <span class="n">E_ysq</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">E_yxT</span><span class="o">.</span><span class="n">shape</span>
    
    <span class="c1">###</span>
    <span class="c1"># Use E_x, E_xxT, E_yxT, E_y, and T to compute the full expected </span>
    <span class="c1"># sufficient matrices as described above.    </span>
    <span class="c1">#</span>
    <span class="c1"># YOUR CODE BELOE</span>
    <span class="n">p_y</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1">#</span>
    <span class="c1">###</span>

    <span class="k">return</span> <span class="n">p_y</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="put-it-all-together">
<h3>Put it all together<a class="headerlink" href="#put-it-all-together" title="Permalink to this heading">#</a></h3>
<p>From here it’s smooth sailing! We just iterate between the variational E step, which involves running CAVI for some number of iterations, and then performing an M step using expected sufficient statistics. We’ll track the ELBO throughout to monitor convergence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">m_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">posterior</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform an M-step to update the model parameters given the data and the </span>
<span class="sd">    posterior from the variational E step.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">compute_expected_suffstats</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">posterior</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">p_z</span> <span class="o">=</span> <span class="n">update_p_z</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">p_x</span> <span class="o">=</span> <span class="n">update_p_x</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">p_y</span> <span class="o">=</span> <span class="n">update_p_y</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">variational_em</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">num_iters</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">num_cavi_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fit the model parameters via variational EM.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Run CAVI</span>
    <span class="n">avg_elbos</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>
        <span class="c1"># Variational E step with CAVI</span>
        <span class="n">these_elbos</span><span class="p">,</span> <span class="n">posterior</span> <span class="o">=</span> <span class="n">cavi</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> 
                                      <span class="n">num_steps</span><span class="o">=</span><span class="n">num_cavi_steps</span><span class="p">)</span>
        <span class="n">avg_elbos</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">these_elbos</span><span class="p">)</span>

        <span class="c1"># M-step</span>
        <span class="n">m_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">posterior</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">avg_elbos</span><span class="p">),</span> <span class="n">posterior</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit the synthetic data</span>
<span class="n">avg_elbos</span><span class="p">,</span> <span class="n">posterior</span> <span class="o">=</span> <span class="n">variational_em</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="n">plot_elbos</span><span class="p">(</span><span class="n">avg_elbos</span><span class="p">,</span> <span class="n">marginal_ll</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "e2508cb27df54ed1841340c4e850d79a", "version_major": 2, "version_minor": 0}</script><img alt="../_images/f4ea42e95786a3f395ca9dbf3ddb79dfbbc23461c0a7df5864b26330e0f69cd8.png" src="../_images/f4ea42e95786a3f395ca9dbf3ddb79dfbbc23461c0a7df5864b26330e0f69cd8.png" />
</div>
</div>
</section>
<section id="problem-2e-short-answer-interpret-the-results">
<h3>Problem 2e [Short Answer]: Interpret the results<a class="headerlink" href="#problem-2e-short-answer-interpret-the-results" title="Permalink to this heading">#</a></h3>
<p>One perhaps counterintuitive aspect of the output is that the ELBO of the fitted model actually exceeds the marginal likeliood of the true model. How can that happen?</p>
<p><em>Answer below this line</em></p>
</section>
<hr class="docutils" />
<section id="problem-2f-cross-validation">
<h3>Problem 2f: Cross validation<a class="headerlink" href="#problem-2f-cross-validation" title="Permalink to this heading">#</a></h3>
<p>Fit the MFA model with variational EM for <span class="math notranslate nohighlight">\(D=1,\ldots, 5\)</span> (inclusive), keeping the number of discrete states fixed to <span class="math notranslate nohighlight">\(K=7\)</span>. For each model, evaluate the evidence lower bound on the test data, using ten steps of CAVI to approximate the posterior. Then compute the exact marginal likelihood using the true model and compare.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_latent_dims</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">test_elbos</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">test_latent_dims</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fitting the MFA model with D =&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">d</span><span class="p">),</span> 
          <span class="s2">&quot;dimensional continuous states.&quot;</span><span class="p">)</span>
    <span class="c1">###</span>
    <span class="c1"># YOUR CODE BELOW</span>
    <span class="c1"># ...</span>
    <span class="n">test_elbos</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="c1">#</span>
    <span class="c1">###</span>

<span class="c1"># Compute the true marginal likelihood of the test dat</span>
<span class="n">true_test_elbo</span> <span class="o">=</span> <span class="n">exact_marginal_lkhd</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_data</span>

<span class="c1"># Plot as a function of continuous dimensionality</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_latent_dims</span><span class="p">,</span> <span class="n">test_elbos</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_latent_dims</span><span class="p">,</span> <span class="n">true_test_elbo</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">test_latent_dims</span><span class="p">),</span> <span class="s1">&#39;:k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;continuous dimension $D$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Test ELBO&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting the MFA model with D = 1 dimensional continuous states.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "dc3ffb7bd430443a9c77bcff5e9d5f18", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting the MFA model with D = 2 dimensional continuous states.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "939e959a91c841dfb6800248820f70c2", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting the MFA model with D = 3 dimensional continuous states.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "a51c0e465e8f47f28dc44c2d7c72865d", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting the MFA model with D = 4 dimensional continuous states.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "945de7897c9f4142817fb9c44c820a3e", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting the MFA model with D = 5 dimensional continuous states.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "48ad12c1547648ac8d365a77bb1a7663", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting the MFA model with D = 6 dimensional continuous states.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "58938c9404ae42d6babfedb87763ba37", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting the MFA model with D = 7 dimensional continuous states.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ed48dde48a9a48bab78aaafe165054d5", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting the MFA model with D = 8 dimensional continuous states.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "18cde248488c483f9e33bfecb55b9051", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting the MFA model with D = 9 dimensional continuous states.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "b16dbe984cae44dcb9a1c432a02ced60", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting the MFA model with D = 10 dimensional continuous states.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "b2ad87248a5840d496bc2be4e6b0db8c", "version_major": 2, "version_minor": 0}</script><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Test ELBO&#39;)
</pre></div>
</div>
<img alt="../_images/6c9f1f9a6135e64e48d93e68135d9f815a2fe96878e649d09893116171d432f3.png" src="../_images/6c9f1f9a6135e64e48d93e68135d9f815a2fe96878e649d09893116171d432f3.png" />
</div>
</div>
</section>
<section id="problem-2g-short-answer-interpret-the-results">
<h3>Problem 2g [Short answer]: Interpret the results<a class="headerlink" href="#problem-2g-short-answer-interpret-the-results" title="Permalink to this heading">#</a></h3>
<p>Would you be surprised to see the fitted models achieve higher ELBOs on test data than the marginal likelihod of the true model? Can you think of any potential concerns with using the ELBO for model comparison; e.g. for selecting the latent state dimension <span class="math notranslate nohighlight">\(D\)</span>?</p>
<hr class="docutils" />
<p><em>Your answer here</em></p>
</section>
</section>
<section id="part-3-apply-it-to-real-data">
<h2>Part 3: Apply it to real data<a class="headerlink" href="#part-3-apply-it-to-real-data" title="Permalink to this heading">#</a></h2>
<p>Finally, we’ll apply the mixture of factor analyzers to calcium imaging data from immobilized worms studied by Kato et al (2015). They also segmented the time series into discrete states based on the neural activity and gave each state a name, using their knowledge of how different neurons correlate with different types of behavior. We’ll try to recapitulate some of their results using the MFA model to infer discrete states.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%capture
!wget -nc https://www.dropbox.com/s/qnjslekm11pyuju/kato2015.zip
!unzip -n kato2015.zip 
</pre></div>
</div>
</div>
</div>
<section id="load-the-data">
<h3>Load the data<a class="headerlink" href="#load-the-data" title="Permalink to this heading">#</a></h3>
<p>The data is stored in a dictionary with a few extra keys for the neuron names and the given discrete state labels and human-interpretable state names.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the data for a single worm</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load_kato</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Extract key constants</span>
<span class="n">num_frames</span><span class="p">,</span> <span class="n">num_neurons</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="n">times</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_frames</span><span class="p">)</span> <span class="o">/</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;fps&quot;</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dict_keys([&#39;neuron_names&#39;, &#39;y&#39;, &#39;z_kato&#39;, &#39;state_names&#39;, &#39;fps&#39;])
</pre></div>
</div>
</div>
</div>
</section>
<section id="perform-pca">
<h3>Perform PCA<a class="headerlink" href="#perform-pca" title="Permalink to this heading">#</a></h3>
<p>We’ll use the principal components for visualization as well as for finding a permutation of the neurons that puts similar neurons, as measured by their loading on the first principal component, near to one another.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s2">&quot;pcs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span>
<span class="n">neuron_perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="plot-the-data">
<h3>Plot the data<a class="headerlink" href="#plot-the-data" title="Permalink to this heading">#</a></h3>
<p>We plot the time series of neural activity on top of the color-coded discrete states given by Kato et al. You should see that the different discrete states correspond to different levels of neural activity across the population of neurons.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;z_kato&quot;</span><span class="p">][</span><span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">times</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_neurons</span> <span class="o">+</span> <span class="mi">2</span><span class="p">),</span> 
           <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">times</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">][:,</span> <span class="n">neuron_perm</span><span class="p">]</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_neurons</span><span class="p">),</span> <span class="s1">&#39;-k&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;time[s]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;neurons&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_neurons</span><span class="p">),</span> 
           <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;neuron_names&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">neuron_perm</span><span class="p">],</span> 
           <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_neurons</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">state_name</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;state_names&quot;</span><span class="p">],</span> <span class="n">palette</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">nan</span><span class="p">],</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nan</span><span class="p">],</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">state_name</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7f414c9e73d0&gt;
</pre></div>
</div>
<img alt="../_images/dda5d286601fc72db4938163c992fbf4986bf2bf8cbbf0e335a17712e0691688.png" src="../_images/dda5d286601fc72db4938163c992fbf4986bf2bf8cbbf0e335a17712e0691688.png" />
</div>
</div>
</section>
<section id="plot-the-pca-trajectories">
<h3>Plot the PCA trajectories<a class="headerlink" href="#plot-the-pca-trajectories" title="Permalink to this heading">#</a></h3>
<p>We can also visualize the population activity as a trajectory through PCA space. Here we plot the trajectory in planes spanned by pairs of principal components. We color code the trajectory based on the given discrete states.</p>
<p><strong>Note</strong>: We smoothed the trajectories a bit to make the visualization nicer.</p>
<p><strong>Note</strong>: These differ from the figures in Kato et al (2015) in that they used PCA on the first order differences in neural activity (akin to the “spikes” in the calcium trace, even though <em>C elegans</em> doesn’t fire action potentials). We found that the first order differences didn’t cluster as nicely with the MFA model, so we are working with the calcium traces directly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pcs_smooth</span> <span class="o">=</span> <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;pcs&quot;</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
        <span class="n">plot_2d_continuous_states</span><span class="p">(</span><span class="n">pcs_smooth</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;z_kato&quot;</span><span class="p">],</span> 
                                  <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">inds</span><span class="o">=</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;PC</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;PC</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">j</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>

<span class="k">for</span> <span class="n">state_name</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;state_names&quot;</span><span class="p">],</span> <span class="n">palette</span><span class="p">):</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">nan</span><span class="p">],</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nan</span><span class="p">],</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> 
                    <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">state_name</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f4ef1fab3a422bbb638fac327c0410c18a171cdf2b47e45c1ad3b7d39b69cf76.png" src="../_images/f4ef1fab3a422bbb638fac327c0410c18a171cdf2b47e45c1ad3b7d39b69cf76.png" />
</div>
</div>
</section>
<section id="problem-3a-short-answer-interpret-the-pca-trajectories">
<h3>Problem 3a [Short Answer]: Interpret the PCA trajectories<a class="headerlink" href="#problem-3a-short-answer-interpret-the-pca-trajectories" title="Permalink to this heading">#</a></h3>
<p>What can you say about the cycle of neural activity in this worm given the PCA trajectories and the state labels provided by Kato et al (2015)?</p>
<p><em>Answer below this line</em></p>
</section>
<hr class="docutils" />
<section id="fit-the-mixture-of-factor-analyzers">
<h3>Fit the mixture of factor analyzers<a class="headerlink" href="#fit-the-mixture-of-factor-analyzers" title="Permalink to this heading">#</a></h3>
<p>Now fit the model. We’ll give it twice as many states as Kato et al (2015) did. This often helps avoid some local optima where states are unused. We’ll use ten dimensional continuous latents, as they do in the paper.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit the worm data</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">num_states</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">worm_model</span> <span class="o">=</span> <span class="n">MixtureOfFactorAnalyzers</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">)</span>

<span class="c1"># Fit the model!</span>
<span class="n">avg_elbos</span><span class="p">,</span> <span class="n">posterior</span> <span class="o">=</span> <span class="n">variational_em</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">worm_model</span><span class="p">,</span> 
                                      <span class="n">num_iters</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
                                      <span class="n">num_cavi_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plot_elbos</span><span class="p">(</span><span class="n">avg_elbos</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "a85814df052b4955be1263b55c117082", "version_major": 2, "version_minor": 0}</script><img alt="../_images/b8eafff96e46fb852ccd48b2ed7e3e7d5ee160871a93ec25e22bbd132644de91.png" src="../_images/b8eafff96e46fb852ccd48b2ed7e3e7d5ee160871a93ec25e22bbd132644de91.png" />
</div>
</div>
</section>
<section id="compute-the-overlap-between-the-given-and-inferred-discrete-states">
<h3>Compute the overlap between the given and inferred discrete states<a class="headerlink" href="#compute-the-overlap-between-the-given-and-inferred-discrete-states" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Find the most likely state segmentation</span>
<span class="n">q_z</span><span class="p">,</span> <span class="n">q_x</span> <span class="o">=</span> <span class="n">posterior</span>
<span class="n">z_inf</span> <span class="o">=</span> <span class="n">q_z</span><span class="o">.</span><span class="n">probs</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># compute overlap with the manually labeled states</span>
<span class="n">overlap</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_states</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_states</span><span class="p">):</span>
        <span class="n">overlap</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;z_kato&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">z_inf</span> <span class="o">==</span> <span class="n">j</span><span class="p">))</span>

<span class="c1"># normalize since sum given states are used less frequently than others</span>
<span class="n">overlap</span> <span class="o">/=</span> <span class="n">overlap</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># permute the inferred labels for easier visualization</span>
<span class="n">z_perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">overlap</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># show the permuted overlap matrix</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">overlap</span><span class="p">[:,</span> <span class="n">z_perm</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Kato et al labels&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;state_names&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;inferred discrete states&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;overlap (column normalized)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>

<span class="c1"># Permute the inferred discrete states per the new ordering</span>
<span class="n">z_inf_perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">z_perm</span><span class="p">)[</span><span class="n">z_inf</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/5fa834aa79c62a458e420def0bba88a74c7e51cab857976f2839a3c8a1919eff.png" src="../_images/5fa834aa79c62a458e420def0bba88a74c7e51cab857976f2839a3c8a1919eff.png" />
</div>
</div>
</section>
<section id="plot-the-inferred-segmentation-and-the-given-state-labels">
<h3>Plot the inferred segmentation and the given state labels<a class="headerlink" href="#plot-the-inferred-segmentation-and-the-given-state-labels" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> 
                        <span class="n">gridspec_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">height_ratios</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]),</span> 
                        <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;z_kato&quot;</span><span class="p">][</span><span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> 
              <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">times</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
              <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$z_{\mathsf</span><span class="si">{Kato}</span><span class="s2">}$&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">z_inf_perm</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">times</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_neurons</span> <span class="o">+</span> <span class="mi">2</span><span class="p">),</span> 
              <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">times</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">][:,</span> <span class="n">neuron_perm</span><span class="p">]</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_neurons</span><span class="p">),</span> 
            <span class="s1">&#39;-k&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;time[s]&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_neurons</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;neuron_names&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">neuron_perm</span><span class="p">],</span> 
                       <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;neurons&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_neurons</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-1.0, 50.0)
</pre></div>
</div>
<img alt="../_images/106d7b02f5c06c9cfab5cb7477718abeacbf6ecb137e12675227534c4974ee92.png" src="../_images/106d7b02f5c06c9cfab5cb7477718abeacbf6ecb137e12675227534c4974ee92.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_inf</span> <span class="o">=</span> <span class="n">q_x</span><span class="o">.</span><span class="n">mean</span>
<span class="n">x_inf_smooth</span> <span class="o">=</span> <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">x_inf</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
        <span class="n">plot_2d_continuous_states</span><span class="p">(</span><span class="n">x_inf_smooth</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;z_kato&quot;</span><span class="p">],</span> 
                                  <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">inds</span><span class="o">=</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;PC</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;PC</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">j</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>

<span class="k">for</span> <span class="n">state_name</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;state_names&quot;</span><span class="p">],</span> <span class="n">palette</span><span class="p">):</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">nan</span><span class="p">],</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nan</span><span class="p">],</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> 
                    <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">state_name</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a4edb4d97573846104b5e201647d736ba9c850a0166b10779aa1af1d1ab0481f.png" src="../_images/a4edb4d97573846104b5e201647d736ba9c850a0166b10779aa1af1d1ab0481f.png" />
</div>
</div>
</section>
<section id="bonus-problem-3b-compare-and-contrast">
<h3>Bonus: Problem 3b: Compare and contrast<a class="headerlink" href="#bonus-problem-3b-compare-and-contrast" title="Permalink to this heading">#</a></h3>
<p>We fit the model with twice as many discrete states as Kato et al (2015) reported. Split this time series into training and test sets and then sweep over <span class="math notranslate nohighlight">\(K\)</span> to choose the number of discrete states by cross validation.</p>
<p><em>Write your analysis code in the cell below and describe your results in words below this line</em></p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">###</span>
<span class="c1"># </span>
<span class="c1"># YOUR ANALYSIS CODE BELOW</span>

<span class="c1">#</span>
<span class="c1">###</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="bonus-problem-3c-assessing-variability-across-initializations">
<h3>Bonus: Problem 3c: Assessing variability across initializations<a class="headerlink" href="#bonus-problem-3c-assessing-variability-across-initializations" title="Permalink to this heading">#</a></h3>
<p>The estimated parameters and the inferred posterior depend on the random intialization of the model. How much does that affect our results? To compare the segmentations across model fits, make a <span class="math notranslate nohighlight">\(T \times T\)</span> matrix that shows how often the most likely discrete state at time <span class="math notranslate nohighlight">\(t\)</span> is the same as that at time <span class="math notranslate nohighlight">\(t'\)</span>.  I.e. let <span class="math notranslate nohighlight">\(\hat{z}_t^{(i)} = \mathrm{argmax}_k q(z_{t}^{(i)}=k)\)</span>, where the superscript <span class="math notranslate nohighlight">\((i)\)</span> denotes inferred posterior from the <span class="math notranslate nohighlight">\(i\)</span>-th model fit. Make a matrix whose entries are the average of the indicator <span class="math notranslate nohighlight">\(\mathbb{I}[\hat{z}_t^{(i)} = \hat{z}_{t'}^{(i)}]\)</span> taken over multiple fits of the MFA model with different random initializations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">###</span>
<span class="c1"># </span>
<span class="c1"># YOUR ANALYSIS CODE BELOW</span>

<span class="c1">#</span>
<span class="c1">###</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="part-4-switching-linear-dynamical-systems-slds">
<h2>Part 4: Switching Linear Dynamical Systems (SLDS)<a class="headerlink" href="#part-4-switching-linear-dynamical-systems-slds" title="Permalink to this heading">#</a></h2>
<p>If you’re interested in digging deeper and fitting models that explicitly incorporate temporal dependencies, check out the <a class="reference external" href="https://github.com/lindermanlab/ssm">SSM</a> package! To get you started, here’s some code to start fitting SLDS to this dataset.</p>
<p>Note that by default SSM uses a Laplace approximation for the continuous state posterior, which it fits with Newton’s method. Likewise, it uses a Monte Carlo approximation for the M-step, which works with non-Gaussian observation models. For the special case of Gaussian observations, we could find the exact posterior update for <span class="math notranslate nohighlight">\(q(x_{1:T})\)</span> and exact M-steps, as described in class. That would be a bit faster than the code below, but this is still fast enough for our purposes.</p>
<p>For more information, check out the <a class="reference external" href="https://lindermanlab.github.io/ssm-docs/">demo notebooks</a>!</p>
<p>Finally, you might also be interested in our latest project, <a class="reference external" href="https://probml.github.io/dynamax/">Dynamax</a>, which has JAX implementations of many probabilistic state space models. (Unfortunately, not yet SLDS!)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">ssm</span>
<span class="k">except</span><span class="p">:</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>git+https://github.com/lindermanlab/ssm.git@master#egg<span class="o">=</span>ssm

<span class="kn">from</span> <span class="nn">ssm</span> <span class="kn">import</span> <span class="n">SLDS</span>
<span class="n">slds</span> <span class="o">=</span> <span class="n">SLDS</span><span class="p">(</span><span class="n">num_neurons</span><span class="p">,</span> <span class="n">num_states</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<span class="n">elbos</span><span class="p">,</span> <span class="n">posterior</span> <span class="o">=</span> <span class="n">slds</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">from_t</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]),</span> <span class="n">num_iters</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="c1"># Plot the normalized elbos</span>
<span class="n">plot_elbos</span><span class="p">(</span><span class="n">elbos</span> <span class="o">/</span> <span class="n">num_frames</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting ssm
  Cloning https://github.com/lindermanlab/ssm.git (to revision master) to /tmp/pip-install-v40s2lyd/ssm_7022d754fa2d40199508dd4c4fc7d9b1
  Running command git clone --filter=blob:none --quiet https://github.com/lindermanlab/ssm.git /tmp/pip-install-v40s2lyd/ssm_7022d754fa2d40199508dd4c4fc7d9b1
  Resolved https://github.com/lindermanlab/ssm.git to commit 6c856ad3967941d176eb348bcd490cfaaa08ba60
  Preparing metadata (setup.py) ... ?25l?25hdone
Requirement already satisfied: numpy&gt;=1.18 in /usr/local/lib/python3.8/dist-packages (from ssm) (1.22.4)
Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from ssm) (1.10.1)
Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from ssm) (0.56.4)
Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from ssm) (1.2.1)
Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from ssm) (4.64.1)
Requirement already satisfied: autograd in /usr/local/lib/python3.8/dist-packages (from ssm) (1.5)
Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from ssm) (0.29.33)
Requirement already satisfied: future&gt;=0.15.2 in /usr/local/lib/python3.8/dist-packages (from autograd-&gt;ssm) (0.16.0)
Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba-&gt;ssm) (6.0.0)
Requirement already satisfied: llvmlite&lt;0.40,&gt;=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba-&gt;ssm) (0.39.1)
Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba-&gt;ssm) (57.4.0)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn-&gt;ssm) (3.1.0)
Requirement already satisfied: joblib&gt;=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn-&gt;ssm) (1.2.0)
Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata-&gt;numba-&gt;ssm) (3.15.0)
Building wheels for collected packages: ssm
  Building wheel for ssm (setup.py) ... ?25l?25hdone
  Created wheel for ssm: filename=ssm-0.0.1-cp38-cp38-linux_x86_64.whl size=562291 sha256=dd86a3a39db8d1948fa3d7685783a081cb71b5f55d899f1d886b0ad38f1c32e0
  Stored in directory: /tmp/pip-ephem-wheel-cache-zqgxan2j/wheels/50/34/75/a5397844576e080cdc001e6421017d9f4e355f83a1dd23f481
Successfully built ssm
Installing collected packages: ssm
Successfully installed ssm-0.0.1
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ca783cce9ec94b188cadffb8b9d3e796", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing with an ARHMM using 25 steps of EM.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "5972113fb5654f7aa75d813dd5b756ba", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "e5cb179e4bbe431d946d0714e79ed4d8", "version_major": 2, "version_minor": 0}</script><img alt="../_images/7eb651ad5b66322aadcccc35f39e71de90205af47b83630e41a055fb1c7fb824.png" src="../_images/7eb651ad5b66322aadcccc35f39e71de90205af47b83630e41a055fb1c7fb824.png" />
</div>
</div>
</section>
<section id="submission-instructions">
<h2>Submission Instructions<a class="headerlink" href="#submission-instructions" title="Permalink to this heading">#</a></h2>
<p><strong>Formatting:</strong> check that your code does not exceed 80 characters in line width. You can set <em>Tools → Settings → Editor → Vertical ruler column</em> to 80 to see when you’ve exceeded the limit.</p>
<p>Download your notebook in .ipynb format and use the following commands to convert it to PDF.</p>
<p><strong>Option 1 (best case): ipynb → pdf</strong> Run the following command to convert to a PDF:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">jupyter</span> <span class="n">nbconvert</span> <span class="o">--</span><span class="n">to</span> <span class="n">pdf</span> <span class="n">lab7_teamname</span><span class="o">.</span><span class="n">ipynb</span>
</pre></div>
</div>
<p>Unfortunately, <code class="docutils literal notranslate"><span class="pre">nbconvert</span></code> sometimes crashes with long notebooks. If that happens, here are a few options:</p>
<p><strong>Option 2 (next best): ipynb → tex → pdf</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">jupyter</span> <span class="n">nbconvert</span> <span class="o">--</span><span class="n">to</span> <span class="n">latex</span> <span class="n">lab7_teamname</span><span class="o">.</span><span class="n">ipynb</span>
<span class="n">pdflatex</span> <span class="n">lab7_teamname</span><span class="o">.</span><span class="n">tex</span>
</pre></div>
</div>
<p><strong>Option 3: ipynb → html → pdf</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">jupyter</span> <span class="n">nbconvert</span> <span class="o">--</span><span class="n">to</span> <span class="n">html</span> <span class="n">lab7_teamname</span><span class="o">.</span><span class="n">ipynb</span>
<span class="c1"># open lab7_teamname.html in browser and print to pdf</span>
</pre></div>
</div>
<p><strong>Dependencies:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">nbconvert</span></code>: If you’re using Anaconda for package management,</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">anaconda</span> <span class="n">nbconvert</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pdflatex</span></code>: It comes with standard TeX distributions like TeXLive, MacTex, etc. Alternatively, you can upload the .tex and supporting files to Overleaf (free with Stanford address) and use it to compile to pdf.</p></li>
</ul>
<p><strong>Upload</strong> your .ipynb and .pdf files to Gradescope.</p>
<p><strong>Only one submission per team!</strong></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./labs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="06_arhmm.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Lab 6: Autoregressive HMMs</p>
      </div>
    </a>
    <a class="right-next"
       href="08_lfads.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lab 8: Sequential VAEs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-0-build-the-generative-model">Part 0: Build the generative model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#make-a-linear-regression-distribution-object">Make a Linear Regression Distribution object</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#make-a-mixture-of-factor-analyzers-object">Make a mixture of factor analyzers object</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-data-from-the-generative-model">Sample data from the generative model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-coordinate-ascent-variational-inference-cavi">Part 1: Coordinate Ascent Variational Inference (CAVI)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1a-math-derive-the-expected-log-likelihood">Problem 1a [Math]: Derive the expected log likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1b-implement-the-discrete-state-update">Problem 1b: Implement the discrete state update</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1c-implement-the-continuous-state-update">Problem 1c: Implement the continuous state update</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1d-short-answer-intuition-for-the-continuous-updates">Problem 1d [Short Answer]: Intuition for the continuous updates</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1e-math-derive-the-evidence-lower-bound">Problem 1e [Math]: Derive the evidence lower bound</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1f-implement-the-elbo">Problem 1f: Implement the ELBO</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1g-math-derive-the-exact-marginal-likelihood">Problem 1g [Math]: Derive the exact marginal likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-the-exact-marginal-likelihood">Implement the exact marginal likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#run-cavi">Run CAVI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#re-examine-the-continuous-state-posterior-after-cavi">Re-examine the continuous state posterior after CAVI</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-variational-em-in-a-mixture-of-factor-analysis-models">Part 2: Variational EM in a mixture of factor analysis models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2a-compute-the-expected-sufficient-statistics">Problem 2a: Compute the expected sufficient statistics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2b-implement-the-m-step-for-the-parameters-of-p-z-mid-theta">Problem 2b: Implement the M-step for the parameters of <span class="math notranslate nohighlight">\(p(z \mid \Theta)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2c-implement-the-m-step-for-parameters-of-p-x-mid-z-theta">Problem 2c: Implement the M-step for parameters of <span class="math notranslate nohighlight">\(p(x \mid z, \Theta)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2d-implement-the-m-step-for-parameters-of-p-y-mid-x-theta">Problem 2d: Implement the M-step for parameters of <span class="math notranslate nohighlight">\(p(y \mid x, \Theta)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#put-it-all-together">Put it all together</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2e-short-answer-interpret-the-results">Problem 2e [Short Answer]: Interpret the results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2f-cross-validation">Problem 2f: Cross validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2g-short-answer-interpret-the-results">Problem 2g [Short answer]: Interpret the results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3-apply-it-to-real-data">Part 3: Apply it to real data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-the-data">Load the data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perform-pca">Perform PCA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-data">Plot the data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-pca-trajectories">Plot the PCA trajectories</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3a-short-answer-interpret-the-pca-trajectories">Problem 3a [Short Answer]: Interpret the PCA trajectories</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fit-the-mixture-of-factor-analyzers">Fit the mixture of factor analyzers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-the-overlap-between-the-given-and-inferred-discrete-states">Compute the overlap between the given and inferred discrete states</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-inferred-segmentation-and-the-given-state-labels">Plot the inferred segmentation and the given state labels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bonus-problem-3b-compare-and-contrast">Bonus: Problem 3b: Compare and contrast</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bonus-problem-3c-assessing-variability-across-initializations">Bonus: Problem 3c: Assessing variability across initializations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-4-switching-linear-dynamical-systems-slds">Part 4: Switching Linear Dynamical Systems (SLDS)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#submission-instructions">Submission Instructions</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Scott Linderman
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>