{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDUIT6-7igYM"
   },
   "source": [
    "# Lab 7: Segmenting behavioral videos with autoregressive HMMs\n",
    "\n",
    "**STATS320: Machine Learning Methods for Neural Data Analysis**\n",
    "\n",
    "_Stanford University. Winter, 2021._\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/slinderman/stats320/blob/main/labs/STAT220_320_Lab_7_Autoregressive_Hidden_Markov_Models_of_Behavior.ipynb)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Team Name:** _Your team name here_\n",
    "\n",
    "**Team Members:** _Names of everyone on your team here_\n",
    "\n",
    "*Due: 11:59pm Thursday, Feb 25, 2021 via GradeScope*\n",
    "\n",
    "---\n",
    "![](https://ars.els-cdn.com/content/image/1-s2.0-S0896627315010375-gr1.jpg)\n",
    "\n",
    "In this lab we'll develop hidden Markov models, specifically Gaussian and autoregressive hidden Markov models, to analyze depth videos of freely behaving mice. We'll implement, from scratch, the model developed by Wiltschko et al (2015) and extended in Markowitz et al (2018). Figure 1 of Wiltschko et al is reproduced above. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## References\n",
    "Markowitz, J. E., Gillis, W. F., Beron, C. C., Neufeld, S. Q., Robertson, K., Bhagat, N. D., ... & Sabatini, B. L. (2018). The striatum organizes 3D behavior via moment-to-moment action selection. Cell, 174(1), 44-58.\n",
    "\n",
    "Wiltschko, A. B., Johnson, M. J., Iurilli, G., Peterson, R. E., Katon, J. M., Pashkovski, S. L., ... & Datta, S. R. (2015). Mapping sub-second structure in mouse behavior. Neuron, 88(6), 1121-1135.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osPXb_Uy3yQ0"
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQEnxDJ5h0G8"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pynwb\n",
    "!pip install git+https://github.com/lindermanlab/ssm.git@master#egg=ssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDbCFyC431bM"
   },
   "outputs": [],
   "source": [
    "# First, import necessary libraries.\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "import seaborn as sns\n",
    "from tqdm.auto import trange\n",
    "from pynwb import NWBHDF5IO\n",
    "from google.colab import files\n",
    "\n",
    "import torch\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "# Specify that we want our tensors on the GPU and in float32\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.float64\n",
    "\n",
    "# Helper function to convert between numpy arrays and tensors\n",
    "to_t = lambda array: torch.tensor(array, device=device, dtype=dtype)\n",
    "from_t = lambda tensor: tensor.to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "yEoFXxMoipyt"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "import cv2\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from tempfile import NamedTemporaryFile\n",
    "import base64\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "# initialize a color palette for plotting\n",
    "palette = sns.xkcd_palette([\"windows blue\",\n",
    "                            \"red\",\n",
    "                            \"medium green\",\n",
    "                            \"dusty purple\",\n",
    "                            \"greyish\",\n",
    "                            \"orange\",\n",
    "                            \"amber\",\n",
    "                            \"clay\",\n",
    "                            \"pink\"])\n",
    "\n",
    "\n",
    "def sum_tuples(a, b):\n",
    "    assert a or b\n",
    "    if a is None:\n",
    "        return b\n",
    "    elif b is None:\n",
    "        return a\n",
    "    else:\n",
    "        return tuple(ai + bi for ai, bi in zip(a, b))\n",
    "\n",
    "\n",
    "_VIDEO_TAG = \"\"\"<video controls>\n",
    " <source src=\"data:video/x-m4v;base64,{0}\" type=\"video/mp4\">\n",
    " Your browser does not support the video tag.\n",
    "</video>\"\"\"\n",
    "\n",
    "def _anim_to_html(anim, fps=20):\n",
    "    # todo: todocument\n",
    "    if not hasattr(anim, '_encoded_video'):\n",
    "        with NamedTemporaryFile(suffix='.mp4') as f:\n",
    "            anim.save(f.name, fps=fps, extra_args=['-vcodec', 'libx264'])\n",
    "            video = open(f.name, \"rb\").read()\n",
    "        anim._encoded_video = base64.b64encode(video)\n",
    "\n",
    "    return _VIDEO_TAG.format(anim._encoded_video.decode('ascii'))\n",
    "\n",
    "def _display_animation(anim, fps=30, start=0, stop=None):\n",
    "    plt.close(anim._fig)\n",
    "    return HTML(_anim_to_html(anim, fps=fps))\n",
    "\n",
    "def play(movie, fps=30, speedup=1, fig_height=6,\n",
    "         filename=None, show_time=False, show=True):\n",
    "    # First set up the figure, the axis, and the plot element we want to animate\n",
    "    T, Py, Px = movie.shape[:3]\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(fig_height * Px/Py, fig_height))\n",
    "    im = plt.imshow(movie[0], interpolation='None', cmap=plt.cm.gray)\n",
    "\n",
    "    if show_time:\n",
    "        tx = plt.text(0.75, 0.05, 't={:.3f}s'.format(0), \n",
    "                    color='white',\n",
    "                    fontdict=dict(size=12),\n",
    "                    horizontalalignment='left',\n",
    "                    verticalalignment='center', \n",
    "                    transform=ax.transAxes)\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_data(movie[i * speedup])\n",
    "        if show_time: \n",
    "            tx.set_text(\"t={:.3f}s\".format(i * speedup / fps))\n",
    "        return im, \n",
    "\n",
    "    # call the animator.  blit=True means only re-draw the parts that have changed.\n",
    "    anim = animation.FuncAnimation(fig, animate, \n",
    "                                   frames=T // speedup, \n",
    "                                   interval=1, \n",
    "                                   blit=True)\n",
    "    plt.close(anim._fig)\n",
    "\n",
    "    # save to mp4 if filename specified\n",
    "    if filename is not None:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            anim.save(f.name, fps=fps, extra_args=['-vcodec', 'libx264'])\n",
    "\n",
    "    # return an HTML video snippet\n",
    "    if show:\n",
    "        print(\"Preparing animation. This may take a minute...\")\n",
    "        return HTML(_anim_to_html(anim, fps=30))\n",
    "\n",
    "\n",
    "def plot_data_and_states(data, states, \n",
    "                         spc=4, slc=slice(0, 900),\n",
    "                         title=None):\n",
    "    times = data[\"times\"][slc]\n",
    "    labels = data[\"labels\"][slc]\n",
    "    x = data[\"data\"][slc]\n",
    "    num_timesteps, data_dim = x.shape\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    ax.imshow(states[None, slc], \n",
    "              cmap=\"cubehelix\", aspect=\"auto\",\n",
    "              extent=(0, times[-1] - times[0], -data_dim * spc, spc))\n",
    "\n",
    "    ax.plot(times - times[0],\n",
    "            x - spc * np.arange(data_dim), \n",
    "            ls='-', lw=3, color='w')\n",
    "    ax.plot(times - times[0],\n",
    "            x - spc * np.arange(data_dim), \n",
    "            ls='-', lw=2, color=palette[0])\n",
    "    \n",
    "    ax.set_yticks(-spc * np.arange(data_dim))\n",
    "    ax.set_yticklabels(np.arange(data_dim))\n",
    "    ax.set_ylabel(\"principal component\")\n",
    "    ax.set_xlim(0, times[-1] - times[0])\n",
    "    ax.set_xlabel(\"time [ms]\")\n",
    "\n",
    "    if title is None:\n",
    "        ax.set_title(\"data and discrete states\")\n",
    "    else:\n",
    "        ax.set_title(title)\n",
    "\n",
    "\n",
    "def extract_syllable_slices(state_idx, \n",
    "                            posteriors, \n",
    "                            pad=30,\n",
    "                            num_instances=50, \n",
    "                            min_duration=5,\n",
    "                            max_duration=45,\n",
    "                            seed=0):\n",
    "    # Find all the start indices and durations of specified state\n",
    "    all_mouse_inds = []\n",
    "    all_starts = []\n",
    "    all_durations = []\n",
    "    for mouse, posterior in enumerate(posteriors):\n",
    "        states = np.argmax(posterior[\"expected_states\"], axis=1)\n",
    "        states = np.concatenate([[-1], states, [-1]])\n",
    "        starts = np.where((states[1:] == state_idx) \\\n",
    "                          & (states[:-1] != state_idx))[0]\n",
    "        stops = np.where((states[:-1] == state_idx) \\\n",
    "                         & (states[1:] != state_idx))[0]\n",
    "        durations = stops - starts\n",
    "        assert np.all(durations >= 1)\n",
    "        all_mouse_inds.append(mouse * np.ones(len(starts), dtype=int))\n",
    "        all_starts.append(starts)\n",
    "        all_durations.append(durations)\n",
    "    \n",
    "    all_mouse_inds = np.concatenate(all_mouse_inds)\n",
    "    all_starts = np.concatenate(all_starts)\n",
    "    all_durations = np.concatenate(all_durations)\n",
    "\n",
    "    # Throw away ones that are too short or too close to start.\n",
    "    # TODO: also throw away ones close to the end\n",
    "    valid = (all_durations >= min_duration) \\\n",
    "            & (all_durations < max_duration) \\\n",
    "            & (all_starts > pad) \n",
    "            \n",
    "    num_valid = np.sum(valid)\n",
    "    all_mouse_inds = all_mouse_inds[valid]\n",
    "    all_starts = all_starts[valid]\n",
    "    all_durations = all_durations[valid]\n",
    "\n",
    "    # Choose a random subset to show\n",
    "    rng = npr.RandomState(seed)\n",
    "    subset = rng.choice(num_valid, \n",
    "                        size=min(num_valid, num_instances), \n",
    "                        replace=False)\n",
    "    \n",
    "    all_mouse_inds = all_mouse_inds[subset]\n",
    "    all_starts = all_starts[subset]\n",
    "    all_durations = all_durations[subset]\n",
    "\n",
    "    # Extract slices for each mouse\n",
    "    slices = []\n",
    "    for mouse in range(len(posteriors)):\n",
    "        is_mouse = (all_mouse_inds == mouse)\n",
    "        slices.append([slice(start, start + dur) for start, dur in \n",
    "                       zip(all_starts[is_mouse], all_durations[is_mouse])])\n",
    "    \n",
    "    return slices\n",
    "\n",
    "\n",
    "def make_crowd_movie(state_idx,\n",
    "                     dataset, \n",
    "                     posteriors, \n",
    "                     pad=30, \n",
    "                     raw_size=(512, 424), \n",
    "                     crop_size=(80, 80), \n",
    "                     offset=(50, 50), \n",
    "                     scale=.5,\n",
    "                     min_height=10, \n",
    "                     **kwargs):\n",
    "    '''\n",
    "    Adapted from https://github.com/dattalab/moseq2-viz/blob/release/moseq2_viz/viz.py\n",
    "\n",
    "    Creates crowd movie video numpy array.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset (list of dicts): list of dictionaries containing data\n",
    "    slices (np.ndarray): video slices of specific syllable label\n",
    "    pad (int): number of frame padding in video\n",
    "    raw_size (tuple): video dimensions.\n",
    "    frame_path (str): variable to access frames in h5 file\n",
    "    crop_size (tuple): mouse crop size\n",
    "    offset (tuple): centroid offsets from cropped videos\n",
    "    scale (int): mouse size scaling factor.\n",
    "    min_height (int): minimum max height from floor to use.\n",
    "    kwargs (dict): extra keyword arguments\n",
    "    Returns\n",
    "    -------\n",
    "    crowd_movie (np.ndarray): crowd movie for a specific syllable.\n",
    "    '''\n",
    "    slices = extract_syllable_slices(state_idx, posteriors)\n",
    "\n",
    "    xc0, yc0 = crop_size[1] // 2, crop_size[0] // 2\n",
    "    xc = np.arange(-xc0, xc0 + 1, dtype='int16')\n",
    "    yc = np.arange(-yc0, yc0 + 1, dtype='int16')\n",
    "\n",
    "    durs = []\n",
    "    for these_slices in slices:\n",
    "        for slc in these_slices:\n",
    "            durs.append(slc.stop - slc.start)\n",
    "    \n",
    "    if len(durs) == 0:\n",
    "        print(\"no valid syllables found for state\", state_idx)\n",
    "        return\n",
    "    max_dur = np.max(durs)\n",
    "\n",
    "    # Initialize the crowd movie\n",
    "    crowd_movie = np.zeros((max_dur + pad * 2, raw_size[1], raw_size[0], 3), \n",
    "                            dtype='uint8')\n",
    "\n",
    "    for these_slices, data in zip(slices, dataset):\n",
    "        for slc in these_slices:\n",
    "            lpad = min(pad, slc.start)\n",
    "            rpad = min(pad, len(data['frames']) - slc.stop)\n",
    "            dur = slc.stop - slc.start\n",
    "            padded_slc = slice(slc.start - lpad, slc.stop + rpad)\n",
    "            centroid_x = data['centroid_x_px'][padded_slc] + offset[0]\n",
    "            centroid_y = data['centroid_y_px'][padded_slc] + offset[1]\n",
    "            angles = np.rad2deg(data['angles'][padded_slc])\n",
    "            frames = (data['frames'][padded_slc] / scale).astype('uint8')\n",
    "            flips = np.zeros(angles.shape, dtype='bool')\n",
    "\n",
    "            for i in range(lpad + dur + rpad):\n",
    "                if np.any(np.isnan([centroid_x[i], centroid_y[i]])):\n",
    "                    continue\n",
    "\n",
    "                rr = (yc + centroid_y[i]).astype('int16')\n",
    "                cc = (xc + centroid_x[i]).astype('int16')\n",
    "\n",
    "                if (np.any(rr < 1)\n",
    "                    or np.any(cc < 1)\n",
    "                    or np.any(rr >= raw_size[1])\n",
    "                    or np.any(cc >= raw_size[0])\n",
    "                    or (rr[-1] - rr[0] != crop_size[0])\n",
    "                    or (cc[-1] - cc[0] != crop_size[1])):\n",
    "                    continue\n",
    "\n",
    "                # rotate and clip the current frame\n",
    "                new_frame_clip = frames[i][:, :, None] * np.ones((1, 1, 3))\n",
    "                rot_mat = cv2.getRotationMatrix2D((xc0, yc0), angles[i], 1)\n",
    "                new_frame_clip = cv2.warpAffine(new_frame_clip.astype('float32'),\n",
    "                                                rot_mat, crop_size).astype(frames.dtype)\n",
    "\n",
    "                # overlay a circle on the mouse\n",
    "                if i >= lpad and i <= pad + dur:\n",
    "                    cv2.circle(new_frame_clip, (xc0, yc0), 3, \n",
    "                               (255, 0, 0), -1)\n",
    "                \n",
    "                # superimpose the clipped mouse \n",
    "                old_frame = crowd_movie[i]\n",
    "                new_frame = np.zeros_like(old_frame)\n",
    "                new_frame[rr[0]:rr[-1], cc[0]:cc[-1]] = new_frame_clip\n",
    "\n",
    "                # zero out based on min_height before taking the non-zeros\n",
    "                new_frame[new_frame < min_height] = 0\n",
    "                old_frame[old_frame < min_height] = 0\n",
    "\n",
    "                new_frame_nz = new_frame > 0\n",
    "                old_frame_nz = old_frame > 0\n",
    "\n",
    "                blend_coords = np.logical_and(new_frame_nz, old_frame_nz)\n",
    "                overwrite_coords = np.logical_and(new_frame_nz, ~old_frame_nz)\n",
    "\n",
    "                old_frame[blend_coords] = .5 * old_frame[blend_coords] \\\n",
    "                    + .5 * new_frame[blend_coords]\n",
    "                old_frame[overwrite_coords] = new_frame[overwrite_coords]\n",
    "\n",
    "                crowd_movie[i] = old_frame\n",
    "\n",
    "    return crowd_movie\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-a1d7Y414VOp"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "Fetch a single nwb file from the dropbox repo. These nwb files contain all relevant data from a specific moseq session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI_KjrF84obQ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget -nc https://www.dropbox.com/s/564wzasu1w7iogh/moseq_data.zip\n",
    "!unzip -n moseq_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIO6YK1Y4--w"
   },
   "source": [
    "# Warm up: Intro to the Neurodata Without Borders (NWB) format\n",
    "\n",
    "_This section was prepared by Akshay Jaggi (Datta Lab)_\n",
    "\n",
    "NWBs are hdf5 files with added structure. Like hdf5 files, they are hierarchically organized into Groups and Datasets. However, unlike hdf5 files, NWBs have rigid organization and labeling of these Groups and Datasets to ensure consistency across labs. \n",
    "\n",
    "The highest level of organization in NWBs are acquisitions and processing modules. Acquisitions are raw data streams (or links to them), and processing modules are groups of analyzed data derived from the acquisitions. \n",
    "\n",
    "MoSeq acquisitions are noisy, complex kinect videos that are then processed into egocentrically aligned, cleaned frames that you'll be working with. Since you don't need to worry about those preprocessing steps, the acquisition folder is empty. All of you relevant data will be in the MoSeq processing module. \n",
    "\n",
    "Inside processing modules, you'll find \"BehavioralTimeSeries,\" which are collections of related behavioral time series. Inside these objects, you'll find \"TimeSeries,\" which are individual time series datasets. \n",
    "\n",
    "The processing module is organized as such: \n",
    "```\n",
    " - MoSeq Processing Module (top level for all MoSeq processed data)\n",
    "   - MoSeq Scalar Time Series (Behavioral time series dictionary for all MoSeq derived scalar time series) \n",
    "     - angle \n",
    "     - area \n",
    "     - etc. \n",
    "   - MoSeq Image Series (Behavioral time series dictionary for all MoSeq derived image time series)\n",
    "     - frames \n",
    "     - masks\n",
    "   - MoSeq PC Series (Behavioral time series dictionary for all MoSeq derived PC time series)\n",
    "     - principal components (with nans inserted for dropped frames)\n",
    "     - principal components cleaned (with no nans)\n",
    "   - MoSeq Label Series (Behavioral time series dictionary for all MoSeq derived lables)\n",
    "     - labels (see above about dropped frames)\n",
    "     - labels cleaned (see above)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYcfJVp3D-oW"
   },
   "source": [
    "## Readin an NWB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCGoy1ys7Py_"
   },
   "outputs": [],
   "source": [
    "nwb_path = 'saline_example_0.nwb'\n",
    "io = NWBHDF5IO(nwb_path, mode='r')\n",
    "nwbfile = io.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjK30LG78CxH"
   },
   "outputs": [],
   "source": [
    "# Print the contents of the processing module\n",
    "nwbfile.processing['MoSeq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdpmfodT8D8j"
   },
   "outputs": [],
   "source": [
    "# Print the contents of one BehavioralTimeSeries\n",
    "nwbfile.processing['MoSeq']['Images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBqzzo_-882T"
   },
   "outputs": [],
   "source": [
    "# Examine the 'frames' time series\n",
    "# One thing you'll notice is that the \"timestamps\" is actually another time series \n",
    "# This is to save storage space when many time series share the same time stamps\n",
    "nwbfile.processing['MoSeq']['Images']['frames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVlhUSC2ELhy"
   },
   "source": [
    "## Load the video frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-ukSaRQ9FpN"
   },
   "outputs": [],
   "source": [
    "# Since NWBs are backed with HDF5, you can do typical lazy/partial loading here\n",
    "# Like hdf5s, you'll need to slice with [:] to get the array.\n",
    "frames = nwbfile.processing['MoSeq']['Images']['frames'].data[:]\n",
    "\n",
    "# Make sure to close the file when you're done!\n",
    "io.close()\n",
    "\n",
    "# Play a movie of the first 30 sec of data\n",
    "play(frames[:900])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NVND65mktaK"
   },
   "source": [
    "## Now we'll load all the mice into memory\n",
    "\n",
    "The frames, even after cropping, are still 80x80 pixels. That's a 3600 dimensional observation. In practice, the frames can be adequately reconstructed with far fewer principal components. As little as ten PCs does a pretty good job of capturing the mouse's posture.\n",
    "\n",
    "The Datta lab has already computed the principal components and included them in the NWB. We'll extract them, along with other relevant information like the centroid position and heading angle of the mouse, which we'll use for making \"crowd\" movies below. Finally, they also included labels from MoSeq, an autoregressive (AR) HMM. You'll build an ARHMM in Part 3 of the lab and infer similar discrete latent state sequences yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVgfZzT22oOg"
   },
   "outputs": [],
   "source": [
    "def load_dataset(indices=None, \n",
    "                 load_frames=True, \n",
    "                 num_pcs=10):\n",
    "    if indices is None:\n",
    "        indices = np.arange(24)\n",
    "        \n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    for t in trange(len(indices)):\n",
    "        i = indices[t]\n",
    "        nwb_path = \"saline_example_{}.nwb\".format(i)\n",
    "        with NWBHDF5IO(nwb_path, mode='r') as io:\n",
    "            f = io.read()\n",
    "            num_frames = len(f.processing['MoSeq']['PCs']['pcs_clean'].data)\n",
    "            train_slc = slice(0, int(0.8 * num_frames))\n",
    "            test_slc = slice(int(0.8 * num_frames)+1, -1)\n",
    "            \n",
    "            train_data, test_data = dict(), dict()\n",
    "            for slc, data in zip([train_slc, test_slc], [train_data, test_data]):\n",
    "                data[\"raw_pcs\"] = f.processing['MoSeq']['PCs']['pcs_clean'].data[slc][:, :num_pcs]\n",
    "                data[\"times\"] = f.processing['MoSeq']['PCs']['pcs_clean'].timestamps[slc][:]\n",
    "                data[\"centroid_x_px\"] = f.processing['MoSeq']['Scalars']['centroid_x_px'].data[slc][:]\n",
    "                data[\"centroid_y_px\"] = f.processing['MoSeq']['Scalars']['centroid_y_px'].data[slc][:]\n",
    "                data[\"angles\"] = f.processing['MoSeq']['Scalars']['angle'].data[slc][:]\n",
    "                data[\"labels\"] = f.processing['MoSeq']['Labels']['labels_clean'].data[slc][:]\n",
    "            \n",
    "            # only load the frames on the test data\n",
    "            test_data[\"frames\"] = f.processing['MoSeq']['Images']['frames'].data[test_slc]\n",
    "            \n",
    "        train_dataset.append(train_data)\n",
    "        test_dataset.append(test_data)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# The full dataset takes about 6GB of ram\n",
    "fps = 30\n",
    "data_dim = 10\n",
    "train_dataset, test_dataset = load_dataset(num_pcs=data_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUQQVypnky0d"
   },
   "source": [
    "##  Standardize the principal components\n",
    "\n",
    "Standardize the principal components so that they are \n",
    "mean zero and unit variance. This will not affect the\n",
    "subsequent modeling, but it will make it easier for us\n",
    "to visualize the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kG8PXal6dSN"
   },
   "outputs": [],
   "source": [
    "\n",
    "def standardize_pcs(dataset, mean=None, std=None):\n",
    "    if mean is None and std is None:\n",
    "        all_pcs = np.vstack([data['raw_pcs'] for data in dataset])\n",
    "        mean = all_pcs.mean(axis=0)\n",
    "        std = all_pcs.std(axis=0)\n",
    "\n",
    "    for data in dataset:\n",
    "        data['data'] = (data['raw_pcs'] - mean) / std\n",
    "    return dataset, mean, std\n",
    "\n",
    "train_dataset, mean, std = standardize_pcs(train_dataset)\n",
    "test_dataset, _, _ = standardize_pcs(test_dataset, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0E-I8ikk2Fw"
   },
   "source": [
    "## Plot a slice of data \n",
    "In the background, we're showing the labels that were given to us from MoSeq, an autoregressive hidden Markov model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrRyMF9_-hLs"
   },
   "outputs": [],
   "source": [
    "  plot_data_and_states(train_dataset[0], train_dataset[0][\"labels\"],\n",
    "                       title=\"data and given discrete states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2FQoYmRFqY7"
   },
   "source": [
    "## Summary\n",
    "\n",
    "You should now have a `train_dataset` and a `test_dataset` loaded in memory. Each dataset is a list of dictionaries, one for each mouse. Each dictionary contains a few keys, most important of which is the `data` key, containing the standardized principal component time series, as shown above. For the test dataset, we also included the `frames` key, which has the original 80x80 images. We'll use these to create the movies of each inferred state.\n",
    "\n",
    "**Note:** Keeping the data in memory is costly but convenient.  You shouldn't run out of memory in this lab, but if you ever did, a better solution might be to write the preprocessed data (e.g. with the standardized PC trajectories) back to the NWB files and reload those files as necessary during fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CD-FmY4jv2pG"
   },
   "source": [
    "# Part 1: Implement the forward-backward algorithm\n",
    "\n",
    "First, implement the forward-backward algorithm for computing the posterior distribution on latent states of a hidden Markov model, $q(z) = p(z \\mid x, \\Theta)$. Specifically, this algorithm will return a $T \\times K$ matrix where each entry represents the posterior probability that $q(z_t = k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCJYwKpPdmMn"
   },
   "source": [
    "## Helper functions to create random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgjJpvprdpeh"
   },
   "outputs": [],
   "source": [
    "def sticky_transitions(num_states, stickiness=0.95):\n",
    "    P = stickiness * np.eye(num_states) \n",
    "    P += (1 - stickiness) / (num_states - 1) * (1 - np.eye(num_states))\n",
    "    return P\n",
    "\n",
    "def random_args(num_timesteps, num_states, seed=0,\n",
    "                offset=0, scale=1):\n",
    "    rng = npr.RandomState(0)\n",
    "    pi = np.ones(num_states) / num_states\n",
    "    P = sticky_transitions(num_states)\n",
    "    log_likes = offset + scale * rng.randn(num_timesteps, num_states)\n",
    "    return pi, P, log_likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rgo3i5YialWH"
   },
   "source": [
    "## Problem 1a: Implement the forward pass\n",
    "\n",
    "As we derived in class, the forward pass recursively computes the _normalized_ forward messages $\\tilde{\\alpha}_t$ and the marginal log likelihood $\\log p(x \\mid \\Theta) = \\sum_{t} \\log A_t$.\n",
    "\n",
    "**Notes**:\n",
    "- This function takes in the _log_ likelihoods, $\\log \\ell_{tk}$, so you'll have to exponentiate in the forward pass\n",
    "- You need to be careful exponentiating though. If the log likelihoods are very negative, they'll all be essentially zero when exponentiated and you'll run into a divide-by-zero error when you compute the normalized forward message. Alternatively, if they're large positive numbers, your exponent will blow up and you'll get nan's in your calculations. \n",
    "- To avoid numerical issues, subtract $\\max_k (\\log \\ell_{tk})$ prior to exponentiating. It won't affect the normalized messages, but you will have to account for it in your computation of the marginal likelihood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7mPzdb_aez3"
   },
   "outputs": [],
   "source": [
    "def forward_pass(initial_dist, transition_matrix, log_likes):\n",
    "    \"\"\"\n",
    "    Perform the (normalized) forward pass of the HMM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    initial_dist: $\\pi$, the initial state distribution. Length K, sums to 1.\n",
    "    transition_matrix: $P$, a KxK transition matrix. Rows sum to 1.\n",
    "    log_likes: $\\log \\ell_{t,k}$, a TxK matrix of _log_ likelihoods.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alphas: TxK matrix with _normalized_ forward messages $\\tilde{\\alpha}_{t,k}$\n",
    "    marginal_ll: Scalar marginal log likelihood $\\log p(x | \\Theta)$\n",
    "    \"\"\"\n",
    "    alphas = np.zeros_like(log_likes)\n",
    "    marginal_ll = 0\n",
    "\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    #\n",
    "    ...\n",
    "    #\n",
    "    ###\n",
    "    return alphas, marginal_ll\n",
    "\n",
    "\n",
    "def test_forward_pass(num_timesteps=100, num_states=10, offset=0):\n",
    "    from ssm.messages import hmm_filter, hmm_normalizer\n",
    "    pi, P, log_likes = random_args(num_timesteps, num_states, offset=offset)\n",
    "    \n",
    "    # Call your code\n",
    "    log_likes_copy = log_likes.copy()\n",
    "    alphas, ll = forward_pass(pi, P, log_likes_copy)\n",
    "    assert np.all(np.isfinite(alphas))\n",
    "    assert np.allclose(alphas.sum(axis=1), 1.0)\n",
    "    assert np.allclose(log_likes, log_likes_copy)\n",
    "    \n",
    "    # Compare to SSM implementation. \n",
    "    # We prepend a dimension to P because SSM allows for \n",
    "    # different transition matrices at each time point\n",
    "    alphas2 = hmm_filter(pi, P[None, ...], log_likes)\n",
    "    ll2 = hmm_normalizer(pi, P[None, ...], log_likes)\n",
    "    assert np.allclose(alphas, alphas2)\n",
    "    assert np.allclose(ll, ll2)\n",
    "\n",
    "test_forward_pass()\n",
    "test_forward_pass(num_timesteps=10000, num_states=50, offset=-1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmWcflIhgw3b"
   },
   "source": [
    "## Problem 1b: Implement the backward pass\n",
    "\n",
    "Recursively compute the backward messages $\\beta_t$. Again, normalize to avoid underflow, and be careful when you exponentiate the log likelihoods. The same trick of subtracting the max before exponentiating will work here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKhMtK8ifj2Q"
   },
   "outputs": [],
   "source": [
    "def backward_pass(transition_matrix, log_likes):\n",
    "    \"\"\"\n",
    "    Perform the (normalized) backward pass of the HMM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transition_matrix: $P$, a KxK transition matrix. Rows sum to 1.\n",
    "    log_likes: $\\log \\ell_{t,k}$, a TxK matrix of _log_ likelihoods.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    betas: TxK matrix with _normalized_ backward messages $\\tilde{\\beta}_{t,k}$\n",
    "    \"\"\"\n",
    "    betas = np.zeros_like(log_likes)\n",
    "    \n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    ...\n",
    "    #\n",
    "    ###\n",
    "    return betas\n",
    "\n",
    "\n",
    "def test_backward_pass(num_timesteps=100, num_states=10, offset=0):\n",
    "    from ssm.messages import backward_pass as ssm_backward_pass\n",
    "    from scipy.special import logsumexp\n",
    "    _, P, log_likes = random_args(num_timesteps, num_states, offset=offset)\n",
    "    \n",
    "    # Call your code\n",
    "    betas = backward_pass(P, log_likes)\n",
    "    assert np.all(np.isfinite(betas))\n",
    "    assert np.allclose(betas.sum(axis=1), 1.0)\n",
    "    \n",
    "    # Compare to SSM implementation. \n",
    "    # We prepend a dimension to P because SSM allows for \n",
    "    # different transition matrices at each time point\n",
    "    log_betas2 = np.zeros_like(betas)\n",
    "    ssm_backward_pass(P[None, ...], log_likes, log_betas2)\n",
    "    betas2 = np.exp(log_betas2 - logsumexp(log_betas2, axis=1, keepdims=True))\n",
    "    assert np.allclose(betas, betas2)\n",
    "    \n",
    "test_backward_pass()\n",
    "test_backward_pass(num_timesteps=10000, num_states=50, offset=-1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmPQ3BYAlI-Y"
   },
   "source": [
    "## Problem 1c: Combine the forward and backward passes to perform the E step\n",
    "\n",
    "Compute the posterior marginal probabilities. We call these the `expected_states` because $q(z_t = k) = \\mathbb{E}_{q(z)}[\\mathbb{I}[z_t = k]]$. To copmute them, combine the forward messages, backward messages, and the likelihoods, then normalize. Again, be careful when exponentiating the likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pvqAsTHhS9N"
   },
   "outputs": [],
   "source": [
    "def E_step(initial_dist, transition_matrix, log_likes):\n",
    "    \"\"\"\n",
    "    Fun the forward and backward passes and then combine to compute the \n",
    "    posterior probabilities q(z_t=k).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    initial_dist: $\\pi$, the initial state distribution. Length K, sums to 1.\n",
    "    transition_matrix: $P$, a KxK transition matrix. Rows sum to 1.\n",
    "    log_likes: $\\log \\ell_{t,k}$, a TxK matrix of _log_ likelihoods.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    posterior: a dictionary with the following key-value pairs:\n",
    "        expected_states: a TxK matrix containing $q(z_t=k)$\n",
    "        marginal_ll: the marginal log likelihood from the forward pass.\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    ...\n",
    "    marginal_ll = ...\n",
    "    expected_states = ...\n",
    "    #\n",
    "    ###\n",
    "    \n",
    "    # Package the results into a dictionary summarizing the posterior\n",
    "    posterior = dict(expected_states=expected_states,\n",
    "                     marginal_ll=marginal_ll)\n",
    "    return posterior\n",
    "\n",
    "def test_E_step(num_timesteps=100, num_states=10, offset=0):\n",
    "    from ssm.messages import hmm_expected_states\n",
    "    pi, P, log_likes = random_args(num_timesteps, num_states, offset=offset)\n",
    "    \n",
    "    # Run your code\n",
    "    posterior = E_step(pi, P, log_likes)\n",
    "    \n",
    "    # Run SSM code\n",
    "    posterior2 = hmm_expected_states(pi, P[None, ...], log_likes)\n",
    "    \n",
    "    assert np.allclose(posterior[\"expected_states\"], posterior2[0])\n",
    "    assert np.allclose(posterior[\"marginal_ll\"], posterior2[2])\n",
    "\n",
    "test_E_step()\n",
    "test_E_step(num_timesteps=10000, num_states=50, offset=-1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWv_l-DIrxRm"
   },
   "source": [
    "## Time it on some more realistic sizes\n",
    "\n",
    "It should take about 1.5 seconds for a $T=36000$ time series with $K=50$ states. For a dataset with 24 mice, that's about 36 seconds per epoch for the E steps. (The M-steps will take about the same amount of time, so you're looking at around 1min per epoch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67qw4unFi7LI"
   },
   "outputs": [],
   "source": [
    "pi, P, log_likes = random_args(36000, 50)\n",
    "%timeit E_step(pi, P, log_likes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAltfVcoOu6L"
   },
   "source": [
    "## Helper function to create a random initial posterior distribution\n",
    "\n",
    "Finally, we'll initialize the HMM with a random posterior distribution. That way our first M step will yield reasonable parameters estimates from a random weighted combination of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63mvfrOoOzc5"
   },
   "outputs": [],
   "source": [
    "def initialize_posteriors(dataset, num_states, seed=0):\n",
    "    rng = npr.RandomState(seed)\n",
    "    posteriors = []\n",
    "    for data in dataset:\n",
    "        expected_states = rng.rand(len(data[\"data\"]), num_states)\n",
    "        expected_states /= expected_states.sum(axis=1, keepdims=True)\n",
    "        posteriors.append(dict(expected_states=expected_states,\n",
    "                               marginal_ll=-np.inf))\n",
    "    return posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ci6teNka_OvN"
   },
   "source": [
    "# Part 2: Gaussian HMM\n",
    "\n",
    "First we'll implement a hidden Markov model (HMM) with Gaussian observations. This is the same model we studied in class,\n",
    "\\begin{align}\n",
    "p(x, z \\mid \\Theta) &= \\mathrm{Cat}(z_1 \\mid \\pi) \\prod_{t=2}^{T} \\mathrm{Cat}(z_t \\mid P_{z_{t-1}}) \\prod_{t=1}^T \\mathcal{N}(x_t \\mid b_{z_t}, Q_{z_t})\n",
    "\\end{align}\n",
    "with parameters $\\Theta = \\pi, P, \\{b_k, Q_k\\}_{k=1}^K$. The observed datapoints are $x_t \\in \\mathbb{R}^{D}$ and the latent states are $z_t \\in \\{1,\\ldots, K\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuFgp3VPZaqT"
   },
   "source": [
    "## Problem 2a: Write a Gaussian Observations object\n",
    "\n",
    "We'll write a `GaussianObservations` class to wrap the parameters and implement key functions for EM. \n",
    "\n",
    "Most important, `M_step` solves for the optimal parameters given the expected sufficient statistics $N_k, \\bar{\\psi}_{k,1}, \\bar{\\psi}_{k,2}, \\bar{\\psi}_{k,3}$, which are passed in as a tuple. The new parameters are written to the class variables `self.means` and `self.covs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAETHLluZjoJ"
   },
   "outputs": [],
   "source": [
    "class GaussianObservations(object):\n",
    "    \"\"\"\n",
    "    Wrapper for a collection of Gaussian observation parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_states, data_dim):\n",
    "        \"\"\"\n",
    "        Initialize a collection of observation parameters for a Gaussian HMM\n",
    "        with `num_states` (i.e. K) discrete states and `data_dim` (i.e. D)\n",
    "        dimensional observations.\n",
    "        \"\"\"\n",
    "        self.num_states = num_states\n",
    "        self.data_dim = data_dim\n",
    "        self.means = np.zeros((num_states, data_dim))\n",
    "        self.covs = np.tile(np.eye(data_dim), (num_states, 1, 1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def precompute_suff_stats(dataset):\n",
    "        \"\"\"\n",
    "        Compute the sufficient statistics of the Gaussian distribution for each\n",
    "        data dictionary in the dataset. This modifies the dataset in place.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: a list of data dictionaries.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing, but the dataset is updated in place to have a new `suff_stats`\n",
    "            key, which contains a tuple of sufficient statistics.\n",
    "        \"\"\"\n",
    "        for data in dataset:\n",
    "            x = data['data']\n",
    "            data['suff_stats'] = (np.ones(len(x)),                  # 1\n",
    "                                  np.einsum('ti,tj->tij', x, x),    # x_t x_t^T\n",
    "                                  x,                                # x_t\n",
    "                                  np.ones(len(x)))                  # 1\n",
    "       \n",
    "    def log_likelihoods(self, data):\n",
    "        \"\"\"\n",
    "        Compute the matrix of log likelihoods of data for each state.\n",
    "        (I like to use torch.distributions for this, though it requires\n",
    "         converting back and forth between numpy arrays and pytorch tensors.)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: a dictionary with multiple keys, including \"data\", the TxD array\n",
    "            of observations for this mouse.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        log_likes: a TxK array of log likelihoods for each datapoint and \n",
    "            discrete state.\n",
    "        \"\"\"\n",
    "        x = to_t(data[\"data\"])\n",
    "        means = to_t(self.means)\n",
    "        covs = to_t(self.covs)\n",
    "\n",
    "        ###\n",
    "        # YOUR CODE BELOW\n",
    "        #\n",
    "        log_likes = ...\n",
    "        #\n",
    "        ###\n",
    "        return log_likes\n",
    "\n",
    "    def M_step(self, stats):\n",
    "        \"\"\"\n",
    "        Compute the Gaussian parameters give the expected sufficient statistics.\n",
    "\n",
    "        Note: add a little bit (1e-4 * I) to the diagonal of each covariance \n",
    "            matrix to ensure that the result is positive definite.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stats: a tuple of expected sufficient statistics\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing, but self.means and self.covs are updated in place.\n",
    "        \"\"\"\n",
    "        Ns, psi1, psi2, psi3 = stats\n",
    "\n",
    "        ###\n",
    "        # YOUR CODE BELOW\n",
    "        ...\n",
    "        #\n",
    "        ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpI6TY-awDnq"
   },
   "outputs": [],
   "source": [
    "def test_gaussian(data_dim=3, seed=0):\n",
    "    rng = npr.RandomState(seed)\n",
    "    data1 = rng.randn(100, data_dim)\n",
    "    data2 = rng.randn(100, data_dim)\n",
    "    data = np.vstack([data1, data2])\n",
    "    num_states = 2\n",
    "\n",
    "    obs = GaussianObservations(num_states, data_dim)\n",
    "    lls = obs.log_likelihoods(dict(data=data))\n",
    "    assert isinstance(lls, np.ndarray)\n",
    "    assert lls.shape == (200, num_states)\n",
    "    assert np.all(np.isfinite(lls))\n",
    "    assert np.isclose(lls.mean(), -4.27187, atol=1e-5)\n",
    "    assert np.isclose(lls.std(), 1.21605, atol=1e-5)\n",
    "\n",
    "    stats = [None, None, None, None]\n",
    "    stats[0] = np.array([100, 100])\n",
    "    stats[1] = np.array([data1.T @ data1, data2.T @ data2])\n",
    "    stats[2] = np.array([data1.sum(0), data2.sum(0)])\n",
    "    stats[3] = np.array([100, 100])\n",
    "\n",
    "    obs.M_step(stats)\n",
    "    assert np.allclose(obs.means[0], data1.mean(axis=0))\n",
    "    assert np.allclose(obs.means[1], data2.mean(axis=0))\n",
    "    assert np.allclose(obs.covs[0], np.cov(data1, rowvar=False, bias=True) \\\n",
    "                        + 1e-4 * np.eye(data_dim), atol=1e-5)\n",
    "    assert np.allclose(obs.covs[1], np.cov(data2, rowvar=False, bias=True) \\\n",
    "                        + 1e-4 * np.eye(data_dim), atol=1e-5)\n",
    "\n",
    "test_gaussian()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnistxraj6Gb"
   },
   "source": [
    "## Precompute the sufficient statistics\n",
    "\n",
    "This updates the datasets in place with the sufficient statistics for a Gaussian distribution.  The statistics are placed in the `suff_stats` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuZCnou3Rs4A"
   },
   "outputs": [],
   "source": [
    "GaussianObservations.precompute_suff_stats(train_dataset)\n",
    "GaussianObservations.precompute_suff_stats(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp_VBMYGRlEw"
   },
   "source": [
    "## Problem 2b: Write a function to compute expected sufficient statistics\n",
    "\n",
    "Let $M$ denote the number of mice in the dataset and let $x_t^{(m)}$ and $z_t^{(m)}$ denote their data and discrete states, respectively. Compute $\\bar{\\psi}_{k,j} = \\sum_{m=1}^M \\sum_{t=1}^{T_m} q(z_{t}^{(m)} = k) \\, f_j(x_t^{(m)})$ for each of the $k$ states and $j$ sufficient statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c180IXBbC_Gp"
   },
   "outputs": [],
   "source": [
    "def compute_expected_suff_stats(dataset, posteriors):\n",
    "    \"\"\"\n",
    "    Compute a tuple of expected sufficient statistics, taking a weighted sum\n",
    "    of the posterior expected states and the sufficient statistics and combining\n",
    "    across all mice (i.e. all the data dictionaries and posterior dictionaries).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: a list of dictionary with multiple keys, including \"data\", the TxD \n",
    "        array of observations for this mouse, and \"suff_stats\", the tuple of \n",
    "        sufficient statistics.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stats: a tuple of weighted sums of sufficient statistics. E.g. if the \n",
    "        \"suff_stats\" key has four arrays, the stats tuple should have four\n",
    "        entires as well. Each entry should be a K x (size of statistic) array\n",
    "        with the expected sufficient statistics for each of the K discrete\n",
    "        states.\n",
    "    \"\"\"\n",
    "    assert isinstance(dataset, list)\n",
    "    assert isinstance(posteriors, list)\n",
    "\n",
    "    # Helper function to compute expected counts and sufficient statistics\n",
    "    # for a single time series and corresponding posterior.\n",
    "    def _compute_expected_suff_stats(data, posterior):\n",
    "        ###\n",
    "        # YOUR CODE BELOW\n",
    "        # Hint: einsum might be useful\n",
    "        q = posterior['expected_states']\n",
    "        stats = ...\n",
    "        #\n",
    "        ###\n",
    "        return stats\n",
    "\n",
    "    # Sum the expected stats over the whole dataset\n",
    "    stats = None\n",
    "    for data, posterior in zip(dataset, posteriors):\n",
    "        these_stats = _compute_expected_suff_stats(data, posterior)\n",
    "        stats = sum_tuples(stats, these_stats)\n",
    "    return stats\n",
    "\n",
    "def test_expec_suff_stats():\n",
    "    rng = npr.RandomState(0)\n",
    "    data = train_dataset[0]\n",
    "    data_dim = data['data'].shape[1]\n",
    "    num_states = 10\n",
    "\n",
    "    # make a random \"posterior\"\n",
    "    q = rng.rand(len(data['data']), num_states)\n",
    "    q /= q.sum(axis=1, keepdims=True)\n",
    "    posterior = dict(expected_states=q)\n",
    "\n",
    "    # call your function\n",
    "    stats = compute_expected_suff_stats([data], [posterior])\n",
    "\n",
    "    # Check that you have the right output shape\n",
    "    assert len(stats) == 4\n",
    "    assert stats[0].shape == (num_states,)\n",
    "    assert stats[1].shape == (num_states, data_dim, data_dim)\n",
    "    assert stats[2].shape == (num_states, data_dim)\n",
    "    assert stats[3].shape == (num_states,)\n",
    "\n",
    "    # Check that we got the same answer\n",
    "    assert np.allclose(stats[0].std(), 8.20117, atol=1e-5)\n",
    "    assert np.allclose(stats[1].std(), 1117.14717, atol=1e-5)\n",
    "    assert np.allclose(stats[2].std(), 746.24645, atol=1e-4)\n",
    "    assert np.allclose(stats[3], stats[0])\n",
    "\n",
    "test_expec_suff_stats() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jcxyq4Mse4q"
   },
   "source": [
    "## Problem 2c: Write a function to fit an HMM with EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EAnR_Aqvn4iH"
   },
   "outputs": [],
   "source": [
    "def fit_hmm(train_dataset, \n",
    "            test_dataset,\n",
    "            initial_dist,\n",
    "            transition_matrix,\n",
    "            observations,\n",
    "            seed=0,\n",
    "            num_iters=50):\n",
    "    \"\"\"\n",
    "    Fit a Hidden Markov Model (HMM) with expectation maximization (EM). \n",
    "    \n",
    "    Note: This is only a partial fit, as this method will treat the initial \n",
    "    state distribution and the transition matrix as fixed!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_dataset: a list of dictionary with multiple keys, including \"data\", \n",
    "        the TxD array of observations for this mouse, and \"suff_stats\", the \n",
    "        tuple of sufficient statistics.\n",
    "\n",
    "    test_dataset: as above but only used for tracking the test log likelihood\n",
    "        during training.\n",
    "\n",
    "    initial_dist: a length-K vector giving the initial state distribution.\n",
    "\n",
    "    transition_matrix: a K x K matrix whose rows sum to 1.\n",
    "\n",
    "    observations: an Observations object with `log_likelihoods` and `M_step` \n",
    "        functions.\n",
    "\n",
    "    seed: random seed for initializing the algorithm.\n",
    "\n",
    "    num_iters: number of EM iterations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_lls: array of likelihoods of training data over EM iterations\n",
    "    test_lls: array of likelihoods of testing data over EM iterations\n",
    "    posteriors: final list of posterior distributions for the training data\n",
    "    test_posteriors: final list of posterior distributions for the test data\n",
    "    \"\"\"\n",
    "    # Get some constants\n",
    "    num_states = observations.num_states\n",
    "    num_train = sum([len(data[\"data\"]) for data in train_dataset])\n",
    "    num_test = sum([len(data[\"data\"]) for data in test_dataset])\n",
    "\n",
    "    # Check the initial distribution and transition matrix\n",
    "    assert initial_dist.shape  == (num_states,) and \\\n",
    "        np.all(initial_dist >= 0) and \\\n",
    "        np.isclose(initial_dist.sum(), 1.0)\n",
    "    assert transition_matrix.shape  == (num_states, num_states) and \\\n",
    "        np.all(transition_matrix >= 0) and \\\n",
    "        np.allclose(transition_matrix.sum(axis=1), 1.0)\n",
    "    \n",
    "    # Initialize with a random posterior\n",
    "    posteriors = initialize_posteriors(train_dataset, num_states, seed=seed)\n",
    "    stats = compute_expected_suff_stats(train_dataset, posteriors)\n",
    "    \n",
    "    # Track the marginal log likelihood of the train and test data\n",
    "    train_lls = []\n",
    "    test_lls = []\n",
    "\n",
    "    # Main loop\n",
    "    for itr in trange(num_iters):\n",
    "        ###\n",
    "        # YOUR CODE BELOW\n",
    "        #\n",
    "        \n",
    "        # M step: update the parameters of the observations using the \n",
    "        #         expected sufficient stats.\n",
    "        ...\n",
    "        \n",
    "        # E step: computhe the posterior for each data dictionary in the dataset\n",
    "        posteriors = ...\n",
    "\n",
    "        # Compute the expected sufficient statistics under the new posteriors\n",
    "        stats = ...\n",
    "\n",
    "        # Store the average train likelihood\n",
    "        avg_train_ll = sum([p[\"marginal_ll\"] for p in posteriors]) / num_train\n",
    "        train_lls.append(avg_train_ll)\n",
    "\n",
    "        # Compute the posteriors for the test dataset too\n",
    "        test_posteriors = ...\n",
    "\n",
    "        # Store the average test likelihood\n",
    "        avg_test_ll = sum([p[\"marginal_ll\"] for p in test_posteriors]) / num_test\n",
    "        test_lls.append(avg_test_ll)\n",
    "        \n",
    "        #\n",
    "        ###\n",
    "        \n",
    "    # convert lls to arrays\n",
    "    train_lls = np.array(train_lls)\n",
    "    test_lls = np.array(test_lls)\n",
    "    return train_lls, test_lls, posteriors, test_posteriors\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zW7ZYBgxGS-p"
   },
   "source": [
    "## Fit it! (Just to one mouse for now...)\n",
    "\n",
    "We'll just use the first mouse's data for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_etTe3wFUPi8"
   },
   "outputs": [],
   "source": [
    "# Build the HMM\n",
    "num_states = 50\n",
    "initial_dist = np.ones(num_states) / num_states\n",
    "transition_matrix = sticky_transitions(num_states, stickiness=0.95)\n",
    "observations = GaussianObservations(num_states, data_dim)\n",
    "\n",
    "# Fit the HMM with EM\n",
    "train_lls, test_lls, train_posteriors, test_posteriors, = \\\n",
    "    fit_hmm(train_dataset[:1], \n",
    "            test_dataset[:1], \n",
    "            initial_dist,\n",
    "            transition_matrix,\n",
    "            observations)    \n",
    "\n",
    "plt.plot(train_lls, label=\"train\")\n",
    "plt.plot(test_lls, '-r', label=\"test\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"avg marginal log lkhd\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScbebU3oLUzC"
   },
   "source": [
    "## Plot the data and the inferred latent states\n",
    "\n",
    "We'll make the same plot as above (in the warm-up) but using our inferred states instead. Hopefully, the states seem to switch along with changes in the data. \n",
    "\n",
    "**Note**: We're showing the state with the highest marginal probability, $z_t^\\star = \\mathrm{arg} \\, \\mathrm{max}_k \\; q(z_t = k)$. This is different from the most likely state path, $z_{1:T}^\\star = \\mathrm{arg}\\,\\mathrm{max} \\; q(z)$. We could compute the latter with the Viterbi algorithm, which is similar to the forward-backward algorithm you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6P_x1S8kUsNc"
   },
   "outputs": [],
   "source": [
    "ghmm_states = train_posteriors[0][\"expected_states\"].argmax(1)\n",
    "plot_data_and_states(train_dataset[0], ghmm_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XU6NM39HYq5v"
   },
   "source": [
    "## Plot the state usage histogram\n",
    "\n",
    "The state usage histogram shows how often each discrete state was used under the posterior distribution. You'll probably see a long tail of states with non-trivial usage (hundreds of frames), all the way out to state 50. That suggests the model is using all its available capacity, and we could probably crank the number of states up even further for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeYeBbLZEzNV"
   },
   "outputs": [],
   "source": [
    "# Sort states by usage\n",
    "ghmm_usage = np.bincount(ghmm_states, minlength=num_states)\n",
    "ghmm_order = np.argsort(ghmm_usage)[::-1]\n",
    "\n",
    "plt.bar(np.arange(num_states), ghmm_usage[ghmm_order])\n",
    "plt.xlabel(\"state index [ordered]\")\n",
    "plt.ylabel(\"num frames\")\n",
    "plt.title(\"histogram of inferred state usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Rp1ttRSm8Zo"
   },
   "source": [
    "## Make \"crowd\" movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boOP-a4x7BA4"
   },
   "outputs": [],
   "source": [
    "play(make_crowd_movie(ghmm_order[0], test_dataset, test_posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gqx2zYX7OmL"
   },
   "outputs": [],
   "source": [
    "play(make_crowd_movie(ghmm_order[1], test_dataset, test_posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCO5xHcCFkxE"
   },
   "outputs": [],
   "source": [
    "play(make_crowd_movie(ghmm_order[2], test_dataset, test_posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dr04KiKDFuvv"
   },
   "outputs": [],
   "source": [
    "play(make_crowd_movie(ghmm_order[3], test_dataset, test_posteriors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHwj7CNgnbTl"
   },
   "source": [
    "## Download crowd movies for each state\n",
    "\n",
    "I've commented this out because it takes about ten minutes. Please do it once though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vvlr6DgjnfXT"
   },
   "outputs": [],
   "source": [
    "# # Make \"crowd\" movies for each state and save them to disk\n",
    "# # Then you can download them and play them offline\n",
    "# for i in trange(num_states):\n",
    "#     try:\n",
    "#         play(make_crowd_movie(ghmm_order[i], test_dataset, test_posteriors),\n",
    "#             filename=\"gaussian_hmm_crowd_{}.mp4\".format(i), show=False)\n",
    "#     except:\n",
    "#         print(\"failed to create a movie for state\", ghmm_order[i])\n",
    "\n",
    "# # Zip the movies up    \n",
    "# !zip gaussian_crowd_movies.zip gaussian_hmm_crowd_*.mp4\n",
    "\n",
    "# # Download the file\n",
    "# files.download(\"gaussian_crowd_movies.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEKMGAv8lSkK"
   },
   "source": [
    "## Problem 2d [Short Answer]: Hyperparameter selection\n",
    "\n",
    "The results above give some qualitative reason to believe the model is \"working\" reasonably. However, there are a few knobs that we had to set. What are they? How could you try to set them in a more principled manner? Is there even a \"right\" setting of them?\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "od7nJS8yH9z4"
   },
   "source": [
    "# Part 3: Autoregressive HMMs\n",
    "\n",
    "Autoregressive hidden Markov models (ARHMMs) replace the Gaussian observations with an AR model:\n",
    "\\begin{align}\n",
    "p(x, z \\mid \\Theta) &= \\mathrm{Cat}(z_1 \\mid \\pi) \\prod_{t=2}^{T} \\mathrm{Cat}(z_t \\mid P_{z_{t-1}}) \\prod_{t=1}^T p(x_t \\mid x_{1:t-1}, z_t)\n",
    "\\end{align}\n",
    "The model is \"autoregressive\" because $x_t$ depends not only on $z_t$ but on $x_{1:t-1}$ as well. The precise form of this dependence varies; here we will consider linear Gaussian dependencies on only the most recent $G$ timesteps,:\n",
    "\\begin{align}\n",
    "p(x_t \\mid x_{1:t-1}, z_t) &= \\mathcal{N}\\left(\\sum_{g=1}^G A_{z_t,g} x_{t-g} + b_{z_t,g}, Q_{z_t} \\right)  \n",
    "\\end{align}\n",
    "\n",
    "The new parameters are $\\Theta = \\pi, P, \\{\\{A_{k,g}, b_{k,g}\\}_{g=1}^G, Q_k\\}_{k=1}^K$, which include weights $A_{k,g} \\in \\mathbb{R}^{D \\times D}$ for each of the $K$ states and the $G$ lags, and a bias vector $b_k \\in \\mathbb{R}^D$.\n",
    "\n",
    "Note that we can write this as a simple **linear regression**,\n",
    "\\begin{align}\n",
    "p(x_t \\mid x_{1:t-1}, z_t) &= \\mathcal{N}\\left(W_k \\phi_t , Q_{z_t} \\right)  \n",
    "\\end{align}\n",
    "where $\\phi_t = (x_{t-1}, \\ldots, x_{t-G}, 1) \\in \\mathbb{R}^{GD +1}$ is a vector of covariates (aka features) that includes the past $G$ time steps along with a 1 for the bias term.\n",
    "\\begin{align}\n",
    "W_k = \\begin{bmatrix} A_{k,1}  & A_{k,2} & \\ldots & A_{kg} & b_k \\end{bmatrix}\n",
    "\\in \\mathbb{R}^{D \\times GD + 1}\n",
    "\\end{align}\n",
    "is a block matrix of the autoregressive weights and the bias. \n",
    "\n",
    "_Note that the covariates are fixed functions of the data so we can precompute them, if we know the number of lags $G$._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPvxGpRCXiJL"
   },
   "source": [
    "## Problem 3a [Math]: Derive the natural parameters and sufficient statistics for a linear regression\n",
    "\n",
    "Expand the expected log likelihood of a linear regression model in terms of $W_k$ and $b_k$,\n",
    "\\begin{align}\n",
    "\\mathcal{J}(W_k, b_k) \\triangleq \\mathbb{E}_{q(z)}\\left[ \\sum_{t=1}^T \\mathbb{I}[z_t=k] \\cdot \\log \\mathcal{N}(x_t \\mid W_k \\phi_t, Q_k) \\right].\n",
    "\\end{align}\n",
    "Write it as a sum of inner products between natural parameters (i.e. functions of $W_k$ and $Q_k$) and expected sufficient statistics $N_k$, $\\bar{\\psi}_{k,1}$, $\\bar{\\psi}_{k,2}$, $\\bar{\\psi}_{k,3}$ (i.e. functions of $q$, $x$ and $\\phi$). \n",
    "\n",
    "**Hint**: The form should look similar to that of the Gaussian distribution we derived in class, except that $\\bar{\\psi}_3$ now depends on $x$ and/or $\\phi$.\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "il1EtQius9Xq"
   },
   "source": [
    "## Problem 3b [Math]: Solve for the optimal linear regression parameters given expected sufficient statistics\n",
    "\n",
    "Solve for $W_k^\\star, Q_k^\\star$ that maximize the objective above in terms of the expected sufficient statistics $N_k$, $\\bar{\\psi}_{k,1}$, $\\bar{\\psi}_{k,2}$, $\\bar{\\psi}_{k,3}$.\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98bDuGZWXpqk"
   },
   "source": [
    "## Problem 3c: Implement an Linear Regression Observations object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MPZmnpNIDQe"
   },
   "outputs": [],
   "source": [
    "class LinearRegressionObservations(object):\n",
    "    \"\"\"\n",
    "    Wrapper for a collection of Gaussian observation parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_states, data_dim, covariate_dim):\n",
    "        \"\"\"\n",
    "        Initialize a collection of observation parameters for an HMM whose \n",
    "        observation distributions are linear regressions. The HMM has \n",
    "        `num_states` (i.e. K) discrete states, `data_dim` (i.e. D)\n",
    "        dimensional observations, and `covariate_dim` covariates. \n",
    "        In an ARHMM, the covariates will be functions of the past data.\n",
    "        \"\"\"\n",
    "        self.num_states = num_states\n",
    "        self.data_dim = data_dim\n",
    "        self.covariate_dim = covariate_dim\n",
    "\n",
    "        # Initialize the model parameters\n",
    "        self.weights = np.zeros((num_states, data_dim, covariate_dim))\n",
    "        self.covs = np.tile(np.eye(data_dim), (num_states, 1, 1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def precompute_suff_stats(dataset):\n",
    "        \"\"\"\n",
    "        Compute the sufficient statistics of the linear regression for each\n",
    "        data dictionary in the dataset. This modifies the dataset in place.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: a list of data dictionaries.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing, but the dataset is updated in place to have a new `suff_stats`\n",
    "            key, which contains a tuple of sufficient statistics.\n",
    "        \"\"\"\n",
    "        ### \n",
    "        # YOUR CODE BELOW\n",
    "        #\n",
    "        ...\n",
    "        #\n",
    "        ###\n",
    "\n",
    "    def log_likelihoods(self, data):\n",
    "        \"\"\"\n",
    "        Compute the matrix of log likelihoods of data for each state.\n",
    "        (I like to use torch.distributions for this, though it requires\n",
    "         converting back and forth between numpy arrays and pytorch tensors.)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: a dictionary with multiple keys, including \"data\", the TxD array\n",
    "            of observations for this mouse.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        log_likes: a TxK array of log likelihoods for each datapoint and \n",
    "            discrete state.\n",
    "        \"\"\"\n",
    "        ### \n",
    "        # YOUR CODE BELOW\n",
    "        #\n",
    "        log_likes = ...\n",
    "        #\n",
    "        ###\n",
    "        return log_likes\n",
    "       \n",
    "    def M_step(self, stats):\n",
    "        \"\"\"\n",
    "        Compute the linear regression parameters given the expected \n",
    "        sufficient statistics.\n",
    "\n",
    "        Note: add a little bit (1e-4 * I) to the diagonal of each covariance \n",
    "            matrix to ensure that the result is positive definite.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stats: a tuple of expected sufficient statistics\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing, but self.weights and self.covs are updated in place.\n",
    "        \"\"\"\n",
    "        Ns, psi1, psi2, psi3 = stats\n",
    "        ###\n",
    "        # YOUR CODE BELOW\n",
    "        ...\n",
    "        #\n",
    "        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXYK0yIEZdvC"
   },
   "source": [
    "## Problem 3d: Precompute the covariates for an autoregressive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2poyS8iZaz7"
   },
   "outputs": [],
   "source": [
    "def precompute_ar_covariates(dataset, \n",
    "                             num_lags=2, \n",
    "                             fit_intercept=True):\n",
    "    for data in dataset:\n",
    "        x = data[\"data\"]\n",
    "        data_dim = x.shape[1]\n",
    "\n",
    "        ###\n",
    "        # YOUR CODE BELOW\n",
    "        data[\"covariates\"] = ...\n",
    "        #\n",
    "        ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B16bSpEbwQvC"
   },
   "source": [
    "## Precompute the AR features and the sufficient statistics\n",
    "\n",
    "We'll store them in memory for convenience, even though it's redundant and memory intensive. \n",
    "\n",
    "**Note: avoid running this cell multiple times as it will cause you to run out of memory and crash the notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKapdY2jbPNy"
   },
   "outputs": [],
   "source": [
    "# First compute the autoregression covariates\n",
    "num_lags = 2\n",
    "precompute_ar_covariates(train_dataset, num_lags=2)\n",
    "precompute_ar_covariates(test_dataset, num_lags=2)\n",
    "\n",
    "# Then precompute the sufficient statistics\n",
    "LinearRegressionObservations.precompute_suff_stats(train_dataset)\n",
    "LinearRegressionObservations.precompute_suff_stats(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTUEkd48TmLV"
   },
   "source": [
    "## Fit it!\n",
    "\n",
    "We can use the same `fit_hmm` function you wrote above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8R9_B16bRM7"
   },
   "outputs": [],
   "source": [
    "# Build the HMM\n",
    "num_states = 50\n",
    "initial_dist = np.ones(num_states) / num_states\n",
    "transition_matrix = sticky_transitions(num_states, stickiness=0.95)\n",
    "observations = LinearRegressionObservations(num_states, data_dim, \n",
    "                                            num_lags * data_dim + 1)\n",
    "\n",
    "# Fit it!\n",
    "train_lls, test_lls, train_posteriors, test_posteriors, = \\\n",
    "    fit_hmm(train_dataset[:1], \n",
    "            test_dataset[:1],\n",
    "            initial_dist,\n",
    "            transition_matrix,\n",
    "            observations)\n",
    "\n",
    "plt.plot(train_lls, label=\"train\")\n",
    "plt.plot(test_lls, '-r', label=\"test\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"avg marginal log lkhd\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqzJJDWUPCcz"
   },
   "source": [
    "## Plot the data and the inferred states\n",
    "\n",
    "\n",
    "We'll make the same plot as above (in the warm-up) but using our inferred states instead. Hopefully, the states seem to switch along with changes in the data. \n",
    "\n",
    "**Note**: We're showing the state with the highest marginal probability, $z_t^\\star = \\mathrm{arg} \\, \\mathrm{max}_k \\; q(z_t = k)$. This is different from the most likely state path, $z_{1:T}^\\star = \\mathrm{arg}\\,\\mathrm{max} \\; q(z)$. We could compute the latter with the Viterbi algorithm, which is similar to the forward-backward algorithm you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DfaK78a7lz7U"
   },
   "outputs": [],
   "source": [
    "arhmm_states = train_posteriors[0][\"expected_states\"].argmax(1)\n",
    "plot_data_and_states(train_dataset[0], arhmm_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVPESoqtPFA1"
   },
   "source": [
    "## Plot the state usage histogram\n",
    "\n",
    "\n",
    "The state usage histogram shows how often each discrete state was used under the posterior distribution. You'll probably see a long tail of states with non-trivial usage (hundreds of frames), all the way out to state 50. That suggests the model is using all its available capacity, and we could probably crank the number of states up even further for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENEZ70G3HTEV"
   },
   "outputs": [],
   "source": [
    "# Sort states by usage\n",
    "arhmm_usage = np.bincount(arhmm_states, minlength=num_states)\n",
    "arhmm_order = np.argsort(arhmm_usage)[::-1]\n",
    "\n",
    "plt.bar(np.arange(num_states), arhmm_usage[arhmm_order])\n",
    "plt.xlabel(\"state index [ordered]\")\n",
    "plt.ylabel(\"num frames\")\n",
    "plt.title(\"histogram of inferred state usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haiNm-SRPIeD"
   },
   "source": [
    "## Plot some \"crowd\" movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTaoUpdIsajB"
   },
   "outputs": [],
   "source": [
    "play(make_crowd_movie(arhmm_order[0], test_dataset, test_posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iR4TyPgAsEJ9"
   },
   "outputs": [],
   "source": [
    "play(make_crowd_movie(arhmm_order[1], test_dataset, test_posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oprWlKZ9LoFl"
   },
   "outputs": [],
   "source": [
    "play(make_crowd_movie(arhmm_order[2], test_dataset, test_posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9TN-4XrLrqX"
   },
   "outputs": [],
   "source": [
    "play(make_crowd_movie(arhmm_order[3], test_dataset, test_posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DE4XcUSdLvPP"
   },
   "outputs": [],
   "source": [
    "play(make_crowd_movie(arhmm_order[4], test_dataset, test_posteriors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLSFWpR8mscG"
   },
   "source": [
    "## Download crowd movies for each state\n",
    "\n",
    "I've commented this out because it takes about ten minutes. Please do it once though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cd0GA1FbL0gh"
   },
   "outputs": [],
   "source": [
    "# # Make \"crowd\" movies for each state and save them to disk\n",
    "# # Then you can download them and play them offline\n",
    "# for i in trange(num_states):\n",
    "#     try:\n",
    "#         play(make_crowd_movie(arhmm_order[i], test_dataset, test_posteriors),\n",
    "#             filename=\"arhmm_crowd_{}.mp4\".format(i), show=False)\n",
    "#     except:\n",
    "#         print(\"Failed to create movie for state \", arhmm_order[i])\n",
    "\n",
    "# # Zip the movies up    \n",
    "# !zip arhmm_crowd_movies.zip arhmm_crowd_*.mp4\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download(\"arhmm_crowd_movies.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC4nYDgehH4o"
   },
   "source": [
    "## Problem 3e [Short Answer]: Compare the Gaussian HMM and ARHMM state usage\n",
    "\n",
    "Below we've plotted the cumulative sum of frames as a function of ordered state index for both the Gaussian HMM and the ARHMM. What does the difference between these two curves tell you about how the two models segment the data?\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1xrfB-lhPxw"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(ghmm_usage[ghmm_order]), label=\"Gaussian HMM\")\n",
    "plt.plot(np.cumsum(arhmm_usage[arhmm_order]), label=\"ARHMM\")\n",
    "plt.plot([0, num_states - 1], [0, np.sum(ghmm_usage)], ':k', label=\"uniform\")\n",
    "plt.xlabel(\"state index [ordered]\")\n",
    "plt.xlim([0, num_states - 1])\n",
    "plt.ylabel(\"cumulative num frames\")\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXbBFU1bPcA2"
   },
   "source": [
    "# Part 4: Fitting ARHMMs with Stochastic EM\n",
    "\n",
    "The EM algorithm above sweeps through the entire dataset during each E-step. In this part, you'll implement a stochastic EM algorithm that performs an E-step on one minibatch of data at a time and keeps a rolling average of the expected sufficient statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhN5K83EjgZo"
   },
   "source": [
    "## Problem 4a: Adapt your EM code to run stochastic EM instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c3Q4iCwMrct"
   },
   "outputs": [],
   "source": [
    "def fit_hmm_stoch_em(train_dataset, \n",
    "                     test_dataset,\n",
    "                     initial_dist,\n",
    "                     transition_matrix,\n",
    "                     observations,\n",
    "                     seed=0,\n",
    "                     num_epochs=5,\n",
    "                     forgetting_rate=0.5):\n",
    "    \"\"\"\n",
    "    Fit a Hidden Markov Model (HMM) with expectation maximization (EM). \n",
    "    \n",
    "    Note: This is only a partial fit, as this method will treat the initial \n",
    "    state distribution and the transition matrix as fixed!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_dataset: a list of dictionary with multiple keys, including \"data\", \n",
    "        the TxD array of observations for this mouse, and \"suff_stats\", the \n",
    "        tuple of sufficient statistics.\n",
    "\n",
    "    test_dataset: as above but only used for tracking the test log likelihood\n",
    "        during training.\n",
    "\n",
    "    initial_dist: a length-K vector giving the initial state distribution.\n",
    "\n",
    "    transition_matrix: a K x K matrix whose rows sum to 1.\n",
    "\n",
    "    observations: an Observations object with `log_likelihoods` and `M_step` \n",
    "        functions.\n",
    "\n",
    "    seed: random seed for initializing the algorithm.\n",
    "\n",
    "    num_iters: number of EM iterations.\n",
    "\n",
    "    forgetting_rate: number > 0 that controls how quickly the step size decays.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_lls: array of likelihoods of training data over EM iterations\n",
    "    test_lls: array of likelihoods of testing data over EM iterations\n",
    "    posteriors: final list of posterior distributions for the training data\n",
    "    test_posteriors: final list of posterior distributions for the test data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get some constants\n",
    "    num_batches = len(train_dataset)\n",
    "    num_states = observations.num_states\n",
    "    num_train = sum([len(data[\"data\"]) for data in train_dataset])\n",
    "    num_test = sum([len(data[\"data\"]) for data in test_dataset])\n",
    "    \n",
    "    # Initialize the step size schedule\n",
    "    schedule = np.arange(1, 1 + num_batches * num_epochs)**(-0.5)\n",
    "    \n",
    "    # Initialize progress bars\n",
    "    outer_pbar = trange(num_epochs)\n",
    "    inner_pbar = trange(num_batches)\n",
    "    outer_pbar.set_description(\"Epoch\")\n",
    "    inner_pbar.set_description(\"Batch\")\n",
    "\n",
    "    # Initialize with a random posterior on the first batch\n",
    "    posteriors = initialize_posteriors(train_dataset[:1], num_states, seed=seed)\n",
    "    stats = compute_expected_suff_stats(train_dataset[:1], posteriors[:1])\n",
    "        \n",
    "    # Main loop\n",
    "    rng = npr.RandomState(seed)\n",
    "    train_lls = []\n",
    "    test_lls = []\n",
    "    for epoch in range(num_epochs):\n",
    "        perm = rng.permutation(num_batches)\n",
    "\n",
    "        inner_pbar.reset()\n",
    "        for itr in range(num_batches):\n",
    "            minibatch = [train_dataset[perm[itr]]]\n",
    "            this_num_train = len(minibatch[0][\"data\"])\n",
    "\n",
    "            ###\n",
    "            # YOUR CODE BELOW\n",
    "            #\n",
    "\n",
    "            # M step: using current stats\n",
    "            ...\n",
    "            \n",
    "            # E step: on this minibatch\n",
    "            posteriors = ...\n",
    "\n",
    "            # Compute sufficient statistics for this batch            \n",
    "            these_stats = ...\n",
    "\n",
    "            # Rescale the statistics as if they came from the whole dataset\n",
    "            rescaled_stats = ...\n",
    "            \n",
    "            # Take a convex combination of the statistics using current step sz\n",
    "            stepsize = schedule[epoch * num_batches + itr]\n",
    "            stats = ...\n",
    "            \n",
    "            #\n",
    "            ###\n",
    "            \n",
    "            # Store the normalized log likelihood for this minibatch\n",
    "            avg_mll = sum([p[\"marginal_ll\"] for p in posteriors]) / this_num_train\n",
    "            train_lls.append(avg_mll)\n",
    "            inner_pbar.set_description(\"Batch LL: {:.3f}\".format(avg_mll))\n",
    "            inner_pbar.update()\n",
    "            \n",
    "\n",
    "        ###\n",
    "        # Evaluate the likelihood and posteriors on the test dataset\n",
    "        # YOUR CODE BELOW\n",
    "        #\n",
    "        test_posteriors = ...\n",
    "        #\n",
    "        ###\n",
    "\n",
    "        avg_test_mll = sum([p[\"marginal_ll\"] for p in test_posteriors]) / num_test\n",
    "        test_lls.append(avg_test_mll)\n",
    "        outer_pbar.set_description(\"Test LL: {:.3f}\".format(avg_test_mll))\n",
    "        outer_pbar.update()\n",
    "\n",
    "    ###\n",
    "    # Finally, compute the posteriors for each training dataset\n",
    "    # YOUR CODE BELOW\n",
    "    print(\"Computing posteriors for the whole training dataset\")\n",
    "    posteriors = ...\n",
    "    #\n",
    "    ###\n",
    "\n",
    "    # convert lls to arrays\n",
    "    train_lls = np.array(train_lls)\n",
    "    test_lls = np.array(test_lls)\n",
    "\n",
    "    return train_lls, test_lls, posteriors, test_posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-uszBvsNcwU"
   },
   "outputs": [],
   "source": [
    "# Build the HMM\n",
    "num_states = 50\n",
    "initial_dist = np.ones(num_states) / num_states\n",
    "transition_matrix = sticky_transitions(num_states, stickiness=0.95)\n",
    "observations = LinearRegressionObservations(num_states, data_dim, \n",
    "                                            num_lags * data_dim + 1)\n",
    "\n",
    "# Fit it with stochastic EM\n",
    "train_lls, test_lls, train_posteriors, test_posteriors, = \\\n",
    "    fit_hmm_stoch_em(train_dataset, \n",
    "                     test_dataset,\n",
    "                     initial_dist,\n",
    "                     transition_matrix,\n",
    "                     observations)\n",
    "\n",
    "# Interpolate the test lls at each minibatch\n",
    "test_lls = np.concatenate([[np.nan], test_lls])\n",
    "test_lls = np.concatenate([\n",
    "    np.repeat(test_lls[:-1], len(train_dataset)), [test_lls[-1]]])\n",
    "\n",
    "plt.plot(train_lls, label=\"train\")\n",
    "plt.plot(test_lls, '-r', label=\"test\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"avg marginal log lkhd\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqtwsBTDdcwZ"
   },
   "source": [
    "## Plot the data and the inferred states\n",
    "\n",
    "\n",
    "We'll make the same plot as above (in the warm-up) but using our inferred states instead. Hopefully, the states seem to switch along with changes in the data. \n",
    "\n",
    "**Note**: We're showing the state with the highest marginal probability, $z_t^\\star = \\mathrm{arg} \\, \\mathrm{max}_k \\; q(z_t = k)$. This is different from the most likely state path, $z_{1:T}^\\star = \\mathrm{arg}\\,\\mathrm{max} \\; q(z)$. We could compute the latter with the Viterbi algorithm, which is similar to the forward-backward algorithm you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AbZjRxfOBLl"
   },
   "outputs": [],
   "source": [
    "plot_data_and_states(train_dataset[0], \n",
    "                     train_posteriors[0][\"expected_states\"].argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FON25DadYVw"
   },
   "source": [
    "## Plot the state usage histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Czu4pAiDbOKD"
   },
   "outputs": [],
   "source": [
    "# Sort states by usage\n",
    "usage = 0\n",
    "for posterior in train_posteriors:\n",
    "    states = np.argmax(posterior[\"expected_states\"], axis=1)\n",
    "    usage += np.bincount(states, minlength=num_states)\n",
    "order = np.argsort(usage)[::-1]\n",
    "\n",
    "plt.bar(np.arange(num_states), usage[order])\n",
    "plt.xlabel(\"state index [ordered]\")\n",
    "plt.ylabel(\"num frames\")\n",
    "plt.title(\"histogram of inferred state usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0NFIF-jdUkb"
   },
   "source": [
    "## Make \"crowd\" movies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oW6uqEeemvty"
   },
   "source": [
    "## Download crowd movies for each state\n",
    "\n",
    "I've commented this out because it takes about ten minutes. Please do it once though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmyYHWDGbP7v"
   },
   "outputs": [],
   "source": [
    "# # Make \"crowd\" movies for each state and save them to disk\n",
    "# # Then you can download them and play them offline\n",
    "# for i in trange(num_states):\n",
    "#     try:\n",
    "#         play(make_crowd_movie(order[i], test_dataset, test_posteriors),\n",
    "#             filename=\"arhmm_sem_crowd_{}.mp4\".format(i), show=False)\n",
    "#     except:\n",
    "#         print(\"Failed to create movie for state\", order)\n",
    "\n",
    "# # Zip the movies up    \n",
    "# !zip arhmm_sem_crowd_movies.zip arhmm_sem_crowd_*.mp4\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download(\"arhmm_sem_crowd_movies.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Akd_RPa3oD5M"
   },
   "source": [
    "## Problem 4b [Short Answer]: Discussion\n",
    "\n",
    "Now that you've completed all three parts, discuss your findings. Did you see any interesting states pop out in your crowd movies, especially when you fit to the entire dataset? Are the less frequently used states interesting or are they just noise? \n",
    "\n",
    "_Discuss below_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9697GqPmYx_"
   },
   "source": [
    "# Submission Instructions\n",
    "\n",
    "\n",
    "**Formatting:** check that your code does not exceed 80 characters in line width. You can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
    "\n",
    "Download your notebook in .ipynb format and use the following commands to convert it to PDF. \n",
    "\n",
    "**Option 1 (best case): ipynb &rarr; pdf** Run the following command to convert to a PDF:\n",
    "```\n",
    "jupyter nbconvert --to pdf lab7_teamname.ipynb\n",
    "```\n",
    "\n",
    "Unfortunately, `nbconvert` sometimes crashes with long notebooks. If that happens, here are a few options:\n",
    "\n",
    "\n",
    "**Option 2 (next best): ipynb &rarr; tex &rarr; pdf**:\n",
    "```\n",
    "jupyter nbconvert --to latex lab7_teamname.ipynb\n",
    "pdflatex lab7_teamname.tex\n",
    "```\n",
    "\n",
    "**Option 3: ipynb &rarr; html &rarr; pdf**:\n",
    "```\n",
    "jupyter nbconvert --to html lab7_teamname.ipynb\n",
    "# open lab7_teamname.html in browser and print to pdf\n",
    "```\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "- `nbconvert`: If you're using Anaconda for package management, \n",
    "```\n",
    "conda install -c anaconda nbconvert\n",
    "```\n",
    "- `pdflatex`: It comes with standard TeX distributions like TeXLive, MacTex, etc. Alternatively, you can upload the .tex and supporting files to Overleaf (free with Stanford address) and use it to compile to pdf.\n",
    "\n",
    "**Upload** your .ipynb and .pdf files to Gradescope. \n",
    "\n",
    "**Only one submission per team!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "STAT220/320 Lab 7: Autoregressive Hidden Markov Models of Behavior.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
