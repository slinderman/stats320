
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Simple Spike Sorting &#8212; Machine Learning Methods for Neural Data Analysis</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Spike Sorting by Deconvolution" href="05_deconv_spike_sorting.html" />
    <link rel="prev" title="Basic Neurobiology" href="03_neurobio.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Methods for Neural Data Analysis</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notes
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="02_probabilistic_modeling.html">
   Probabilistic Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_neurobio.html">
   Basic Neurobiology
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Simple Spike Sorting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_deconv_spike_sorting.html">
   Spike Sorting by Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="99_references.html">
   References
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Labs
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/00_pytorch_primer.html">
   Lab 0: PyTorch Primer
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://colab.research.google.com/github/slinderman/stats320/blob/winter2023/lectures/04_simple_spike_sorting.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/lectures/04_simple_spike_sorting.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-simple-probabilistic-model">
   A simple probabilistic model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assumptions">
   Assumptions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-gaussian-distribution">
   The Gaussian distribution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#matrix-factorization-perspective">
   Matrix factorization perspective
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#accounting-for-scale-invariance">
   Accounting for scale invariance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prior-on-amplitudes">
   Prior on amplitudes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-joint-probability">
   The joint probability
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-the-model">
   Fitting the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#updating-the-waveforms">
   Updating the waveforms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finishing-the-optimization">
     Finishing the optimization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#updating-the-amplitudes">
   Updating the amplitudes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-final-algorithm">
   The final algorithm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   Further Reading
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Simple Spike Sorting</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-simple-probabilistic-model">
   A simple probabilistic model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assumptions">
   Assumptions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-gaussian-distribution">
   The Gaussian distribution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#matrix-factorization-perspective">
   Matrix factorization perspective
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#accounting-for-scale-invariance">
   Accounting for scale invariance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prior-on-amplitudes">
   Prior on amplitudes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-joint-probability">
   The joint probability
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-the-model">
   Fitting the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#updating-the-waveforms">
   Updating the waveforms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finishing-the-optimization">
     Finishing the optimization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#updating-the-amplitudes">
   Updating the amplitudes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-final-algorithm">
   The final algorithm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   Further Reading
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="simple-spike-sorting">
<h1>Simple Spike Sorting<a class="headerlink" href="#simple-spike-sorting" title="Permalink to this headline">#</a></h1>
<p><img alt="neuropixels" src="../_images/neuropixels.png" /></p>
<p>With that background, we come to our first neural data analysis problem: <strong>spike sorting</strong>.  The black vertical stripes in <a class="reference internal" href="03_neurobio.html#id5"><span class="std std-numref">Fig. 6</span></a>d (reproduced above) are extracellular action potentials (EAPs) measured across adjacent recording channels on the Neuropixel probe, which arise from a spike on a nearby neuron.</p>
<blockquote>
<div><p><em>The spike sorting problem is to identify the spikes in the multi-channel voltage recording and assign those spikes to individual neurons based on the spike waveform and the channels that were activated.</em></p>
</div></blockquote>
<section id="a-simple-probabilistic-model">
<h2>A simple probabilistic model<a class="headerlink" href="#a-simple-probabilistic-model" title="Permalink to this headline">#</a></h2>
<p>I know we just picked on spherical cows, but there really is value in starting with simplified models. To get started on spike sorting, let’s consider a zoomed out view of a Neuropixels recording, like that shown in <a class="reference internal" href="03_neurobio.html#id5"><span class="std std-numref">Fig. 6</span></a>d. Specifically, let’s imagine downsampling the 30 kHz time series to 500 Hz. Then we can represent the multi-channel voltage recording as a matrix <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{C \times T}\)</span> where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(C\)</span> is the number of channels</p></li>
<li><p><span class="math notranslate nohighlight">\(T\)</span> is the number of 2 ms time bins</p></li>
<li><p><span class="math notranslate nohighlight">\(x_{c,t}\)</span> is the average voltage on channel <span class="math notranslate nohighlight">\(c\)</span> during time bin <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
</ul>
<p>At this temporal resolution, each spike is typically contained within a single time bin.</p>
</section>
<section id="assumptions">
<h2>Assumptions<a class="headerlink" href="#assumptions" title="Permalink to this headline">#</a></h2>
<p>To model this data, we will make a few assumptions:</p>
<ol class="simple">
<li><p>Assume there are <span class="math notranslate nohighlight">\(K\)</span> neurons in the vicinity of the probe. When the <span class="math notranslate nohighlight">\(k\)</span>-th neuron spikes, its EAP produces a signature <strong>waveform</strong> on the channels. We model the waveform as a vector, <span class="math notranslate nohighlight">\(\mathbf{w}_k = (w_{k,1}, \ldots, w_{k,C}) \in \mathbb{R}^C\)</span>, where <span class="math notranslate nohighlight">\(w_{k,c}\)</span> represents the average magnitude of the EAP produced on channel <span class="math notranslate nohighlight">\(c\)</span> each time neuron <span class="math notranslate nohighlight">\(k\)</span> spikes.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(\mathbf{a}_k = (a_{k,1}, \ldots, a_{k,T}) \in \mathbb{R}_+^T\)</span> denote the time series of spike <strong>amplitudes</strong> for neuron <span class="math notranslate nohighlight">\(k\)</span>. Since neurons spike only a few times a second, the amplitude is typically zero, but when the neuron does spike it has a positive amplitude.</p></li>
<li><p>If two neurons fire at the same, their waveforms add together in the measured voltage.</p></li>
<li><p>The voltage recordings are noisy, so in addition to the spike waveforms we also have independent Gaussian noise <span class="math notranslate nohighlight">\(\epsilon_{c,t} \in \mathcal{N}(0, \sigma^2)\)</span> for each channel <span class="math notranslate nohighlight">\(c\)</span> and time bin <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
</ol>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note that the amplitudes are non-negative real numbers (<span class="math notranslate nohighlight">\(a_{k,t} \in \mathbb{R}_+\)</span>). We do not allow spikes with <strong>negative</strong> amplitude.</p>
</div>
</section>
<section id="the-gaussian-distribution">
<h2>The Gaussian distribution<a class="headerlink" href="#the-gaussian-distribution" title="Permalink to this headline">#</a></h2>
<div class="admonition-the-gaussian-distribution admonition">
<p class="admonition-title">The Gaussian Distribution</p>
<p>We denote a <a class="reference external" href="https://en.wikipedia.org/wiki/Normal_distribution">Gaussian (aka normal)</a> random variable <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span> by,</p>
<div class="math notranslate nohighlight">
\[
x \sim \mathcal{N}(\mu, \sigma^2),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu = \mathbb{E}[x]\)</span> is the mean and <span class="math notranslate nohighlight">\(\sigma^2 = \mathbb{V}[x]\)</span> is the variance. The Gaussian pdf is,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{N}(x; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left\{ -\frac{1}{2 \sigma^2}(x - \mu)^2\right\}.
\]</div>
<p>The Gaussian distribution has many important properties. For example,linear transformations of <span class="math notranslate nohighlight">\(x\)</span> are also Gaussian:</p>
<div class="math notranslate nohighlight">
\[
x \sim \mathcal{N}(\mu, \sigma^2) \Rightarrow ax + b \sim \mathcal{N}(a \mu + b, a^2 \sigma^2). 
\]</div>
<p>We will cover more nice properties of the Gaussian distribution as the course goes on.</p>
</div>
<p>With these assumptions, we model the measured voltage as a sum of spike waveforms, weighted by the amplitudes, with Gaussian noise:</p>
<div class="math notranslate nohighlight">
\[
x_{c,t} \sim \mathcal{N} \left( \sum_{k=1}^K w_{k,c} a_{k,t}, \sigma^2 \right).
\]</div>
</section>
<section id="matrix-factorization-perspective">
<h2>Matrix factorization perspective<a class="headerlink" href="#matrix-factorization-perspective" title="Permalink to this headline">#</a></h2>
<p>Here’s another way to view the model. Combine the waveforms and amplitudes into matrices</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbf{W} &amp;= 
\begin{bmatrix}
| &amp;  &amp; | \\
\mathbf{w}_1 &amp; \ldots &amp; \mathbf{w}_K \\
| &amp; &amp; | 
\end{bmatrix}
\in \mathbb{R}^{C \times K},
&amp;
\mathbf{A} &amp;= 
\begin{bmatrix}
| &amp;  &amp; | \\
\mathbf{a}_1 &amp; \ldots &amp; \mathbf{a}_K \\
| &amp; &amp; | 
\end{bmatrix}
\in \mathbb{R}^{T \times K}.
\end{align*}
\end{split}\]</div>
<p>and let <span class="math notranslate nohighlight">\(\mathbf{E} \in \mathbb{R}^{C \times T}\)</span> denote the noise matrix with entries <span class="math notranslate nohighlight">\(\epsilon_{c,t}\)</span>.</p>
<p>Our model is that <span class="math notranslate nohighlight">\(\mathbf{X} = \mathbf{W} \mathbf{A}^\top + \mathbf{E}\)</span>.
That is, the matrix of voltage measurements is the outer product of the waveforms and the amplitudes.</p>
<p>This is called a <strong>matrix factorization</strong> model, since the data matrix is modeled as the outer product of two factors (plus noise). In particular, since the amplitudes are constrained to be non-negative and the waveforms are unconstrained, this is a <strong>semi-non-negative</strong> matrix factorization model <span id="id1">[<a class="reference internal" href="99_references.html#id9" title="Chris HQ Ding, Tao Li, and Michael I Jordan. Convex and semi-nonnegative matrix factorizations. IEEE transactions on pattern analysis and machine intelligence, 32(1):45–55, 2008.">Ding <em>et al.</em>, 2008</a>]</span>.</p>
</section>
<section id="accounting-for-scale-invariance">
<h2>Accounting for scale invariance<a class="headerlink" href="#accounting-for-scale-invariance" title="Permalink to this headline">#</a></h2>
<p>Notice that the model is <strong>invariant to rescaling</strong>. We could multiply the amplitudes <span class="math notranslate nohighlight">\(\mathbf{a}_k\)</span> by any positive constant <span class="math notranslate nohighlight">\(c &gt; 0\)</span> and they would still be non-negative. As long as we multiply the corresponding waveforms by <span class="math notranslate nohighlight">\(c^{-1}\)</span>, the product <span class="math notranslate nohighlight">\(\mathbf{W} \mathbf{A}^\top\)</span> remains unchanged.</p>
<p>We can remove this degree of freedom by constraining the weights to be unit norm; i.e. enforce <span class="math notranslate nohighlight">\(\|\mathbf{w}_k\| = 1\)</span> for all <span class="math notranslate nohighlight">\(k\)</span>. One way to do this is by giving the waveforms a uniform prior on the unit sphere,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}_k \sim \mathrm{Unif}(\mathbb{S}_{C-1}).
\]</div>
<div class="admonition-notation admonition">
<p class="admonition-title">Notation</p>
<p>We denote the unit hypersphere in <span class="math notranslate nohighlight">\(C\)</span> dimensions by</p>
<div class="math notranslate nohighlight">
\[
\mathbb{S}_{C-1} = \left\{ \mathbf{u}: \mathbf{u} \in \mathbb{R}^C \text{ and } \|\mathbf{u}\|_2 = 1 \right\}
\]</div>
<p>It is a (<span class="math notranslate nohighlight">\(C-1\)</span>)-dimensional manifold embedded in <span class="math notranslate nohighlight">\(\mathbb{R}^C\)</span>.</p>
</div>
<div class="admonition-the-uniform-distribution admonition">
<p class="admonition-title">The uniform distribution</p>
<p>We denote a random variable <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{X}\)</span> that follows the uniform distribution by</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x} \sim \mathrm{Unif}(\mathbb{X})
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{X}\)</span> is the support. It has density</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Unif}(\mathbf{x}; \mathbb{X}) = \frac{1}{|\mathbb{X}|} \cdot \mathbb{I}[\mathbf{x} \in \mathbb{X}]
\]</div>
<p>where <span class="math notranslate nohighlight">\(|\mathbb{X}|\)</span> is the volume of <span class="math notranslate nohighlight">\(\mathbb{X}\)</span>.</p>
</div>
</section>
<section id="prior-on-amplitudes">
<h2>Prior on amplitudes<a class="headerlink" href="#prior-on-amplitudes" title="Permalink to this headline">#</a></h2>
<p>To complete the model, we place an <a class="reference external" href="https://en.wikipedia.org/wiki/Exponential_distribution"><strong>exponential</strong></a> prior on amplitudes,</p>
<div class="math notranslate nohighlight">
\[
a_{k,t} \sim \mathrm{Exp}(\lambda)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is the inverse-scale (aka rate) parameter.</p>
<div class="admonition-the-exponential-distribution admonition">
<p class="admonition-title">The Exponential distribution</p>
<p>We denote an exponential random variable <span class="math notranslate nohighlight">\(x \in \mathbb{R}_+\)</span> by,</p>
<div class="math notranslate nohighlight">
\[
x \sim \mathrm{Exp}(\lambda)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is the <strong>inverse scale</strong> or <strong>rate</strong> parameter. It has mean <span class="math notranslate nohighlight">\(\mathbb{E}[x] = \lambda^{-1}\)</span> and variance <span class="math notranslate nohighlight">\(\mathbb{V}[x] = \lambda^{-2}\)</span>. Its pdf is,</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Exp}(x; \lambda) = \lambda e^{-\lambda x}.
\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Compare the gamma pdf from the last chapter to the exponential pdf above. Show that the exponential distribution is a special case of the gamma distribution.</p>
</div>
<p>We treat the waveforms and the noise variance as parameters; i.e. we don’t put priors on them.</p>
</section>
<section id="the-joint-probability">
<h2>The joint probability<a class="headerlink" href="#the-joint-probability" title="Permalink to this headline">#</a></h2>
<p>Finally, we can write the joint probability,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\mathbf{X}, \mathbf{W}, \mathbf{A}) 
&amp;= p(\mathbf{X} \mid \mathbf{W}, \mathbf{A}) \, p(\mathbf{W}) \, p(\mathbf{A}) \\
&amp;= \left[ \prod_{c=1}^C \prod_{t=1}^T \mathcal{N} \left(x_{c,t} \mid \sum_{k=1}^K w_{k,c} a_{k,t}, \sigma^2 \right) \right] \\
&amp;\qquad \times \left[ \prod_{k=1}^K \mathrm{Unif}(\mathbf{w}_k; \mathbb{S}_{C-1}) \right] 
\times \left[ \prod_{k=1}^K \prod_{t=1}^T \mathrm{Exp}(a_{k,t}; \lambda) \right].
\end{align*}
\end{split}\]</div>
<p>(As before, we suppressed the dependence on the parameters; i.e., <span class="math notranslate nohighlight">\(\sigma^2\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span>.)</p>
</section>
<section id="fitting-the-model">
<h2>Fitting the model<a class="headerlink" href="#fitting-the-model" title="Permalink to this headline">#</a></h2>
<p>Like last time, we will “fit” the model by performing maximum <em>a posteriori</em> (MAP) estimation with coordinate ascent. Specifically, our algorithm will be:</p>
<ul class="simple">
<li><p>repeat until convergence:</p>
<ul>
<li><p>for <span class="math notranslate nohighlight">\(k=1,\ldots,K\)</span>:</p>
<ul>
<li><p>Set <span class="math notranslate nohighlight">\(\mathbf{w}_k = \text{arg max} \; p(\mathbf{X}, \mathbf{W}, \mathbf{A})\)</span> holding all else fixed</p></li>
<li><p>Set <span class="math notranslate nohighlight">\(\mathbf{a}_k = \text{arg max} \; p(\mathbf{X}, \mathbf{W}, \mathbf{A})\)</span> holding all else fixed</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="updating-the-waveforms">
<h2>Updating the waveforms<a class="headerlink" href="#updating-the-waveforms" title="Permalink to this headline">#</a></h2>
<p>First, consider optimizing the waveforms. Maximizing the joint probability wrt <span class="math notranslate nohighlight">\(\mathbf{w}_k\)</span> is equivalent to maximizing the log joint probability, which is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\log p(\mathbf{X}, \mathbf{W}, \mathbf{A}) 
&amp;= \sum_{c=1}^C \sum_{t=1}^T \log \mathcal{N}\left(x_{c,t} \mid \sum_{k=1}^K w_{k,c} a_{k,t}, \sigma^2 \right) + \log \mathbb{I}[\mathbf{w}_k \in \mathbb{S}_{C-1}] + c' \\
&amp;= -\frac{1}{2\sigma^2} \sum_{c=1}^C \sum_{t=1}^T \left(x_{c,t} - \sum_{k=1}^K w_{k,c} a_{k,t}\right)^2 + \log \mathbb{I}[\mathbf{w}_k \in \mathbb{S}_{C-1}] + c'' \\
&amp;= -\frac{1}{2\sigma^2} \sum_{c=1}^C \sum_{t=1}^T \left(r_{c,t} - w_{k,c} a_{k,t} \right)^2 + \log \mathbb{I}[\mathbf{w}_k \in \mathbb{S}_{C-1}] + c'' \\
\end{align*}
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
r_{c,t} = x_{c,t} - \sum_{j \neq k} w_{j,c} a_{j,c}
\]</div>
<p>is the <strong>residual</strong>, and <span class="math notranslate nohighlight">\(c'\)</span> and <span class="math notranslate nohighlight">\(c''\)</span> are constants wrt <span class="math notranslate nohighlight">\(\mathbf{w}_k\)</span>.</p>
<p>The solution will become more clear if we write it in vector form. Let <span class="math notranslate nohighlight">\(\mathbf{r}_t = (r_{1,t}, \ldots, r_{C,t})\)</span> denote the vector of residuals. Then we can get rid of the sum over channels and write the log joint probability as,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\log p(\mathbf{X}, \mathbf{W}, \mathbf{A}) 
&amp;=  -\frac{1}{2\sigma^2} \sum_{t=1}^T (\mathbf{r}_{t} - \mathbf{w}_{k} a_{k,t})^\top (\mathbf{r}_{t} - \mathbf{w}_{k} a_{k,t}) + \log \mathbb{I}[\mathbf{w}_k \in \mathbb{S}_{C-1}] + c'' \\
&amp;= \sum_{t=1}^T \mathcal{N}(\mathbf{r}_t; \mathbf{w}_k a_{k,t}, \sigma^2 \mathbf{I}) + \log \mathbb{I}[\mathbf{w}_k \in \mathbb{S}_{C-1}] + c'' \\
\end{align*}
\end{split}\]</div>
<p>where we have taken this opportunity to introduce the multivariate normal distribution.</p>
<div class="admonition-the-multivariate-normal-distribution admonition">
<p class="admonition-title">The multivariate normal distribution</p>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution"><strong>multivariate normal</strong></a> distribution is the multi-dimensional generalization of the scalar Gaussian/normal distribution introduced above. We denote a multivariate normal random variable <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^D\)</span> by,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma}).
\]</div>
<p>Its density is,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{N}(\mathbf{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma})
= (2 \pi)^{-\frac{D}{2}} |\boldsymbol{\Sigma}|^{-\frac{1}{2}} \exp \left\{ -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \right\}.
\]</div>
<p>When <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \sigma^2 \mathbf{I}\)</span>, we call it a <strong>spherical Gaussian</strong> distribution. Then its density reduces to a product of scalar Gaussian densities, implying that the entries of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> are independent Gaussian random variables.</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\mathcal{N}(\mathbf{x}; \boldsymbol{\mu}, \sigma^2 \mathbf{I})
&amp;= \prod_{d=1}^D \mathcal{N}(x_d \mid \mu_d, \sigma^2).
\end{align*}
\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Prove the equality between the spherical Gaussian densities and the product of scalar Gaussian densities.</p>
</div>
<section id="finishing-the-optimization">
<h3>Finishing the optimization<a class="headerlink" href="#finishing-the-optimization" title="Permalink to this headline">#</a></h3>
<p>To complete the optimization wrt <span class="math notranslate nohighlight">\(\mathbf{w}_k\)</span>, let’s further expand the multivariate density and drop terms that don’t depend on the variable of interest,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\log p(\mathbf{X}, \mathbf{W}, \mathbf{A}) 
&amp;= \frac{1}{\sigma^2} \sum_{t=1}^T \left(\mathbf{r}_{t}^\top \mathbf{w}_{k} a_{k,t} - \frac{a_{k,t}^2}{2} \mathbf{w}_k^\top \mathbf{w}_k \right) + \log \mathbb{I}[\mathbf{w}_k \in \mathbb{S}_{C-1}] + c''
\end{align*}
\]</div>
<p>The log-indicator simply captures that this is a constrained optimization problem: <span class="math notranslate nohighlight">\(\mathbf{w}_k\)</span> must be a normalized vector, otherwise this term is negative infinity. Note that <span class="math notranslate nohighlight">\(\mathbf{w}_k^\top \mathbf{w}_k = 1\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{w}_k \in \mathbb{S}_{C-1}\)</span>, so the second term in parentheses is actually a constant for all valid <span class="math notranslate nohighlight">\(\mathbf{w}_k\)</span>.</p>
<p>Thus, maximizing with respect to <span class="math notranslate nohighlight">\(\mathbf{w}_k\)</span> amounts to solving the following constrained optimization problem,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbf{w}_k^\star 
&amp;= \text{arg}\, \max_{\mathbf{w}_{k}  \in \mathbb{S}_{C-1}}  \left( \sum_{t=1}^T a_{k,t} \mathbf{r}_{t} \right)^\top \mathbf{w}_{k} \\
&amp;= \text{arg}\, \max_{\mathbf{w}_{k}  \in \mathbb{S}_{C-1}}  \left\langle \sum_{t=1}^T a_{k,t} \mathbf{r}_t, \, \mathbf{w}_{k} \right \rangle \\
&amp;= \text{arg}\, \max_{\mathbf{w}_{k}  \in \mathbb{S}_{C-1}}  \left\langle \mathbf{R} \mathbf{a}_k, \, \mathbf{w}_{k} \right \rangle
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{R} \in \mathbb{R}^{C \times T}\)</span> is the matrix of residuals with columns <span class="math notranslate nohighlight">\([\mathbf{r}_1, \ldots, \mathbf{r}_T]\)</span>.</p>
<p>Maximing this linear objective subject to a unit norm constraint has a well known solution — make <span class="math notranslate nohighlight">\(\mathbf{w}_k\)</span> parallel to the other vector in the inner product:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}_k^\star \propto \mathbf{R} \mathbf{a}_k.
\]</div>
</section>
</section>
<section id="updating-the-amplitudes">
<h2>Updating the amplitudes<a class="headerlink" href="#updating-the-amplitudes" title="Permalink to this headline">#</a></h2>
<p>Now let’s derive updates for the amplitudes. As a function of <span class="math notranslate nohighlight">\(a_{k,t}\)</span>, the log joint probability is,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\log p(\mathbf{X}, \mathbf{W}, \mathbf{A})
&amp;=  \frac{\mathbf{r}_{t}^\top \mathbf{w}_{k} a_{k,t}}{\sigma^2} - \frac{a_{k,t}^2}{2 \sigma^2} - \lambda a_{k,t} + \log \mathbb{I}[a_{k,t} \geq 0] + c
\end{align*}
\]</div>
<p>Maximizing wrt <span class="math notranslate nohighlight">\(a_{k,t}\)</span> is simply maximizing a quadratic objective over the non-negative reals,</p>
<div class="admonition-quadratic-optimzation-with-non-negativity-constraints admonition">
<p class="admonition-title">Quadratic optimzation with non-negativity constraints</p>
<p>To solve the constrained optimization problem, <span class="math notranslate nohighlight">\(\max_{x \geq 0} f(x)\)</span> with</p>
<div class="math notranslate nohighlight">
\[ 
f(x) = -\frac{\alpha}{2} x^2 + \beta x + \gamma,
\]</div>
<p>and <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>, note that the objective is concave and the unconstrained solution is at <span class="math notranslate nohighlight">\(x^\star = \beta / \alpha\)</span>. If <span class="math notranslate nohighlight">\(x^\star &lt; 0\)</span>, then <span class="math notranslate nohighlight">\(f(x)\)</span> must be decreasing at <span class="math notranslate nohighlight">\(x=0\)</span> so that the constrained optimum is at,</p>
<div class="math notranslate nohighlight">
\[
x^\star = \max \left\{0, \beta/\alpha \right\}
\]</div>
<p>and the solution is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(x^\star) = 
\begin{cases}
\gamma + \frac{\beta^2}{2 \alpha} &amp; \text{if } \beta \geq 0 \\
\gamma &amp; \text{if } \beta &lt; 0
\end{cases}
\end{split}\]</div>
</div>
<p>From the box above, we have,</p>
<div class="math notranslate nohighlight">
\[
a_{k,t}^\star = \max \left\{0, \, \sigma^2 
\left(\frac{\mathbf{r}_{t}^\top \mathbf{w}_{k}}{\sigma^2} - \lambda \right)\right\} 
= \max\left\{0, \, \mathbf{r}_{t}^\top \mathbf{w}_{k} - \lambda \sigma^2 \right\}
\]</div>
<p>The first term, <span class="math notranslate nohighlight">\(\mathbf{r}_{t}^\top \mathbf{w}_{k}\)</span>, is the projection of the residual onto the waveform for neuron <span class="math notranslate nohighlight">\(k\)</span>. The product of hyperparameters <span class="math notranslate nohighlight">\(\lambda\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> defines the threshold that projection must exceed in order to designate a spike in amplitude.</p>
<p>In vector form, this simplifies to</p>
<div class="math notranslate nohighlight">
\[
\mathbf{a}_k^\star = \max \{0, \, \mathbf{R}^\top \mathbf{w}_k - \lambda \sigma^2 \}
\]</div>
<div class="admonition-shrinkage admonition">
<p class="admonition-title">Shrinkage</p>
<p>Note that even if the projection <span class="math notranslate nohighlight">\(\mathbf{r}_{t}^\top \mathbf{w}_{k}\)</span> exceeds the threshold, the optimal amplitude is still “shrunk” by a factor of <span class="math notranslate nohighlight">\(\lambda \sigma^2\)</span>. This is a common feature of <span class="math notranslate nohighlight">\(\ell_1\)</span> optimization problems; i.e., optimization problems with regularization on the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm of the solution. The exponential prior on amplitudes induces such an optimization problem, and indeed there is a close correspondence between MAP estimation in Bayesian models with exponential priors and maximum likelihood estimation with <span class="math notranslate nohighlight">\(\ell_1\)</span> regularization, like the LASSO problem <span id="id2">[<a class="reference internal" href="99_references.html#id10" title="Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Volume 2. Springer, 2009.">Hastie <em>et al.</em>, 2009</a>]</span>.</p>
</div>
</section>
<section id="the-final-algorithm">
<h2>The final algorithm<a class="headerlink" href="#the-final-algorithm" title="Permalink to this headline">#</a></h2>
<p>Now that we have derived both updates, we can revise our final algorithm slightly:</p>
<ul class="simple">
<li><p>repeat until convergence:</p>
<ul>
<li><p>for <span class="math notranslate nohighlight">\(k=1,\ldots,K\)</span>:</p>
<ul>
<li><p>Compute the residual <span class="math notranslate nohighlight">\(\mathbf{R} = \mathbf{X} - \sum_{j \neq k} \mathbf{w}_j \mathbf{a}_j^\top\)</span></p></li>
<li><p>Set <span class="math notranslate nohighlight">\(\mathbf{w}_k \propto \mathbf{R} \mathbf{a}_k\)</span></p></li>
<li><p>Set <span class="math notranslate nohighlight">\(\mathbf{a}_k = \max \{0, \, \mathbf{R}^\top \mathbf{w}_k - \lambda \sigma^2 \}\)</span></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You don’t have to recompute the residual from scratch each iteration. Just do a rank one update after each iteration.</p>
</div>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">#</a></h2>
<p>This chapter introduced the spike sorting problem for electrophysiological (“ephys”) recordings with modern tools like Neuropixels. The same strategy applies to other ephys methods, like dense multielectrode arrays.</p>
<p>In this first pass, we framed the problem as one of semi-non-negative matrix factorization, or semi-NMF. We took a Bayesian approach, with priors on the waveforms and amplitudes. We derived a coordinate ascent algorithm for MAP estimation and found that the updates have pleasingly simple form.</p>
<p>However, we made an undesirable assumption in deriving this algorithm: we started by downsampling our voltage traces to the point that a spike fit within a single time bin. What is the point of recording at 30kHz if we are going to average into 2ms bins!? In the next chapter, we will relax this assumption and show how to do spike sorting with a <strong>convolutional matrix factorization</strong> model.</p>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">#</a></h2>
<p>Modeling via matrix factorization is one of those great ideas that has been discovered over and over again. In the probabilistic machine learning literature, some nice references include:</p>
<ul class="simple">
<li><p><span id="id3">[<a class="reference internal" href="99_references.html#id13" title="Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–622, 1999.">Tipping and Bishop, 1999</a>]</span>: develops probabilistic PCA, an unconstrained matrix factorization</p></li>
<li><p><span id="id4">[<a class="reference internal" href="99_references.html#id14" title="Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755):788–791, 1999.">Lee and Seung, 1999</a>]</span>: non-negative matrix factorization</p></li>
<li><p><span id="id5">[<a class="reference internal" href="99_references.html#id17" title="Sam Roweis and Zoubin Ghahramani. A unifying review of linear Gaussian models. Neural computation, 11(2):305–345, 1999.">Roweis and Ghahramani, 1999</a>]</span>: PCA, sparse PCA, ICA, etc. as linear Gaussian models</p></li>
<li><p><span id="id6">[<a class="reference internal" href="99_references.html#id12" title="Andriy Mnih and Russ R Salakhutdinov. Probabilistic matrix factorization. Advances in neural information processing systems, 2007.">Mnih and Salakhutdinov, 2007</a>]</span>: an unconstrained matrix factorization for the Netflix challenge</p></li>
<li><p><span id="id7">[<a class="reference internal" href="99_references.html#id9" title="Chris HQ Ding, Tao Li, and Michael I Jordan. Convex and semi-nonnegative matrix factorizations. IEEE transactions on pattern analysis and machine intelligence, 32(1):45–55, 2008.">Ding <em>et al.</em>, 2008</a>]</span>: semi-NMF and other related models</p></li>
<li><p><span id="id8">[<a class="reference internal" href="99_references.html#id16" title="Prem K Gopalan and David M Blei. Efficient discovery of overlapping communities in massive networks. Proceedings of the National Academy of Sciences, 110(36):14534–14539, 2013.">Gopalan and Blei, 2013</a>]</span>: Poisson matrix factorization</p></li>
</ul>
<p>Many models are really matrix factorization in disguise. For example, latent Dirichlet allocation <span id="id9">[<a class="reference internal" href="99_references.html#id15" title="David M Blei, Andrew Y Ng, and Michael I Jordan. Latent Dirichlet allocation. Journal of machine Learning research, 3(Jan):993–1022, 2003.">Blei <em>et al.</em>, 2003</a>]</span> is quite similar to NMF.</p>
<p>I don’t know of a reference for our specific model with norm constraints and exponential priors for spike sorting. However, it falls nicely within this broader landscape of matrix factorization methods, and it leads nicely to the model people actually use, as we’ll see next.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="03_neurobio.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Basic Neurobiology</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="05_deconv_spike_sorting.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Spike Sorting by Deconvolution</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Scott Linderman<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>