
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Generalized Linear Models &#8212; Machine Learning Methods for Neural Data Analysis</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="References" href="99_references.html" />
    <link rel="prev" title="Summary Statistics" href="08_summary_stats.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Methods for Neural Data Analysis</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Labs
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/00_pytorch_primer.html">
   Lab 0: PyTorch Primer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/01_spike_sorting.html">
   Lab 1: Spike Sorting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/02_calcium_imaging.html">
   Lab 2: Calcium Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/03_pose_tracking.html">
   Lab 3: Markerless Pose Tracking
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/04_glms.html">
   Lab 4: Generalized Linear Models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="02_probabilistic_modeling.html">
   Probabilistic Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_neurobio.html">
   Basic Neurobiology
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unit I: Signal Extraction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="04_simple_spike_sorting.html">
   Simple Spike Sorting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_deconv_spike_sorting.html">
   Spike Sorting by Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_calcium_imaging.html">
   Demixing Calcium Imaging Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_pose_tracking.html">
   Markerless Pose Tracking
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unit II: Encoding &amp; Decoding
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="08_summary_stats.html">
   Summary Statistics
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Generalized Linear Models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="99_references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://colab.research.google.com/github/slinderman/stats320/blob/winter2023/lectures/09_glm.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/lectures/09_glm.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-nonlinear-poisson-lnp-models">
   Linear Nonlinear Poisson (LNP) models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#incorporating-spike-history">
   Incorporating spike history
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-neuronal-spike-train-models">
   Multi-neuronal spike train models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basis-function-encodings">
   Basis function encodings
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalized-linear-models-glms">
   Generalized linear models (GLMs)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-estimation-mle">
   Maximum likelihood estimation (MLE)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradients-and-hessians">
     Gradients and Hessians
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exponential-family-distributions">
   Exponential family distributions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exponential-family-glms">
   Exponential family GLMs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradients-for-canonical-exponential-family-glms">
   Gradients for canonical exponential family GLMs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#glms-in-neuroscience">
   GLMs in neuroscience
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Generalized Linear Models</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-nonlinear-poisson-lnp-models">
   Linear Nonlinear Poisson (LNP) models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#incorporating-spike-history">
   Incorporating spike history
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-neuronal-spike-train-models">
   Multi-neuronal spike train models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basis-function-encodings">
   Basis function encodings
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalized-linear-models-glms">
   Generalized linear models (GLMs)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-estimation-mle">
   Maximum likelihood estimation (MLE)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradients-and-hessians">
     Gradients and Hessians
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exponential-family-distributions">
   Exponential family distributions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exponential-family-glms">
   Exponential family GLMs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradients-for-canonical-exponential-family-glms">
   Gradients for canonical exponential family GLMs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#glms-in-neuroscience">
   GLMs in neuroscience
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="generalized-linear-models">
<h1>Generalized Linear Models<a class="headerlink" href="#generalized-linear-models" title="Permalink to this headline">#</a></h1>
<p>Now that we have a better sense for neural spike traints, let’s build probablistic models that predict neural responses to sensory stimuli or other covariates. These are called <strong>encoding models</strong>, and ideally, these models will recapitulate summary statistics of interest.</p>
<section id="linear-nonlinear-poisson-lnp-models">
<h2>Linear Nonlinear Poisson (LNP) models<a class="headerlink" href="#linear-nonlinear-poisson-lnp-models" title="Permalink to this headline">#</a></h2>
<p>First, consider a single neuron. Let <span class="math notranslate nohighlight">\(y_{t} \in \mathbb{N}_0\)</span> denote the number of spikes it fires in the <span class="math notranslate nohighlight">\(t\)</span>-th time bin. (As before, assume time bins are length <span class="math notranslate nohighlight">\(\Delta\)</span>, typically 5-100 ms.) Let <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> denote the covariates at time <span class="math notranslate nohighlight">\(t\)</span>. For example, the covariates may be features of a sensory stimulus at time bin <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>A common modeling assumption in neuroscience is that neural spike counts are <strong>conditionally Poisson</strong></p>
<div class="math notranslate nohighlight">
\[
y_{t} \sim \mathrm{Po}(\lambda(\mathbf{x}_{1:t}) \cdot \Delta),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x}_{1:t} = (\mathbf{x}_1, \ldots, \mathbf{x}_t)\)</span> is the stimulus up to and including time <span class="math notranslate nohighlight">\(t\)</span>, and where <span class="math notranslate nohighlight">\(\lambda(\mathbf{x}_{1:t})\)</span> is a conditional <strong>firing rate</strong> that depends on the stimuli.</p>
<p>As written above, the firing rate <span class="math notranslate nohighlight">\(\lambda\)</span> looks like a rather complex function… it takes in an arbitrarily long stimulus history and outputs a non-negative scalar. We will make a few simplifying assumptions in order to construct our first model.</p>
<ol class="simple">
<li><p>Assume that <span class="math notranslate nohighlight">\(\lambda\)</span> only depends on a finite set of <strong>features</strong> of the stimulus history, <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_t = (\phi_1(\mathbf{x}_{1:t}), \ldots, \phi_{D}(\mathbf{x}_{1:t}))^\top \in \mathbb{R}^D\)</span>. For example, the features may be the most recent <span class="math notranslate nohighlight">\(D\)</span> frames of the stimulus, corresponding to <span class="math notranslate nohighlight">\(\phi_d(\mathbf{x}_{1:t}) = \mathbf{x}_{t-d}\)</span>.</p></li>
<li><p>Assume that <span class="math notranslate nohighlight">\(\lambda\)</span> only depends on <strong>linear projections</strong> of the features, <span class="math notranslate nohighlight">\(\mathbf{w}^\top \boldsymbol{\phi}_t \in \mathbb{R}\)</span>, for some weights <span class="math notranslate nohighlight">\(\mathbf{w} \in \mathbb{R}^D\)</span>. We will call <span class="math notranslate nohighlight">\(\mathbf{w}^\top \boldsymbol{\phi}_t\)</span> the <strong>activation</strong> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p>Finally, assume that <span class="math notranslate nohighlight">\(\lambda\)</span> maps the activation through a <strong>rectifying nonlinearity</strong>, <span class="math notranslate nohighlight">\(f: \mathbb{R} \mapsto \mathbb{R}_+\)</span>, to obtain a non-negative firing rate.</p></li>
</ol>
<p>Altogether, these assumptions imply a <strong>linear nonlinear Poisson (LNP)</strong> model,</p>
<div class="math notranslate nohighlight">
\[
y_t \sim \mathrm{Po}(f(\mathbf{w}^\top \boldsymbol{\phi}_t) \cdot \Delta)
\]</div>
<p>Typical choices of rectifying nonlinearity are the exponential function, <span class="math notranslate nohighlight">\(f(a) = e^a\)</span>, and the softplus function, <span class="math notranslate nohighlight">\(f(a) = \log (1+e^a)\)</span>.</p>
</section>
<section id="incorporating-spike-history">
<h2>Incorporating spike history<a class="headerlink" href="#incorporating-spike-history" title="Permalink to this headline">#</a></h2>
<p>The model above treats the spike counts <span class="math notranslate nohighlight">\(y_t\)</span> and <span class="math notranslate nohighlight">\(y_{t'}\)</span> as <strong>conditionally independent</strong> given the stimulus. However, we know this assumption is invalid due to neurons’ refractory period: after a neuron spikes, it cannot spike for at least a few milliseconds. For small time bins, these dependencies matter.</p>
<p>A simple way to address this model misspecification is to allow the firing rate to depend on both the stimulus and the <strong>spike history</strong>, <span class="math notranslate nohighlight">\(\lambda(\mathbf{x}_{1:t}, \mathbf{y}_{1:t-1})\)</span>. We can do so by including the spike history in the features,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\phi}_t = \left(\phi_1(\mathbf{x}_{1:t}, \mathbf{y}_{1:t-1}), \ldots, \phi_D(\mathbf{x}_{1:t}, \mathbf{y}_{1:t-1}) \right)^\top.
\]</div>
<p>This way, some of our features can capture the stimulus, and others can capture recent spike history. For example, one of our features might be <span class="math notranslate nohighlight">\(\phi_d(\mathbf{x}_{1:t}, \mathbf{y}_{1:t-1}) = y_{t-d}\)</span>. In the language of statistical time series models, these spike history terms make this an <strong>autoregressive (AR) model</strong>.</p>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Suppose our features were <span class="math notranslate nohighlight">\(\phi_d(\mathbf{x}_{1:t}, \mathbf{y}_{1:t-1}) = y_{t-d}\)</span> for <span class="math notranslate nohighlight">\(d=1,\ldots,D\)</span>. If neurons have a refractory period that prevents firing in two adjacent time bins, what would you expect the best-fitting weights <span class="math notranslate nohighlight">\(\mathbf{w} \in \mathbb{R}^D\)</span> to look like?</p>
</div>
</section>
<section id="multi-neuronal-spike-train-models">
<h2>Multi-neuronal spike train models<a class="headerlink" href="#multi-neuronal-spike-train-models" title="Permalink to this headline">#</a></h2>
<p>So far, we’ve considered models for a single neuron. In practice, we will often record from many neurons simultaneously, and we would like our models to capture correlations between neurons.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{y}_t = (y_{t,1}, \ldots, y_{t,N})^\top \in \mathbb{N}_0^N\)</span> denote the vector of spike counts from <span class="math notranslate nohighlight">\(N\)</span> neurons in time bin <span class="math notranslate nohighlight">\(t\)</span>. We can generalize the LNP model above as,</p>
<div class="math notranslate nohighlight">
\[
y_{t,n} \sim \mathrm{Po}(f(\mathbf{w}_n^\top \boldsymbol{\phi}_t) \cdot \Delta)
\]</div>
<p>where the weights <span class="math notranslate nohighlight">\(\mathbf{w}_n \in \mathbb{R}^D\)</span> are specific to neuron <span class="math notranslate nohighlight">\(n\)</span>, and where <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_t \in \mathbb{R}^D\)</span> now includes features of the stimulus as well as the spike history of <em>all neurons</em>.</p>
<p>For example, we might have,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\phi}_t = (\mathbf{x}_t,\ldots,\mathbf{x}_{t-L}, y_{t-1,1}, \ldots, y_{t-L,1}, \ldots, y_{t-1,N}, \ldots, y_{t-L,N}, 1)
\]</div>
<p>where <span class="math notranslate nohighlight">\(L\)</span> is the maximum lag of stimulus and spike history to be considered. The final 1 in <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_t\)</span> is a <strong>bias</strong> term that allows the model to learn a baseline firing rate.</p>
<p>The entries of <span class="math notranslate nohighlight">\(\mathbf{w}_n\)</span> associated with the features <span class="math notranslate nohighlight">\((y_{t-1,m}, \ldots, y_{t-L,m})^\top\)</span> can be thought of as <strong>coupling filters</strong>, which model how spikes on neuron <span class="math notranslate nohighlight">\(m\)</span> influence the future firing rate of neuron <span class="math notranslate nohighlight">\(n\)</span>.</p>
</section>
<section id="basis-function-encodings">
<h2>Basis function encodings<a class="headerlink" href="#basis-function-encodings" title="Permalink to this headline">#</a></h2>
<p>The model above has <span class="math notranslate nohighlight">\(\mathcal{O}(N^2 L)\)</span> weights for the coupling filters. For small bin sizes, <span class="math notranslate nohighlight">\(L\)</span> may need to include dozens of past time bins to capture all the pairwise interactions. However, these coupling filters are often approximately smooth functions of the time lag. One way to cut down on parameters and capture this smoothness is to use a <strong>basis function representation</strong>. For example, one of the features can be,</p>
<div class="math notranslate nohighlight">
\[
\phi_{m,b}(\mathbf{x}_{1:t}, \mathbf{y}_{1:t-1}) = \sum_{\ell=1}^L y_{t-\ell,m} e^{-\frac{1}{2 \sigma^2}(\ell - \mu_b)^2}.
\]</div>
<p>This is a <strong>radial basis function</strong> encoding of the spike history of neuron <span class="math notranslate nohighlight">\(m\)</span>. It is a weighted sum of past spiking, where the weights are a squared exponential (aka Gaussian) kernel centered on delay <span class="math notranslate nohighlight">\(\mu_b\)</span>. We can use <span class="math notranslate nohighlight">\(B &lt; L\)</span> basis functions to summarize the spike history over the last <span class="math notranslate nohighlight">\(L\)</span> time bins.</p>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Show that the feature above can be written as a convolution of the spike history with a squared exponential kernel.</p>
</div>
</section>
<section id="generalized-linear-models-glms">
<h2>Generalized linear models (GLMs)<a class="headerlink" href="#generalized-linear-models-glms" title="Permalink to this headline">#</a></h2>
<p>The model described above, with stimulus features, spike history terms, and basis function encodings, is what neuroscientists often call “the” generalized linear model (GLM), after <span id="id1">Pillow <em>et al.</em> [<a class="reference internal" href="99_references.html#id59" title="Jonathan W Pillow, Jonathon Shlens, Liam Paninski, Alexander Sher, Alan M Litke, EJ Chichilnisky, and Eero P Simoncelli. Spatio-temporal correlations and visual signalling in a complete neuronal population. Nature, 454(7207):995–999, 2008.">2008</a>]</span>. Of course, in statistics we know that this model is just one instance of a broad family of GLMs, which are characterized by linear projections of covariates, nonlinear link functions, and exponential family conditional distributions <span id="id2">[<a class="reference internal" href="99_references.html#id63" title="Peter McCullagh and John Nelder. Generalized linear models. Routledge, 1983.">McCullagh and Nelder, 1983</a>]</span>. In fact, we have already encountered one GLM in this course: the logistic regression model from <a class="reference internal" href="07_pose_tracking.html"><span class="doc std std-doc">Unit 1</span></a>.</p>
</section>
<section id="maximum-likelihood-estimation-mle">
<h2>Maximum likelihood estimation (MLE)<a class="headerlink" href="#maximum-likelihood-estimation-mle" title="Permalink to this headline">#</a></h2>
<p>As with logistic regression, the GLMs constructed above won’t have closed form solutions for the MLE. However, they will be amenable to the optimization methods discussed previously: (stochastic) gradient descent, Newton’s method, etc.</p>
<p>Let’s follow the same recipe as before to compute the gradient of the negative log likelihood of a GLM with an exponential nonlinearity,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{L}(\mathbf{w}) 
&amp;= - \log p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}) \\
&amp;= -\sum_{t=1}^T \sum_{n=1}^N \log \mathrm{Po}(y_{t,n} \mid e^{\mathbf{w}_n^\top \boldsymbol{\phi}_t} \cdot \Delta) \\
&amp;= -\sum_{t=1}^T \sum_{n=1}^N \left( -\log y_{t,n}! + y_{t,n} \mathbf{w}_n^\top \boldsymbol{\phi}_t  + y_{t,n} \log \Delta - e^{\mathbf{w}_n^\top \boldsymbol{\phi}_t} \cdot \Delta \right)  \\
&amp;= -\sum_{t=1}^T \sum_{n=1}^N \left(y_{t,n} \mathbf{w}_n^\top \boldsymbol{\phi}_t - e^{\mathbf{w}_n^\top \boldsymbol{\phi}_t} \cdot \Delta \right)  + c
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(c\)</span> is constant wrt <span class="math notranslate nohighlight">\(\mathbf{w}_n\)</span>.</p>
<div class="admonition-separability admonition">
<p class="admonition-title">Separability</p>
<p>The loss function is a sum over neurons, and that the weights for neuron <span class="math notranslate nohighlight">\(n\)</span> do not interact with those of other neurons <span class="math notranslate nohighlight">\(m\)</span>. That means we can write the objective function as,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\mathbf{w}) = \sum_{n=1}^N \mathcal{L}_n(\mathbf{w}_n),
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_n(\mathbf{w}_n) = - \left(y_{t,n} \mathbf{w}_n^\top \boldsymbol{\phi}_t - e^{\mathbf{w}_n^\top \boldsymbol{\phi}_t} \cdot \Delta \right).
\]</div>
<p>We can optimize the weights for each neuron separately. (And in parallel, if you like!)</p>
</div>
<section id="gradients-and-hessians">
<h3>Gradients and Hessians<a class="headerlink" href="#gradients-and-hessians" title="Permalink to this headline">#</a></h3>
<p>Now take the gradient wrt the weights of neuron <span class="math notranslate nohighlight">\(n\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\nabla_{\mathbf{w}_n} \mathcal{L}_n(\mathbf{w}_n) 
&amp;= -\sum_{t=1}^T (y_{t,n} - e^{\mathbf{w}_n^\top \boldsymbol{\phi}_t} \cdot \Delta) \boldsymbol{\phi}_t \\
&amp;= -\sum_{t=1}^T (y_{t,n} - \mathbb{E}[y_{t,n} \mid \mathbf{w}_n] ) \boldsymbol{\phi}_t 
\end{aligned}
\end{split}\]</div>
<p>The gradient takes an intuitive form: it is a sum of the feature vectors, <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_t\)</span>, weighted by the <strong>error</strong> <span class="math notranslate nohighlight">\((y_{t,n} - \mathbb{E}[y_{t,n} \mid \mathbf{w}_n])\)</span>. Recall that we found the same simple form  for the gradient of the logistic regression model. We’ll show that this form is characteristic of a special family of GLMs in just a second.</p>
<p>First, note that the negative log likelihood is again a convex function. We can see that from the Hessian,</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\mathbf{w}_n}^2 \mathcal{L}_n(\mathbf{w}_n) 
= \sum_{t=1}^T (e^{\mathbf{w}_n^\top \boldsymbol{\phi}_t} \cdot \Delta) \, \boldsymbol{\phi}_t \boldsymbol{\phi}_t^\top.
\]</div>
<p>Since this is a weighted sum of outer products with positive weights, the Hessian is positive semi-definite (PSD) and the negative log likelihood is convex. Convexity implies that gradient descent and other optimization methods will (under weak conditions) find a global optimum.</p>
</section>
</section>
<section id="exponential-family-distributions">
<h2>Exponential family distributions<a class="headerlink" href="#exponential-family-distributions" title="Permalink to this headline">#</a></h2>
<p>The reason why the Poisson GLM and logistic regression had such nice mathematical forms is because they are both <strong>exponential family</strong> GLMs. Exponential family distributions have densities of the form,</p>
<div class="math notranslate nohighlight">
\[
p(y; \theta) = h(y) \exp \left\{ \langle t(y), \theta \rangle - A(\theta) \right\}
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta\)</span> is the <strong>natural parameter</strong> of the distribution</p></li>
<li><p><span class="math notranslate nohighlight">\(t(y)\)</span> is a <strong>sufficient statistic</strong> of the data</p></li>
<li><p><span class="math notranslate nohighlight">\(h(y)\)</span> is the <strong>base measure</strong>, which we won’t worry too much about</p></li>
<li><p><span class="math notranslate nohighlight">\(A(\theta)\)</span> is the <strong>log normalizer</strong>, which ensures the density integrates to one.</p></li>
</ul>
<p>In order for the density to integrate to one, the log normalizer must be,</p>
<div class="math notranslate nohighlight">
\[
A(\theta) = \log \int h(y) \exp \left\{ \langle t(y), \theta \rangle \right\} \, \mathrm{d} y
\]</div>
<p>Many of the distributions we’ve encountered thus far are exponential family distributions: Poisson, Bernoulli, Gaussian, gamma, etc.</p>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<p>The Poisson distribution can be written in exponential family form by rearranging its pmf to,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathrm{Po}(y; \lambda) 
&amp;= \frac{1}{y!} \lambda^y e^{-\lambda} \\
&amp;= \frac{1}{y!} \exp \left\{ y \log \lambda - \lambda \right\} \\
&amp;= \frac{1}{y!} \exp \left\{ y \theta - e^\theta \right\} \\
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(t(y) = y\)</span> is the sufficient statistic, <span class="math notranslate nohighlight">\(\theta = \log \lambda\)</span> is the natural parameter, <span class="math notranslate nohighlight">\(e^\theta\)</span> is the log normalizer, and <span class="math notranslate nohighlight">\(h(y) = 1/y!\)</span> is the base measure.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Write the Bernoulli pmf,</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Bern}(y; p) = p^y (1-p)^{1-y},
\]</div>
<p>in exponential family form. What is the natural parameter, the sufficient statistic, the base measure, and the log normalizer?</p>
</div>
</section>
<section id="exponential-family-glms">
<h2>Exponential family GLMs<a class="headerlink" href="#exponential-family-glms" title="Permalink to this headline">#</a></h2>
<p>An exponential family GLM is one in which the observation distribution is an exponential family and the natural parameter is a linear-nonlinear function of the features,</p>
<div class="math notranslate nohighlight">
\[
\theta = g(\mathbf{w}^\top \boldsymbol{\phi}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(g: \mathbb{R} \mapsto \mathbb{R}\)</span> is a monotonically increasing function.</p>
<!-- The canonical choice is to make $g$ the identity function.

Buf if we make $g$ the identiy function, then we simply have a linear model. Where did the _generalized_ (i.e., nonlinear) part go? Note that the model is linear in the **natural parameters** $\theta$ of the distribution, whereas the Poisson GLM and logistic regression were defined in terms of the **mean parameters**. -->
<p>Note that this model is defined in terms of the natural parameters <span class="math notranslate nohighlight">\(\theta\)</span> rather than the mean parameters <span class="math notranslate nohighlight">\(\mathbb{E}[t(y) \mid \theta]\)</span> (which for a Poisson distribution is simply <span class="math notranslate nohighlight">\(\mathbb{E}[y \mid \theta]\)</span>).  How do the natural and mean parameters relate to one another? Through the gradient of the log normalizer,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
A'(\theta) &amp;\triangleq \frac{\mathrm{d}}{\mathrm{d} \theta} A(\theta) \\
&amp;= \frac{\mathrm{d}}{\mathrm{d} \theta} \log \int h(y) \exp \left\{ \langle t(y), \theta \rangle\right\} \, \mathrm{d}y \\
&amp;= \frac{\int t(y)  h(y) \exp \left\{ \langle t(y), \theta \rangle\right\} \, \mathrm{d}y}{\int h(y) \exp \left\{ \langle t(y), \theta \rangle\right\} \, \mathrm{d}y} \\
&amp;= \int t(y)  h(y) \exp \left\{ \langle t(y), \theta \rangle - A(\theta) \right\} \, \mathrm{d}y \\
&amp;= \mathbb{E}[t(y) \mid \theta]
\end{aligned}
\end{split}\]</div>
<p>In short, <em>gradients of the log normalizer yield expected sufficient statistics!</em></p>
<!-- When the sufficient statistics are linearly independent, the exponential family is said to be **minimal**, and there is a 1:1 mapping between the natural and mean parameters. That is, in a minimal exponential family, the gradient of the log normalizer is invertible. -->
<p>Now let’s write the GLM in terms of the mapping to the mean parameters,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbb{E}[t(y) \mid \mathbf{w}, \boldsymbol{\phi}]
= A'(\theta) 
= f(\mathbf{w}^\top \boldsymbol{\phi})
\end{aligned}
\]</div>
<p>where <span class="math notranslate nohighlight">\(f(a) = A'(g(a))\)</span> is the <strong>mean function</strong>. The <strong>canonical mean function</strong> is obtained by setting <span class="math notranslate nohighlight">\(g\)</span> to the identity function, so that <span class="math notranslate nohighlight">\(f\)</span> is simply the gradient of the log normalizer.</p>
<p>For example, in the Poisson distribution, the sufficient statistic is <span class="math notranslate nohighlight">\(t(y) = y\)</span>, and the log normalizer is <span class="math notranslate nohighlight">\(A(\theta) = e^\theta\)</span>. The derivative of the log normalizer is also <span class="math notranslate nohighlight">\(A'(\theta) = e^\theta\)</span>, so the canonical Poisson GLM is,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[y \mid \mathbf{w}, \boldsymbol{\phi}] = e^{\mathbf{w}^\top \boldsymbol{\phi}_t},
\]</div>
<p>just like we had above! (Note: the equivalence is exact when bin size is <span class="math notranslate nohighlight">\(\Delta = 1\)</span>; the scaling factor doesn’t change the story much.)</p>
<!-- For example, in the Poisson GLM described above, we modeled the Poisson mean as $\lambda = e^{\mathbf{w}^\top \boldsymbol{\phi}}$. (Assume a bin size of $\Delta = 1$ for simplicity.) To put this in exponential family form, we need the natural parameter  -->
</section>
<section id="gradients-for-canonical-exponential-family-glms">
<h2>Gradients for canonical exponential family GLMs<a class="headerlink" href="#gradients-for-canonical-exponential-family-glms" title="Permalink to this headline">#</a></h2>
<p>Now consider the negative log likelihood in a canonical exponential family GLM,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{L}(\mathbf{w})
&amp;= - \sum_{i=1}^T \log p(y_i \mid \mathbf{w}, \boldsymbol{\phi}_i) \\
&amp;= - \sum_{i=1}^T \langle t(y_i), \mathbf{w}^\top \boldsymbol{\phi}_i \rangle - A(\mathbf{w}^\top \boldsymbol{\phi}_i) + c
\end{aligned}
\end{split}\]</div>
<p>The gradient is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\nabla \mathcal{L}(\mathbf{w})
&amp;= - \sum_{i=1}^T (t(y_i) - A'(\mathbf{w}^\top \boldsymbol{\phi}_i)) \, \boldsymbol{\phi}_i \\
&amp;= - \sum_{i=1}^T (t(y_i) - \mathbb{E}[t(y_i) \mid \mathbf{w}, \boldsymbol{\phi}_i]) \, \boldsymbol{\phi}_i.
\end{aligned}
\end{split}\]</div>
<p>Thus, we see that the simple form of the gradient — a sum of features weighted by the corresponding error — follows from a general property of exponential family GLMs.</p>
</section>
<section id="glms-in-neuroscience">
<h2>GLMs in neuroscience<a class="headerlink" href="#glms-in-neuroscience" title="Permalink to this headline">#</a></h2>
<p>Generalized linear models are central tools in systems neuroscience. Work by <span id="id3">Paninski [<a class="reference internal" href="99_references.html#id64" title="Liam Paninski. Maximum likelihood estimation of cascade point-process neural encoding models. Network: Computation in Neural Systems, 15(4):243–262, 2004.">2004</a>]</span>, <span id="id4">Truccolo <em>et al.</em> [<a class="reference internal" href="99_references.html#id65" title="Wilson Truccolo, Uri T Eden, Matthew R Fellows, John P Donoghue, and Emery N Brown. A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects. Journal of neurophysiology, 93(2):1074–1089, 2005.">2005</a>]</span>, and <span id="id5">Pillow <em>et al.</em> [<a class="reference internal" href="99_references.html#id59" title="Jonathan W Pillow, Jonathon Shlens, Liam Paninski, Alexander Sher, Alan M Litke, EJ Chichilnisky, and Eero P Simoncelli. Spatio-temporal correlations and visual signalling in a complete neuronal population. Nature, 454(7207):995–999, 2008.">2008</a>]</span> laid the theoretical foundations for much of the experimental work that followed.</p>
<ul class="simple">
<li><p><span id="id6">Park <em>et al.</em> [<a class="reference internal" href="99_references.html#id66" title="Il Memming Park, Miriam LR Meister, Alexander C Huk, and Jonathan W Pillow. Encoding and decoding in parietal cortex during sensorimotor decision-making. Nature neuroscience, 17(10):1395–1403, 2014.">2014</a>]</span>, <span id="id7">Yates <em>et al.</em> [<a class="reference internal" href="99_references.html#id67" title="Jacob L Yates, Il Memming Park, Leor N Katz, Jonathan W Pillow, and Alexander C Huk. Functional dissection of signal and noise in MT and LIP during decision-making. Nature neuroscience, 20(9):1285–1292, 2017.">2017</a>]</span> applied GLMs to characterize heterogeneous responses in multi-neuronal spike train recordings from lateral intraparietal cortex (LIP) during decision making.</p></li>
<li><p><span id="id8">Hardcastle <em>et al.</em> [<a class="reference internal" href="99_references.html#id68" title="Kiah Hardcastle, Niru Maheswaranathan, Surya Ganguli, and Lisa M Giocomo. A multiplexed, heterogeneous, and adaptive code for navigation in medial entorhinal cortex. Neuron, 94(2):375–387, 2017.">2017</a>]</span> applied a similar analysis to characterize functional cell types in medial entorhinal cortex (MEC), finding grid cells, border cells, and much more.</p></li>
<li><p><span id="id9">Vidne <em>et al.</em> [<a class="reference internal" href="99_references.html#id69" title="Michael Vidne, Yashar Ahmadian, Jonathon Shlens, Jonathan W Pillow, Jayant Kulkarni, Alan M Litke, EJ Chichilnisky, Eero Simoncelli, and Liam Paninski. Modeling the impact of common noise inputs on the network activity of retinal ganglion cells. Journal of computational neuroscience, 33:97–121, 2012.">2012</a>]</span> characterized the responses of retinal ganglion cells, allowing for correlated noise processes, and <span id="id10">Freeman <em>et al.</em> [<a class="reference internal" href="99_references.html#id70" title="Jeremy Freeman, Greg D Field, Peter H Li, Martin Greschner, Deborah E Gunning, Keith Mathieson, Alexander Sher, Alan M Litke, Liam Paninski, Eero P Simoncelli, and others. Mapping nonlinear receptive field structure in primate retina at single cone resolution. Elife, 4:e05241, 2015.">2015</a>]</span> further refined the model to include common inputs from bipolar cells. <span id="id11">McIntosh <em>et al.</em> [<a class="reference internal" href="99_references.html#id60" title="Lane McIntosh, Niru Maheswaranathan, Aran Nayebi, Surya Ganguli, and Stephen Baccus. Deep learning models of the retinal response to natural scenes. Advances in Neural Information Processing Systems, 2016.">2016</a>]</span> framed these models as convolutional neural networks, which we will study in lab.</p></li>
<li><p>On the methodological side, much work has gone into fitting these models at scale <span id="id12">[<a class="reference internal" href="99_references.html#id71" title="Alexandro D Ramirez and Liam Paninski. Fast inference in generalized linear models via expected log-likelihoods. Journal of computational neuroscience, 36:215–234, 2014.">Ramirez and Paninski, 2014</a>, <a class="reference internal" href="99_references.html#id72" title="David Zoltowski and Jonathan W Pillow. Scaling the Poisson glm to massive neural datasets through polynomial approximations. Advances in neural information processing systems, 2018.">Zoltowski and Pillow, 2018</a>]</span>, extending the models with structured prior distributions <span id="id13">[<a class="reference internal" href="99_references.html#id73" title="Scott Linderman, Ryan P Adams, and Jonathan W Pillow. Bayesian latent structure discovery from multi-neuron recordings. Advances in neural information processing systems, 2016.">Linderman <em>et al.</em>, 2016</a>]</span>, making them more biophysically realistic <span id="id14">[<a class="reference internal" href="99_references.html#id75" title="Kenneth W Latimer, Fred Rieke, and Jonathan W Pillow. Inferring synaptic inputs from spikes with a conductance-based neural encoding model. Elife, 8:e47012, 2019.">Latimer <em>et al.</em>, 2019</a>]</span>, and understanding the extent of spiking responses that GLMs can capture <span id="id15">[<a class="reference internal" href="99_references.html#id74" title="Alison I Weber and Jonathan W Pillow. Capturing the dynamical repertoire of single neurons with generalized linear models. Neural computation, 29(12):3260–3289, 2017.">Weber and Pillow, 2017</a>]</span>.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="08_summary_stats.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Summary Statistics</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="99_references.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">References</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Scott Linderman<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>