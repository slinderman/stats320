
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Probabilistic Modeling &#8212; Machine Learning Methods for Neural Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet">
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=4ec06e9971c5264fbd345897d5258098f11cc577" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94">
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=8bf782fb4ee92b3d3646425e50f299c4e1fd152d"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/02_probabilistic_modeling';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Basic Neurobiology" href="03_neurobio.html" />
    <link rel="prev" title="Lab 6: Autoregressive HMMs" href="../labs/06_arhmm.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../index.html">

  
  
  
  
  
  
  

  
  
    <p class="title logo__title">Machine Learning Methods for Neural Data Analysis</p>
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../labs/00_pytorch_primer.html">
                        Lab 0: PyTorch Primer
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../labs/01_spike_sorting.html">
                        Lab 1: Spike Sorting
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../labs/02_calcium_imaging.html">
                        Lab 2: Calcium Deconvolution
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../labs/03_pose_tracking.html">
                        Lab 3: Markerless Pose Tracking
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../labs/04_glms.html">
                        Lab 4: Generalized Linear Models
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../labs/05_decoding.html">
                        Lab 5: Bayesian Decoding
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../labs/06_arhmm.html">
                        Lab 6: Autoregressive HMMs
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="#">
                        Probabilistic Modeling
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="03_neurobio.html">
                        Basic Neurobiology
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="04_simple_spike_sorting.html">
                        Simple Spike Sorting
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="05_deconv_spike_sorting.html">
                        Spike Sorting by Deconvolution
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="06_calcium_imaging.html">
                        Demixing Calcium Imaging Data
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="07_pose_tracking.html">
                        Markerless Pose Tracking
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="08_summary_stats.html">
                        Summary Statistics
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="09_glm.html">
                        Generalized Linear Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="10_poisson_processes.html">
                        Poisson Processes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="11_decoding.html">
                        Decoding Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="12_mixtures_em.html">
                        Mixture Models and the EM Algorithm
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="99_references.html">
                        References
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      
        <div class="navbar-end-item navbar-persistent--container">
          
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
        </div>
      
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>


  
  
    <div class="navbar-persistent--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
    </div>
  

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../labs/00_pytorch_primer.html">
                        Lab 0: PyTorch Primer
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../labs/01_spike_sorting.html">
                        Lab 1: Spike Sorting
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../labs/02_calcium_imaging.html">
                        Lab 2: Calcium Deconvolution
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../labs/03_pose_tracking.html">
                        Lab 3: Markerless Pose Tracking
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../labs/04_glms.html">
                        Lab 4: Generalized Linear Models
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../labs/05_decoding.html">
                        Lab 5: Bayesian Decoding
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../labs/06_arhmm.html">
                        Lab 6: Autoregressive HMMs
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="#">
                        Probabilistic Modeling
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="03_neurobio.html">
                        Basic Neurobiology
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="04_simple_spike_sorting.html">
                        Simple Spike Sorting
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="05_deconv_spike_sorting.html">
                        Spike Sorting by Deconvolution
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="06_calcium_imaging.html">
                        Demixing Calcium Imaging Data
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="07_pose_tracking.html">
                        Markerless Pose Tracking
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="08_summary_stats.html">
                        Summary Statistics
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="09_glm.html">
                        Generalized Linear Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="10_poisson_processes.html">
                        Poisson Processes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="11_decoding.html">
                        Decoding Models
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="12_mixtures_em.html">
                        Mixture Models and the EM Algorithm
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="99_references.html">
                        References
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item">
  


<a class="navbar-brand logo" href="../index.html">

  
  
  
  
  
  
  

  
  
    <p class="title logo__title">Machine Learning Methods for Neural Data Analysis</p>
  
</a>
    </div>
    <div class="sidebar-start-items__item">
<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Labs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../labs/00_pytorch_primer.html">Lab 0: PyTorch Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/01_spike_sorting.html">Lab 1: Spike Sorting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/02_calcium_imaging.html">Lab 2: Calcium Deconvolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/03_pose_tracking.html">Lab 3: Markerless Pose Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/04_glms.html">Lab 4: Generalized Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/05_decoding.html">Lab 5: Bayesian Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/06_arhmm.html">Lab 6: Autoregressive HMMs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundations</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Probabilistic Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_neurobio.html">Basic Neurobiology</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit I: Signal Extraction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_simple_spike_sorting.html">Simple Spike Sorting</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_deconv_spike_sorting.html">Spike Sorting by Deconvolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_calcium_imaging.html">Demixing Calcium Imaging Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_pose_tracking.html">Markerless Pose Tracking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit II: Encoding &amp; Decoding</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="08_summary_stats.html">Summary Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_glm.html">Generalized Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_poisson_processes.html">Poisson Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_decoding.html">Decoding Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit III: Unsupervised Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="12_mixtures_em.html">Mixture Models and the EM Algorithm</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="99_references.html">References</a></li>
</ul>

    </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

  
  <div id="rtd-footer-container"></div>

      </div>
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        <label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" data-toggle="tooltip" data-placement="right" title="Toggle primary sidebar">
            <span class="fa-solid fa-bars"></span>
        </label>
    </div>
    <div class="header-article__right">


<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="https://colab.research.google.com/github/slinderman/stats320/blob/winter2023/lectures/02_probabilistic_modeling.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</a>
      
  </ul>
</div>

<button onclick="toggleFullScreen()"
  class="btn btn-sm"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="../_sources/lectures/02_probabilistic_modeling.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</a>
      
      <li>
<button onclick="printPdf(this)"
  class="btn btn-sm dropdown-item"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</a>
      
  </ul>
</div>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary" data-toggle="tooltip" data-placement="left" title="Toggle secondary sidebar">
            <span class="fa-solid fa-list"></span>
        </label>
    </div>
</div>
            </div>
            
            

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Probabilistic Modeling</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-example">
   Simple example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-from-a-poisson-distribution">
   Sampling from a Poisson distribution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-a-poisson-distribution">
   Fitting a Poisson distribution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#solving-for-the-mle">
   Solving for the MLE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adding-a-prior-distribution">
   Adding a prior distribution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-from-a-gamma-distribution">
   Sampling from a gamma distribution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-a-poisson-distribution-with-a-gamma-prior">
   Fitting a Poisson distribution with a gamma prior
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-inference">
   Bayesian inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-a-posteriori-inference">
   Maximum
   <em>
    a posteriori
   </em>
   inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conjugate-priors">
   Conjugate priors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#solving-for-the-map-estimate">
   Solving for the MAP estimate
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#map-estimation-in-our-simulated-example">
   MAP estimation in our simulated example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mixture-models-and-latent-variables">
   Mixture models and latent variables
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-a-poisson-mixture-model">
   Sampling a Poisson mixture model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-a-mixture-model-by-coordinate-ascent">
   Fitting a mixture model by coordinate ascent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   Further Reading
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>

            <article class="bd-article" role="main">
              
  <section class="tex2jax_ignore mathjax_ignore" id="probabilistic-modeling">
<h1>Probabilistic Modeling<a class="headerlink" href="#probabilistic-modeling" title="Permalink to this heading">#</a></h1>
<p>Probabilistic models are distributions over data.
The shape of the distribution is determine by model <strong>parameters</strong>.
Our goal to <strong>estimate</strong> or <strong>infer</strong> those parameters from observed data.</p>
<p>As the course goes on, we will encounter more and more complex datasets, and we will construct more and more sophisticated models.
However, our models will still be composed of just a handful of basic building blocks, and our central goal of parameter estimation and inference remains the same.</p>
<p>Slides for this chapter can be downloaded <a class="reference download internal" download="" href="../_downloads/5dd281d54149572dad40c5c89b45b45f/lecture02.pdf"><code class="xref download docutils literal notranslate"><span class="pre">here</span></code></a>.</p>
<section id="simple-example">
<h2>Simple example<a class="headerlink" href="#simple-example" title="Permalink to this heading">#</a></h2>
<p>For example, let <span class="math notranslate nohighlight">\(x_t \in \mathbb{N}_0\)</span> denote the number of spikes a neuron fires in time bin <span class="math notranslate nohighlight">\(t\)</span>. One of the simplest (and yet surprisingly not bad) models of neural spike counts is the <a class="reference external" href="https://en.wikipedia.org/wiki/Poisson_distribution"><strong>Poisson distribution</strong></a> with rate <span class="math notranslate nohighlight">\(\lambda \in \mathbb{R}_+\)</span>,</p>
<div class="math notranslate nohighlight">
\[x_t \sim \mathrm{Pois}(\lambda).\]</div>
<p>The Poisson distribution has a simple <strong>probability mass function (pmf)</strong>,</p>
<div class="math notranslate nohighlight">
\[\mathrm{Pois}(x_t; \lambda) = \frac{1}{x_t!} \lambda^{x_t} e^{-\lambda}.\]</div>
<p>For Poisson random variables, the <strong>mean</strong>, aka <strong>expected value</strong>, <span class="math notranslate nohighlight">\(\mathbb{E}[x_t]\)</span>, and <strong>variance</strong>, <span class="math notranslate nohighlight">\(\mathbb{V}[x_t]\)</span>, are both equal to <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<div class="admonition-notation admonition">
<p class="admonition-title">Notation</p>
<p><span class="math notranslate nohighlight">\(x_t \in \mathbb{N}_0\)</span> means that the variable <span class="math notranslate nohighlight">\(x_t\)</span> is in (<span class="math notranslate nohighlight">\(\in\)</span>) the set <span class="math notranslate nohighlight">\(\mathbb{N}_0\)</span>, which is shorthand for the non-negative integers,</p>
<div class="math notranslate nohighlight">
\[\mathbb{N}_0 = \{0,1,2,\ldots\}.\]</div>
<p><span class="math notranslate nohighlight">\(\lambda \in \mathbb{R}_+\)</span> means that the rate <span class="math notranslate nohighlight">\(\lambda\)</span> is a non-negative real number.</p>
</div>
<div class="admonition-notation admonition">
<p class="admonition-title">Notation</p>
<p>The first equation says that the spike count <span class="math notranslate nohighlight">\(x_t\)</span> is a <strong>random variable</strong> whose distribution is (<span class="math notranslate nohighlight">\(\sim\)</span>) Poisson with rate <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>In the second equation, <span class="math notranslate nohighlight">\(\mathrm{Pois}(x_t; \lambda)\)</span> refers to the pmf of the Poisson distribution evaluated at the point <span class="math notranslate nohighlight">\(x_t\)</span>. The notation can be a little confusing at first, but it’s a standard convention.</p>
</div>
</section>
<section id="sampling-from-a-poisson-distribution">
<h2>Sampling from a Poisson distribution<a class="headerlink" href="#sampling-from-a-poisson-distribution" title="Permalink to this heading">#</a></h2>
<p>Expand the code below to see how to sample a Poisson distribution using the <code class="docutils literal notranslate"><span class="pre">torch.distributions.Poisson</span></code> object in <a class="reference external" href="https://pytorch.org/docs/stable/">PyTorch</a>. The code plots the empirical distribution of 1000 independent samples from the Poisson distribution alongside the Poisson pmf.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Poisson</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Construct a Poisson distribution with rate 5.0 and draw 1000 samples</span>
<span class="n">rate</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">pois</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">pois</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1000</span><span class="p">,))</span>

<span class="c1"># Plot a histogram of the samples and overlay the pmf</span>
<span class="n">bins</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bins</span> <span class="o">+</span> <span class="mf">.5</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">pois</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">bins</span><span class="p">)),</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;pmf&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;p(x)&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
<img alt="../_images/0668e6281747bb38ef956010be694636455f443dc02556eb3861cbcc5f66e8bb.png" src="../_images/0668e6281747bb38ef956010be694636455f443dc02556eb3861cbcc5f66e8bb.png" />
</div>
</div>
</section>
<section id="fitting-a-poisson-distribution">
<h2>Fitting a Poisson distribution<a class="headerlink" href="#fitting-a-poisson-distribution" title="Permalink to this heading">#</a></h2>
<p>Now suppose you observe the empirical spike counts sampled above (i.e. the blue histogram) and you want to estimate the rate <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, \ldots, x_T)\)</span> denote the vector of spike counts.</p>
<p>Since the simulated spike counts are <strong>independent</strong> random variables, their <strong>joint probability</strong> is a product of Poisson pmf’s,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\mathbf{x}; \lambda) 
&amp;= \prod_{t=1}^T p(x_t; \lambda) \\
&amp;= \prod_{t=1}^T \mathrm{Pois}(x_t; \lambda) \\
&amp;= \prod_{t=1}^T \frac{1}{x_t!} \lambda^{x_t} e^{-\lambda}.
\end{align*}
\end{split}\]</div>
<p>We want to find the rate that maximizes this probability,</p>
<div class="math notranslate nohighlight">
\[
\lambda_{\mathsf{MLE}} = \text{arg max} \; p(\mathbf{x}; \lambda).
\]</div>
<p>This is called <strong>maximum likelihood estimation</strong>, and <span class="math notranslate nohighlight">\(\lambda_{\mathsf{MLE}}\)</span> is called the <strong>maximum likelihood estimate (MLE)</strong>.</p>
</section>
<section id="solving-for-the-mle">
<h2>Solving for the MLE<a class="headerlink" href="#solving-for-the-mle" title="Permalink to this heading">#</a></h2>
<p>In this simple model, we can derive a closed form expression for the MLE.</p>
<p>First, note that maximizing <span class="math notranslate nohighlight">\(p(\mathbf{x}; \lambda)\)</span> is the same as maximizing <span class="math notranslate nohighlight">\(\log p(\mathbf{x}; \lambda)\)</span>, since the log is a concave function.</p>
<p>Taking logs of both sides, we find</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\log p(\mathbf{x}; \lambda) 
&amp;= \sum_{t=1}^T - \log x_t! + x_t \log \lambda -\lambda.
\end{align*}
\]</div>
<p>To maximize with respect to <span class="math notranslate nohighlight">\(\lambda\)</span>, we simply take the derivative, set it to zero, and solve for <span class="math notranslate nohighlight">\(\lambda\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\mathrm{d}}{\mathrm{d} \lambda} \log p(\mathbf{x}; \lambda) 
&amp;= \sum_{t=1}^T \left( \frac{x_t}{\lambda} - 1 \right) \\
&amp;= \frac{1}{\lambda} \left( \sum_{t=1}^T x_t \right) - T
\end{align*}
\end{split}\]</div>
<p>Setting to zero and solving yields,</p>
<div class="math notranslate nohighlight">
\[
\lambda_{\mathsf{MLE}} = \frac{1}{T} \sum_{t=1}^T x_t.
\]</div>
<p>In other words, the MLE is just the mean of the empirical spike counts, which seems sensible!</p>
</section>
<section id="adding-a-prior-distribution">
<h2>Adding a prior distribution<a class="headerlink" href="#adding-a-prior-distribution" title="Permalink to this heading">#</a></h2>
<p>Maximum likelihood estimation finds the rate that maximizes the probability of the observed spike counts, but it doesn’t take into account any prior information.</p>
<p>For example, this may not be the first neuron you’ve ever encountered. Maybe, based on your experience, you have a sense for the distribution of neural firing rates. That knowledge can be encoded in a <strong>prior distribution</strong>.</p>
<p>One common choice of prior on rates is the <a class="reference external" href="https://en.wikipedia.org/wiki/Gamma_distribution"><strong>gamma distribution</strong></a>,</p>
<div class="math notranslate nohighlight">
\[
\lambda \sim \mathrm{Ga}(\alpha, \beta).
\]</div>
<p>The gamma distribution has <strong>support</strong> for <span class="math notranslate nohighlight">\(\lambda \in \mathbb{R}_+\)</span>, and it is governed by two parameters:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span>, the <strong>shape</strong> or <strong>concentration</strong> parameter, and</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span>, the <strong>inverse scale</strong> or <strong>rate</strong> parameter.</p></li>
</ul>
<p>It’s <strong>probability density function (pdf)</strong> is,</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Ga}(\lambda; \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha - 1} e^{-\beta \lambda}.
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Gamma(\cdot)\)</span> denotes the gamma function.</p>
<div class="dropdown admonition">
<p class="admonition-title">PDFs vs PMFs</p>
<p>Since the gamma distribution has <em>continuous</em> support over the non-negative reals as opposed to <em>discrete</em> support over, say, the non-negative integers, it has a probability <em>density</em> function rather than a probability <em>mass</em> function.</p>
<p>Technically, the probability that <span class="math notranslate nohighlight">\(\lambda\)</span> falls in a subset of <span class="math notranslate nohighlight">\(\mathbb{R}_+\)</span>, like the interval <span class="math notranslate nohighlight">\([a, b)\)</span>, is the integral of the probability density function,</p>
<div class="math notranslate nohighlight">
\[
\Pr(\lambda \in [a, b)) = \int_a^b \mathrm{Ga}(\lambda; \alpha, \beta) \, \mathrm{d} \lambda.
\]</div>
</div>
</section>
<section id="sampling-from-a-gamma-distribution">
<h2>Sampling from a gamma distribution<a class="headerlink" href="#sampling-from-a-gamma-distribution" title="Permalink to this heading">#</a></h2>
<p>Expand the code below to see how to sample a gamma distribution using the <a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#gamma"><code class="docutils literal notranslate"><span class="pre">torch.distributions.Gamma</span></code></a> object. The code plots the empirical distribution of 1000 independent samples from the gamma distribution, binned into 25 evenly sized bins. It overlays the gamma pdf for comparison.</p>
<p>In words, we might say this particular gamma distribution conveys a prior belief that</p>
<blockquote>
<div><p><em>Firing rates are usually around 10 spikes/time bin with standard deviation of around 3 spikes/time bin.</em></p>
</div></blockquote>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Gamma</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Construct a gamma distribution </span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">10.0</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">10.0</span><span class="o">/</span><span class="mf">10.0</span>
<span class="n">gam</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
<span class="n">lambdas</span> <span class="o">=</span> <span class="n">gam</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1000</span><span class="p">,))</span>

<span class="c1"># Plot a histogram of the samples and overlay the pmf</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">bins</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">lambdas</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">gam</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">grid</span><span class="p">)),</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\lambda$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$p(\lambda)$&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/d19dd3d039ffa8130ec3b7d202608ad96acea19015fea342b570c3b12d0c0a72.png" src="../_images/d19dd3d039ffa8130ec3b7d202608ad96acea19015fea342b570c3b12d0c0a72.png" />
</div>
</div>
</section>
<section id="fitting-a-poisson-distribution-with-a-gamma-prior">
<h2>Fitting a Poisson distribution with a gamma prior<a class="headerlink" href="#fitting-a-poisson-distribution-with-a-gamma-prior" title="Permalink to this heading">#</a></h2>
<p>When we add in the prior distribution on <span class="math notranslate nohighlight">\(\lambda\)</span>, it becomes a random variable too. Now we have to consider the <strong>joint distribution</strong> of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\mathbf{x}, \lambda) 
&amp;= p(\mathbf{x} \mid \lambda) \, p(\lambda) \\
&amp;= \left[ \prod_{t=1}^T \mathrm{Pois}(x_t \mid \lambda) \right] \, \mathrm{Ga}(\lambda; \alpha, \beta) 
\end{align*}
\end{split}\]</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>We dropped the dependence on the parameters of the gamma prior, <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>. Technically, we should write <span class="math notranslate nohighlight">\(p(\mathbf{x}, \lambda; \alpha, \beta)\)</span>, but that gets cumbersome.</p>
</div>
<div class="admonition-the-product-rule-the-sum-rule-and-bayes-rule admonition">
<p class="admonition-title">The Product Rule, the Sum Rule, and Bayes’ Rule</p>
<p>In the first line we applied the <strong>product rule</strong> of probability, which says that we can rewrite a joint distribution as a product of a <strong>marginal distribution</strong> and a <strong>conditional distribution</strong></p>
<div class="math notranslate nohighlight">
\[
p(x, y) = p(x) \, p(y \mid x).
\]</div>
<p>The order doesn’t matter; we could alternatively write,</p>
<div class="math notranslate nohighlight">
\[
p(x, y) = p(y) p(x \mid y).
\]</div>
<p>The marginal distributions <span class="math notranslate nohighlight">\(p(x)\)</span> and <span class="math notranslate nohighlight">\(p(y)\)</span> are obtained via the <strong>sum rule</strong>,</p>
<div class="math notranslate nohighlight">
\[
p(x) = \sum_{y \in \mathcal{Y}} p(x, y)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> is the support of the random variable <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>Finally, putting both together, we obtain <strong>Bayes’ rule</strong>,</p>
<div class="math notranslate nohighlight">
\[
p(x \mid y) = \frac{p(x, y)}{p(y)} = \frac{p(y \mid x) \, p(x)}{p(y)}.
\]</div>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Bars vs semicolons</p>
<p>Why do we sometimes use bars (<span class="math notranslate nohighlight">\(\mid\)</span>) and other times semicolons (;)?
Technically, we use the bar when we are writing a <strong>conditional distribution</strong> of one random variable given another, like <span class="math notranslate nohighlight">\(p(x \mid y)\)</span> in Bayes’ rule above. When we simply want to write a function that depends on some parameters, we use a semicolon, like <span class="math notranslate nohighlight">\(p(x; \theta)\)</span>.</p>
<p>So why did we switch from writing <span class="math notranslate nohighlight">\(p(\mathbf{x}; \lambda)\)</span> to <span class="math notranslate nohighlight">\(p(\mathbf{x} \mid \lambda)\)</span>? Because when we placed a prior on <span class="math notranslate nohighlight">\(\lambda\)</span>, we switched from treating <span class="math notranslate nohighlight">\(\lambda\)</span> as a parameter to instead thinking of it as a random variable. It’s a technical distinction, but we’ll try to be precise!</p>
</div>
</section>
<section id="bayesian-inference">
<h2>Bayesian inference<a class="headerlink" href="#bayesian-inference" title="Permalink to this heading">#</a></h2>
<p>What does it mean to “fit” the rate of a Poisson distribution under a gamma prior? Formally, we perform <strong>Bayesian inference</strong>.</p>
<p>We want to compute the <strong>posterior distribution</strong> of the rate <span class="math notranslate nohighlight">\(\lambda\)</span> <em>given</em> the observed spike counts <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> (and the prior parameters <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>, which are assumed fixed),</p>
<div class="math notranslate nohighlight">
\[
p(\lambda \mid \mathbf{x}).
\]</div>
<p>By Bayes’ rule (see box above), the posterior distribution is equal to the ratio of the <strong>joint distribution</strong> over the <strong>marginal distribution</strong>,</p>
<div class="math notranslate nohighlight">
\[
p(\lambda \mid \mathbf{x}) = \frac{p(\mathbf{x}, \lambda)}{p(\mathbf{x})}.
\]</div>
<p>Note that the denominator (the marginal distribution) does not depend on <span class="math notranslate nohighlight">\(\lambda\)</span>, so the posterior is proportional to the joint,</p>
<div class="math notranslate nohighlight">
\[
p(\lambda \mid \mathbf{x}) \propto p(\mathbf{x}, \lambda).
\]</div>
</section>
<section id="maximum-a-posteriori-inference">
<h2>Maximum <em>a posteriori</em> inference<a class="headerlink" href="#maximum-a-posteriori-inference" title="Permalink to this heading">#</a></h2>
<p>A simple summary of the posterior distribution is its <strong>mode</strong> — the point(s) where the pdf is maximized,</p>
<div class="math notranslate nohighlight">
\[
\lambda_{\mathsf{MAP}} = \text{arg max} \; p(\lambda \mid \mathbf{x}).
\]</div>
<p>or equivalently,</p>
<div class="math notranslate nohighlight">
\[
\lambda_{\mathsf{MAP}} = \text{arg max} \; p(\lambda, \mathbf{x}).
\]</div>
<p>since the posterior is proportional to the joint.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>True Bayesians cringe at MAP estimation! <em>How can a single point (the mode) summarize an entire distribution!?</em> It can’t, but we’ll use it for now and be better Bayesians later in the course.</p>
</div>
</section>
<section id="conjugate-priors">
<h2>Conjugate priors<a class="headerlink" href="#conjugate-priors" title="Permalink to this heading">#</a></h2>
<p>Now let’s go back and expand the Poisson pmf and the gamma pdf in the joint distribution,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\lambda \mid \mathbf{x}) &amp; \propto p(\mathbf{x}, \lambda) \\
&amp;= \left[ \prod_{t=1}^T \mathrm{Pois}(x_t \mid \lambda) \right] \, \mathrm{Ga}(\lambda; \alpha, \beta) \\
&amp;= \left[ \prod_{t=1}^T \frac{1}{x_t!} \lambda^{x_t} e^{-\lambda} \right] \, \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha - 1} e^{-\beta \lambda}
\end{align*}
\end{split}\]</div>
<p>Many of these terms can be combined! After simplifying,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\lambda \mid \mathbf{x}) 
&amp; \propto p(\mathbf{x}, \lambda) \\
&amp;= C \lambda^{\alpha' - 1} e^{-\beta' \lambda} \\
&amp;\propto \mathrm{Ga}(\lambda \mid \alpha', \beta')
\end{align*}
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\alpha' &amp;= \alpha + \sum_{t=1}^T x_t \\
\beta' &amp;= \beta + T
\end{align*}
\end{split}\]</div>
<p>and <span class="math notranslate nohighlight">\(C\)</span> is a constant with respect to <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>Thus, the posterior distribution is a gamma distribution, just like the prior! For this reason, we say the gamma is a <a class="reference external" href="https://en.wikipedia.org/wiki/Conjugate_prior"><strong>conjugate prior</strong></a> for the rate of the Poisson distribution.</p>
</section>
<section id="solving-for-the-map-estimate">
<h2>Solving for the MAP estimate<a class="headerlink" href="#solving-for-the-map-estimate" title="Permalink to this heading">#</a></h2>
<p>How do we solve for the MAP estimate, <span class="math notranslate nohighlight">\(\lambda_{\mathsf{MAP}} = \text{arg max} \; p(\lambda \mid \mathbf{x})\)</span>?</p>
<p>Now that you know the posterior is a gamma distribution, you can expand its pdf, take the log, take the derivative wrt <span class="math notranslate nohighlight">\(\lambda\)</span>, set it to zero and solve.</p>
<p>Or you can just go to the Wikipedia page on the <a class="reference external" href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma distribution</a> and see that its mode is</p>
<div class="math notranslate nohighlight">
\[
\lambda_{\mathsf{MAP}} = \frac{\alpha' - 1}{\beta'} = \frac{\alpha - 1 + \sum_{t=1}^T x_t}{\beta + T}
\]</div>
<p>for <span class="math notranslate nohighlight">\(\alpha' \geq 1\)</span>, and 0 otherwise.</p>
<div class="dropdown admonition">
<p class="admonition-title">Uninformative priors</p>
<p>When does the MAP estimate coincide with the MLE? Intuitively, that happens when the prior probability is flat, or <strong>uninformative</strong>. Unfortunately, it’s impossible to have a flat prior over the entire set of non-negative reals since the pdf has to integrate to one. However, we obtain an uninformative gamma prior in the limit that <span class="math notranslate nohighlight">\(\alpha \to 1\)</span> and <span class="math notranslate nohighlight">\(\beta \to 0\)</span>. In that limit, <span class="math notranslate nohighlight">\(\alpha' \to \sum_t x_t\)</span> and <span class="math notranslate nohighlight">\(\beta' \to T\)</span> so that <span class="math notranslate nohighlight">\(\lambda_{\mathsf{MAP}} \to \frac{1}{T} \sum_t x_t = \lambda_{\mathsf{MLE}}\)</span>.</p>
</div>
</section>
<section id="map-estimation-in-our-simulated-example">
<h2>MAP estimation in our simulated example<a class="headerlink" href="#map-estimation-in-our-simulated-example" title="Permalink to this heading">#</a></h2>
<p>Take the simulated data from above. What is the MAP estimate of the rate given the observed spike counts and the prior, and how does it compare to the MLE? We have…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha_post</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">xs</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">beta_post</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>

<span class="n">lambda_mle</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">lambda_map</span> <span class="o">=</span> <span class="p">(</span><span class="n">alpha_post</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">beta_post</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MLE:&quot;</span><span class="p">,</span> <span class="n">lambda_mle</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MAP:&quot;</span><span class="p">,</span> <span class="n">lambda_map</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MLE: tensor(5.0280)
MAP: tensor(5.0320)
</pre></div>
</div>
</div>
</div>
<div class="admonition-questions admonition">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>Why is the MAP estimate slightly higher than the MLE?</p></li>
<li><p>What if you make <span class="math notranslate nohighlight">\(T\)</span> smaller? Change the code above to sample <span class="math notranslate nohighlight">\(20\)</span> spike counts instead of 1000. How does that change the MAP/MLE difference?</p></li>
</ul>
</div>
</section>
<section id="mixture-models-and-latent-variables">
<h2>Mixture models and latent variables<a class="headerlink" href="#mixture-models-and-latent-variables" title="Permalink to this heading">#</a></h2>
<figure class="align-default" id="up-down">
<img alt="../_images/up_down.png" src="../_images/up_down.png" />
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Example of a mixture model with a latent variable <span class="math notranslate nohighlight">\(z_t\)</span> specifying whether the neuron is in the up or down state.</span><a class="headerlink" href="#up-down" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>We’ve already gotten a lot of mileage out of this simple Poisson example! However, real data will rarely be so simple. One way to build richer models is by introducing <strong>latent variables</strong>.</p>
<p>For example, suppose that at each time bin <span class="math notranslate nohighlight">\(t\)</span>, the neuron can either be in an <em>up</em> state with a high firing rate, or a <em>down</em> state with low firing rate.</p>
<p>Let <span class="math notranslate nohighlight">\(z_t \in \{0,1\}\)</span> be a latent variable (something we don’t explicitly observe) that denotes which state the neuron in is at that time, with 1 meaning <em>up</em> and 0 meaning <em>down</em>. Likewise, let <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_0\)</span> denote the corresponding firing rates, with <span class="math notranslate nohighlight">\(\lambda_1 &gt; \lambda_0\)</span>.</p>
<p>We will place gamma priors the firing rates, <span class="math notranslate nohighlight">\(\lambda_1 \sim \mathrm{Ga}(\alpha, \beta)\)</span> and <span class="math notranslate nohighlight">\(\lambda_0 \sim \mathrm{Ga}(\alpha, \beta)\)</span>. (We could get fancy here, but let’s keep it simple for now.)</p>
<p>Finally, assume that the latent variables are equally probable and independent across time. Formally, we can write that as a <a class="reference external" href="https://en.wikipedia.org/wiki/Categorical_distribution"><strong>categorical distribution</strong></a> with equal probabilities for both states,</p>
<div class="math notranslate nohighlight">
\[
z_t \sim \mathrm{Cat}([\tfrac{1}{2}, \tfrac{1}{2}]).
\]</div>
<p>The resulting model is called a <strong>mixture model</strong> because the marginal distribution, <span class="math notranslate nohighlight">\(p(x_t \mid \boldsymbol{\lambda})\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{\lambda} = (\lambda_0, \lambda_1)\)</span>, is a mixture of two Poisson distributions,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(x_t \mid \boldsymbol{\lambda}) 
&amp;= \sum_{z_t \in \{0,1\}} p(x_t, z_t \mid \boldsymbol{\lambda}) \\
&amp;= \sum_{z_t \in \{0,1\}} p(x_t \mid z_t, \boldsymbol{\lambda}) \, p(z_t) \\
&amp;= \frac{1}{2} \mathrm{Pois}(x_t \mid \lambda_0) + \frac{1}{2} \mathrm{Pois}(x_t \mid \lambda_1) 
\end{align*}
\end{split}\]</div>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p>What probability rule did we use to get this marginal distribution?</p>
</div>
</section>
<section id="sampling-a-poisson-mixture-model">
<h2>Sampling a Poisson mixture model<a class="headerlink" href="#sampling-a-poisson-mixture-model" title="Permalink to this heading">#</a></h2>
<p>Let’s draw samples from the mixture model, like we did above for the single Poisson distribution.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Poisson</span><span class="p">,</span> <span class="n">Categorical</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Construct a Poisson mixture model with down rate 1.0 and up rate 10.0</span>
<span class="n">rates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">])</span>
<span class="n">cat</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]))</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Sample the states from the categorical distribution</span>
<span class="n">zs</span> <span class="o">=</span> <span class="n">cat</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="n">T</span><span class="p">,))</span>

<span class="c1"># Sample the spike counts from a Poisson distribution </span>
<span class="c1"># using the rate for the corresponding state.</span>
<span class="c1"># Note: this uses PyTorch&#39;s broadcasting semantics.</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">rates</span><span class="p">[</span><span class="n">zs</span><span class="p">])</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

<span class="c1"># Compute the mixture probability at a range of bins</span>
<span class="n">bins</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> 
<span class="n">up_pmf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Poisson</span><span class="p">(</span><span class="n">rates</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">bins</span><span class="p">))</span>
<span class="n">down_pmf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Poisson</span><span class="p">(</span><span class="n">rates</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">bins</span><span class="p">))</span>
<span class="n">mixture_pmf</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">up_pmf</span> <span class="o">+</span> <span class="n">down_pmf</span><span class="p">)</span>

<span class="c1"># Plot a histogram of the samples and overlay the pmf</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bins</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">mixture_pmf</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;pmf&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;p(x)&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/000ebc2147783a62cc9820f4b9e44c96058431c1d186433a8f89c5e8de26923f.png" src="../_images/000ebc2147783a62cc9820f4b9e44c96058431c1d186433a8f89c5e8de26923f.png" />
</div>
</div>
<p>With this model, we can capture <strong>multimodal</strong> firing rate distributions.</p>
<div class="dropdown admonition">
<p class="admonition-title">The <code class="docutils literal notranslate"><span class="pre">MixtureSameFamily</span></code> class</p>
<p>We manually sampled the mixture distribution by first sampling the <code class="docutils literal notranslate"><span class="pre">Categorical</span></code> and then the <code class="docutils literal notranslate"><span class="pre">Poisson</span></code>. PyTorch also provides a <a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#mixturesamefamily"><code class="docutils literal notranslate"><span class="pre">MixtureSameFamily</span></code></a> class to do this in one step.</p>
</div>
</section>
<section id="fitting-a-mixture-model-by-coordinate-ascent">
<h2>Fitting a mixture model by coordinate ascent<a class="headerlink" href="#fitting-a-mixture-model-by-coordinate-ascent" title="Permalink to this heading">#</a></h2>
<p>Conceptually, fitting the mixture model is no different than fitting the the simple Poisson model above.</p>
<p>We will perform MAP estimation to find,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\mathbf{z}_{\mathsf{MAP}}, \boldsymbol{\lambda}_{\mathsf{MAP}}
= \text{arg max} \; p(\mathbf{z}, \boldsymbol{\lambda} \mid \mathbf{x})
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{z} = (z_1, \ldots, z_T)\)</span>. Again, this is equivalent to maximizing the joint probability.</p>
<p>Expanding the joint distribution over spike counts, latent variables, and rates,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\mathbf{x}, \mathbf{z}, \boldsymbol{\lambda})
&amp;= \left[ \prod_{t=1}^T p(x_t \mid z_t, \boldsymbol{\lambda}) \, p(z_t) \right]
p(\boldsymbol{\lambda})  \\
&amp;= \left[ \prod_{t=1}^T \mathrm{Pois}(x_t \mid \lambda_{z_t}) \times \frac{1}{2} \right] \, \mathrm{Ga}(\lambda_0; \alpha, \beta) \, \mathrm{Ga}(\lambda_1; \alpha, \beta)
\end{align*}
\end{split}\]</div>
<p>Our strategy for maximizing this objective is to iteratively maximize with respect to one variable at a time, holding the rest fixed. This is called <strong>coordinate ascent</strong>.</p>
<p>Fixing the rates, the most likely state at time <span class="math notranslate nohighlight">\(t\)</span> is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_t = 
\begin{cases} 
1 &amp; \text{if } \; \mathrm{Pois}(x_t \mid \lambda_1) \geq \mathrm{Pois}(x_t \mid \lambda_0) \\
0 &amp; \text{otherwise}
\end{cases}
\end{split}\]</div>
<p>Fixing the states, the most likely rates are</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\lambda_k &amp;= \frac{\alpha_k' -1}{\beta_k'} \\
\alpha_k' &amp;= \alpha + \sum_{t=1}^T x_t \, \mathbb{I}[z_t = k] \\
\beta_k' &amp;= \beta + \sum_{t=1}^T \mathbb{I}[z_t = k]
\end{align*}
\end{split}\]</div>
<p>for <span class="math notranslate nohighlight">\(k \in \{0, 1\}\)</span>.</p>
<div class="admonition-notation admonition">
<p class="admonition-title">Notation</p>
<p>The indicator function <span class="math notranslate nohighlight">\(\mathbb{I}[y]\)</span> equals 1 if the predicate <span class="math notranslate nohighlight">\(y\)</span> is true; otherwise it equals zero.</p>
</div>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>Derive these coordinate updates!</p>
<p>How would they change if the state were not equally probable <em>a priori</em>? That is, what if instead</p>
<div class="math notranslate nohighlight">
\[
z_t \sim \mathrm{Cat}([\pi_0, \pi_1])
\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi_0\)</span> and <span class="math notranslate nohighlight">\(\pi_1\)</span> are the probabilities of down and up states, respectively?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Connection to K-Means</p>
<p>You may have encountered the K-Means clustering algorithm in other courses. In K-Means, you alternate between assining datapoints to the closest cluster and then updating the cluster centroids to the mean of their assigned datapoints.</p>
<p>Our coordinate ascent algorithm is very closely related! You can think of K-Means as coordinate ascent in a Gaussian mixture model with identity covariance and uninformative priors. That will make more sense in later weeks when we introduce the Gaussian distribution.</p>
</div>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h2>
<p>This chapter introduced the basics of probabilistic modeling:</p>
<ul class="simple">
<li><p>We encountered 3 common distributions: Poisson, gamma, and categorical.</p></li>
<li><p>We learned how to construct joint distributions using the product rule, how to compute marginal distributions with the sum rule, and how to find the posterior distribution with Bayes’ rule.</p></li>
<li><p>We learned about maximum likelihood estimation (MLE) and maximum <em>a posteriori</em> (MAP) estimation.</p></li>
<li><p>We encountered conjugate priors where the posterior distribution is in the same family, making calculations particularly simple.</p></li>
<li><p>Finally, we learned how to construct more flexible models by introducing latent variables, and how to perform MAP estimation in those models using coordinate ascent.</p></li>
</ul>
<p>Next time, we’ll apply these concepts to our first real neural data analysis problem: spike sorting electrophysiological recordings.</p>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this heading">#</a></h2>
<p>There are many great references on probabilistic modeling. I like:</p>
<ul class="simple">
<li><p>Ch 2.1 and 2.2 of <span id="id1">[<a class="reference internal" href="99_references.html#id2" title="Kevin P. Murphy. Probabilistic Machine Learning: Advanced Topics. MIT Press, 2023. URL: http://probml.github.io/book2.">Murphy, 2023</a>]</span></p></li>
<li><p>Ch 1.2 of <span id="id2">[<a class="reference internal" href="99_references.html#id3" title="Christopher M Bishop. Pattern Recognition and Machine Learning. Springer, 2006. URL: https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.">Bishop, 2006</a>]</span></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

            </article>
            

            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
  <a class='left-prev' id="prev-link" href="../labs/06_arhmm.html" title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
          <p class="prev-next-subtitle">previous</p>
          <p class="prev-next-title">Lab 6: Autoregressive HMMs</p>
      </div>
  </a>
  <a class='right-next' id="next-link" href="03_neurobio.html" title="next page">
  <div class="prev-next-info">
      <p class="prev-next-subtitle">next</p>
      <p class="prev-next-title">Basic Neurobiology</p>
  </div>
  <i class="fa-solid fa-angle-right"></i>
  </a>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-example">
   Simple example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-from-a-poisson-distribution">
   Sampling from a Poisson distribution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-a-poisson-distribution">
   Fitting a Poisson distribution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#solving-for-the-mle">
   Solving for the MLE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adding-a-prior-distribution">
   Adding a prior distribution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-from-a-gamma-distribution">
   Sampling from a gamma distribution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-a-poisson-distribution-with-a-gamma-prior">
   Fitting a Poisson distribution with a gamma prior
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-inference">
   Bayesian inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-a-posteriori-inference">
   Maximum
   <em>
    a posteriori
   </em>
   inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conjugate-priors">
   Conjugate priors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#solving-for-the-map-estimate">
   Solving for the MAP estimate
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#map-estimation-in-our-simulated-example">
   MAP estimation in our simulated example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mixture-models-and-latent-variables">
   Mixture models and latent variables
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-a-poisson-mixture-model">
   Sampling a Poisson mixture model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-a-mixture-model-by-coordinate-ascent">
   Fitting a mixture model by coordinate ascent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   Further Reading
  </a>
 </li>
</ul>

</nav>
</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Scott Linderman
</p>

  </div>
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2022.<br>

</p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
Last updated on None.<br>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </div>
        </footer>
        

      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>

  </body>
</html>