

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Hidden Markov Models &#8212; Machine Learning Methods for Neural Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/13_hmms';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="References" href="99_references.html" />
    <link rel="prev" title="Mixture Models and the EM Algorithm" href="12_mixtures_em.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    <p class="title logo__title">Machine Learning Methods for Neural Data Analysis</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Labs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../labs/00_pytorch_primer.html">Lab 0: PyTorch Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/01_spike_sorting.html">Lab 1: Spike Sorting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/02_calcium_imaging.html">Lab 2: Calcium Deconvolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/03_pose_tracking.html">Lab 3: Markerless Pose Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/04_glms.html">Lab 4: Generalized Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/05_decoding.html">Lab 5: Bayesian Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/06_arhmm.html">Lab 6: Autoregressive HMMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/07_slds.html">Lab 7: Switching LDS</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="02_probabilistic_modeling.html">Probabilistic Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_neurobio.html">Basic Neurobiology</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit I: Signal Extraction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_simple_spike_sorting.html">Simple Spike Sorting</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_deconv_spike_sorting.html">Spike Sorting by Deconvolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_calcium_imaging.html">Demixing Calcium Imaging Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_pose_tracking.html">Markerless Pose Tracking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit II: Encoding &amp; Decoding</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="08_summary_stats.html">Summary Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_glm.html">Generalized Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_poisson_processes.html">Poisson Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_decoding.html">Decoding Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit III: Unsupervised Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="12_mixtures_em.html">Mixture Models and the EM Algorithm</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Hidden Markov Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="99_references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/13_hmms.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Hidden Markov Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-mixture-models">Gaussian Mixture Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-em-algorithm">The EM algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Hidden Markov Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-and-learning-in-hmm">Inference and Learning in HMM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-predictive-distributions">Computing the predictive distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-posterior-marginal-distributions">Computing the posterior marginal distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-backward-messages">Computing the backward messages</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-do-the-backward-messages-represent">What do the backward messages represent?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-posterior-pairwise-marginals">Computing the posterior pairwise marginals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizing-the-messages-for-numerical-stability">Normalizing the messages for numerical stability</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#em-for-hidden-markov-models">EM for Hidden Markov Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="hidden-markov-models">
<h1>Hidden Markov Models<a class="headerlink" href="#hidden-markov-models" title="Permalink to this heading">#</a></h1>
<p><strong>Hidden Markov Models (HMMs)</strong> are latent variable models for sequential data. Like the mixture models from the previous chapter, HMMs have discrete latent states. Unlike mixture models, the discrete latent states of an HMM are not independent: the state at time <span class="math notranslate nohighlight">\(t\)</span> depends on the state at time <span class="math notranslate nohighlight">\(t-1\)</span>. These dependencies allow the HMM to capture how states transition from one to another over time. Thanks to the <strong>Markovian</strong> dependency structure, posterior inference and parameter estimation in HMMs remain tractable.</p>
<section id="gaussian-mixture-models">
<h2>Gaussian Mixture Models<a class="headerlink" href="#gaussian-mixture-models" title="Permalink to this heading">#</a></h2>
<p>Recall the basic Gaussian mixture model,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    z_t &amp;\stackrel{\text{iid}}{\sim} \mathrm{Cat}(\boldsymbol{\pi}) \\
    x_t ~\vert~z_t &amp;\sim \mathcal{N}(\boldsymbol{\mu}_{z_t}, \boldsymbol{\Sigma}_{z_t})
\end{aligned}
\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(z_t \in \{1,\ldots,K\}\)</span> is a <strong>latent mixture assignment</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(x_t \in \mathbb{R}^D\)</span> is an <strong>observed data point</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\pi}\in \Delta_K\)</span>,
<span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k \in \mathbb{R}^D\)</span>, and
<span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_k \in \mathbb{R}_{\succeq 0}^{D \times D}\)</span> are
parameters</p></li>
</ul>
<p>(Here we’ve switched to indexing data points by <span class="math notranslate nohighlight">\(t\)</span> rather than <span class="math notranslate nohighlight">\(n\)</span>.)</p>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span> denote the set of parameters. We can be
Bayesian and put a prior on <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span> and run Gibbs or VI,
or we can point estimate <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span> with EM, etc.</p>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Draw the graphical model for a GMM.</p>
</div>
</section>
<section id="the-em-algorithm">
<h2>The EM algorithm<a class="headerlink" href="#the-em-algorithm" title="Permalink to this heading">#</a></h2>
<p>Recall the EM algorithm for mixture models,</p>
<ul>
<li><p><strong>E step:</strong> Compute the posterior distribution</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
                q(\boldsymbol{z}_{1:T})
                &amp;= p(\boldsymbol{z}_{1:T} ~\vert~\boldsymbol{x}_{1:T}; \boldsymbol{\Theta}) \\
                &amp;= \prod_{t=1}^T p(z_t ~\vert~\boldsymbol{x}_t; \boldsymbol{\Theta}) \\
                &amp;= \prod_{t=1}^T q_t(z_t)
    \end{aligned}
    \end{split}\]</div>
</li>
<li><p><strong>M step:</strong> Maximize the ELBO wrt <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
                \mathcal{L}(\boldsymbol{\Theta})
                &amp;= \mathbb{E}_{q(\boldsymbol{z}_{1:T})}\left[\log p(\boldsymbol{x}_{1:T}, \boldsymbol{z}_{1:T}; \boldsymbol{\Theta}) - \log q(\boldsymbol{z}_{1:T}) \right] \\
                &amp;= \mathbb{E}_{q(\boldsymbol{z}_{1:T})}\left[\log p(\boldsymbol{x}_{1:T}, \boldsymbol{z}_{1:T}; \boldsymbol{\Theta}) \right]  + c.
            \end{aligned}
    \end{split}\]</div>
</li>
</ul>
<p>For exponential family mixture models, the M-step only requires expected
sufficient statistics.</p>
</section>
<section id="id1">
<h2>Hidden Markov Models<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>Hidden Markov Models (HMMs) are like mixture models
with temporal dependencies between the mixture assignments.</p>
<!-- ::: {.center}
![image](figures/lap7/hmm.png){width="70%"}
::: -->
<p>This graphical model says that the joint distribution factors as,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
    p(z_{1:T}, \boldsymbol{x}_{1:T}) &amp;= p(z_1) \prod_{t=2}^T p(z_t ~\vert~z_{t-1}) \prod_{t=1}^T p(\boldsymbol{x}_t ~\vert~z_t).\end{aligned}
\]</div>
<p>We call this an HMM because the <em>hidden</em> states follow a Markov chain,
<span class="math notranslate nohighlight">\(p(z_1) \prod_{t=2}^T p(z_t ~\vert~z_{t-1})\)</span>.</p>
<p>An HMM consists of three components:</p>
<ol class="arabic simple">
<li><p><strong>Initial distribution:</strong>
<span class="math notranslate nohighlight">\(z_1 \sim \mathrm{Cat}(\boldsymbol{\pi}_0)\)</span></p></li>
<li><p><strong>Transition matrix:</strong>
<span class="math notranslate nohighlight">\(z_t \sim \mathrm{Cat}(\boldsymbol{P}_{z_{t-1}})\)</span> where
<span class="math notranslate nohighlight">\(\boldsymbol{P}\in [0,1]^{K \times K}\)</span> is a <em>row-stochastic</em>
transition matrix with rows <span class="math notranslate nohighlight">\(\boldsymbol{P}_k\)</span>.</p></li>
<li><p><strong>Emission distribution:</strong>
<span class="math notranslate nohighlight">\(\boldsymbol{x}_t \sim p(\cdot ~\vert~\boldsymbol{\theta}_{z_t})\)</span></p></li>
</ol>
</section>
<section id="inference-and-learning-in-hmm">
<h2>Inference and Learning in HMM<a class="headerlink" href="#inference-and-learning-in-hmm" title="Permalink to this heading">#</a></h2>
<p>We are interested in questions like:</p>
<ul class="simple">
<li><p>What are the <em>predictive distributions</em> of
<span class="math notranslate nohighlight">\(p(z_{t+1} ~\vert~\boldsymbol{x}_{1:t})\)</span>?</p></li>
<li><p>What is the <em>posterior marginal</em> distribution
<span class="math notranslate nohighlight">\(p(z_t ~\vert~\boldsymbol{x}_{1:T})\)</span>?</p></li>
<li><p>What is the <em>posterior pairwise marginal</em> distribution
<span class="math notranslate nohighlight">\(p(z_t, z_{t+1} ~\vert~\boldsymbol{x}_{1:T})\)</span>?</p></li>
<li><p>What is the <em>posterior mode</em>
<span class="math notranslate nohighlight">\(z_{1:T}^\star = \mathop{\mathrm{arg\,max}}p(z_{1:T} ~\vert~\boldsymbol{x}_{1:T})\)</span>?</p></li>
<li><p>How can we <em>sample the posterior</em>
<span class="math notranslate nohighlight">\(p(\boldsymbol{z}_{1:T} ~\vert~\boldsymbol{x}_{1:T})\)</span> of an HMM?</p></li>
<li><p>What is the <em>marginal likelihood</em> <span class="math notranslate nohighlight">\(p(\boldsymbol{x}_{1:T})\)</span>?</p></li>
<li><p>How can we <em>learn the parameters</em> of an HMM?</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>On the surface, what makes these inference problems harder than in the simple mixture model case?</p>
</div>
</section>
<section id="computing-the-predictive-distributions">
<h2>Computing the predictive distributions<a class="headerlink" href="#computing-the-predictive-distributions" title="Permalink to this heading">#</a></h2>
<p>The predictive distributions give the probability of the latent state <span class="math notranslate nohighlight">\(z_{t+1}\)</span> given observations <em>up to but not including</em> time <span class="math notranslate nohighlight">\(t+1\)</span>. Let,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        \alpha_{t+1}(z_{t+1}) &amp;\triangleq p(z_{t+1}, \boldsymbol{x}_{1:t}) \\
        &amp;= \sum_{z_1=1}^K \cdots \sum_{z_{t}=1}^K p(z_1) \prod_{s=1}^{t} p(\boldsymbol{x}_s ~\vert~z_s) \, p(z_{s+1} ~\vert~z_{s})\\
        &amp;= \sum_{z_{t}=1}^K \left[ \left( \sum_{z_1=1}^K \cdots \sum_{z_{t-1}=1}^K p(z_1) \prod_{s=1}^{t-1} p(\boldsymbol{x}_s ~\vert~z_s) \, p(z_{s+1} ~\vert~z_{s}) \right) p(\boldsymbol{x}_t ~\vert~z_t) \, p(z_{t+1} ~\vert~z_{t}) \right]  \\
        &amp;= \sum_{z_{t}=1}^K \alpha_t(z_t) \, p(\boldsymbol{x}_t ~\vert~z_t) \, p(z_{t+1} ~\vert~z_{t}).
\end{aligned}
\end{split}\]</div>
<p>We call <span class="math notranslate nohighlight">\(\alpha_t(z_t)\)</span> the <em>forward messages</em>. We
can compute them recursively! The base case is
<span class="math notranslate nohighlight">\(p(z_1 ~\vert~\varnothing) \triangleq p(z_1)\)</span>.</p>
<p>We can also write these
recursions in a vectorized form. Let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        \boldsymbol{\alpha}_t &amp;=
        \begin{bmatrix}
        \alpha_t(z_t = 1) \\
        \vdots \\
        \alpha_t(z_t=K)
        \end{bmatrix}
        =
        \begin{bmatrix}
        p(z_t = 1, \boldsymbol{x}_{1:t-1}) \\
        \vdots \\
        p(z_t = K, \boldsymbol{x}_{1:t-1})
        \end{bmatrix}
        \qquad \text{and} \qquad
        \boldsymbol{l}_t =
        \begin{bmatrix}
        p(\boldsymbol{x}_t ~\vert~z_t = 1) \\
        \vdots \\
        p(\boldsymbol{x}_t ~\vert~z_t = K)
        \end{bmatrix}
    \end{aligned}
\end{split}\]</div>
<p>both be vectors in <span class="math notranslate nohighlight">\(\mathbb{R}_+^K\)</span>. Then,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
        \boldsymbol{\alpha}_{t+1} &amp;= \boldsymbol{P}^\top (\boldsymbol{\alpha}_t \odot \boldsymbol{l}_t)
    \end{aligned}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\odot\)</span> denotes the Hadamard (elementwise)
product and <span class="math notranslate nohighlight">\(\boldsymbol{P}\)</span> is the transition matrix.</p>
<p>Finally, to get the
predictive distributions we just have to normalize,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
        p(z_{t+1} ~\vert~\boldsymbol{x}_{1:t}) &amp;\propto p(z_{t+1}, \boldsymbol{x}_{1:t}) = \alpha_{t+1}(z_{t+1}).
    \end{aligned}
\]</div>
<div class="tip admonition">
<p class="admonition-title">Question</p>
<p>What does the normalizing constant
tell us?</p>
</div>
</section>
<section id="computing-the-posterior-marginal-distributions">
<h2>Computing the posterior marginal distributions<a class="headerlink" href="#computing-the-posterior-marginal-distributions" title="Permalink to this heading">#</a></h2>
<p>The posterior marginal
distributions give the probability of the latent state <span class="math notranslate nohighlight">\(z_{t}\)</span> given
<em>all the observations</em> up to time <span class="math notranslate nohighlight">\(T\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        p(z_{t} ~\vert~\boldsymbol{x}_{1:T})
        &amp;\propto \sum_{z_1=1}^K \cdots \sum_{z_{t-1}=1}^K \sum_{z_{t+1}=1}^K \cdots \sum_{z_T=1}^K p(\boldsymbol{z}_{1:T}, \boldsymbol{x}_{1:T}) \\
        &amp;= \nonumber
        \bigg[ \sum_{z_{t}=1}^K \cdots \sum_{z_{t-1}=1}^K p(z_1) \prod_{s=1}^{t-1} p(\boldsymbol{x}_s ~\vert~z_s) \, p(z_{s+1} ~\vert~z_{s}) \bigg]
        \times  p(\boldsymbol{x}_t ~\vert~z_t)   \\
        &amp;\qquad \times \bigg[ \sum_{z_{t+1}=1}^K \cdots \sum_{z_T=1}^K \prod_{u=t+1}^T p(z_{u} ~\vert~z_{u-1}) \, p(\boldsymbol{x}_u ~\vert~z_u) \bigg] \\
        &amp;= \alpha_t(z_t) \times p(\boldsymbol{x}_t ~\vert~z_t) \times \beta_t(z_t)
    \end{aligned}
\end{split}\]</div>
<p>where we have introduced the <em>backward messages</em>
<span class="math notranslate nohighlight">\(\beta_t(z_t)\)</span>.</p>
</section>
<section id="computing-the-backward-messages">
<h2>Computing the backward messages<a class="headerlink" href="#computing-the-backward-messages" title="Permalink to this heading">#</a></h2>
<p>The backward messages can be computed
recursively too,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        \beta_t(z_t)
        &amp;\triangleq \sum_{z_{t+1}=1}^K \cdots \sum_{z_T=1}^K \prod_{u=t+1}^T p(z_{u} ~\vert~z_{u-1}) \, p(\boldsymbol{x}_u ~\vert~z_u) \\
        &amp;= \sum_{z_{t+1}=1}^K p(z_{t+1} ~\vert~z_t) \, p(\boldsymbol{x}_{t_1} ~\vert~z_{t+1}) \left(\sum_{z_{t+2}=1}^K \cdots \sum_{z_T=1}^K \prod_{u=t+2}^T p(z_{u} ~\vert~z_{u-1}) \, p(\boldsymbol{x}_u ~\vert~z_u) \right) \\
        &amp;= \sum_{z_{t+1}=1}^K p(z_{t+1} ~\vert~z_t) \, p(\boldsymbol{x}_{t_1} ~\vert~z_{t+1}) \, \beta_{t+1}(z_{t+1}).
    \end{aligned}
\end{split}\]</div>
<p>For the base case, let <span class="math notranslate nohighlight">\(\beta_T(z_T) = 1\)</span>.</p>
<p>Let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        \boldsymbol{\beta}_t &amp;=
        \begin{bmatrix}
        \beta_t(z_t = 1) \\
        \vdots \\
        \beta_t(z_t=K)
        \end{bmatrix}
    \end{aligned}
\end{split}\]</div>
<p>be a vector in <span class="math notranslate nohighlight">\(\mathbb{R}_+^K\)</span>. Then,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
        \boldsymbol{\beta}_{t} &amp;= \boldsymbol{P}(\boldsymbol{\beta}_{t+1} \odot \boldsymbol{l}_{t+1}).
    \end{aligned}
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_T = \boldsymbol{1}_K\)</span>.</p>
<p>Now we have everything we need to compute the posterior marginal,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
        p(z_t = k ~\vert~\boldsymbol{x}_{1:T}) &amp;= \frac{\alpha_{t,k} \, l_{t,k} \, \beta_{t,k}}{\sum_{j=1}^K \alpha_{t,j} l_{t,j} \beta_{t,j}}.
    \end{aligned}
\]</div>
<p>We just derived the <strong>forward-backward algorithm</strong> for
HMMs <span id="id2">[<a class="reference internal" href="99_references.html#id80" title="Lawrence Rabiner and Biinghwang Juang. An introduction to hidden Markov models. ieee assp magazine, 3(1):4–16, 1986.">Rabiner and Juang, 1986</a>]</span>].}`</p>
<section id="what-do-the-backward-messages-represent">
<h3>What do the backward messages represent?<a class="headerlink" href="#what-do-the-backward-messages-represent" title="Permalink to this heading">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>If the forward
messages represent the predictive probabilities
<span class="math notranslate nohighlight">\(\alpha_{t+1}(z_{t+1}) = p(z_{t+1}, \boldsymbol{x}_{1:t})\)</span>, what do the
backward messages represent?</p>
</div>
</section>
</section>
<section id="computing-the-posterior-pairwise-marginals">
<h2>Computing the posterior pairwise marginals<a class="headerlink" href="#computing-the-posterior-pairwise-marginals" title="Permalink to this heading">#</a></h2>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Use the forward
and backward messages to compute the posterior pairwise marginals
<span class="math notranslate nohighlight">\(p(z_t, z_{t+1} ~\vert~\boldsymbol{x}_{1:T})\)</span>.</p>
</div>
</section>
<section id="normalizing-the-messages-for-numerical-stability">
<h2>Normalizing the messages for numerical stability<a class="headerlink" href="#normalizing-the-messages-for-numerical-stability" title="Permalink to this heading">#</a></h2>
<p>If you’re working with
long time series, especially if you’re working with 32-bit floating
point, you need to be careful.</p>
<p>The messages involve products of probabilities, which can quickly
overflow.</p>
<p>There’s a simple fix though: after each step, re-normalize the messages
so that they sum to one. I.e replace</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
        \boldsymbol{\alpha}_{t+1} &amp;= \boldsymbol{P}^\top (\boldsymbol{\alpha}_t \odot \boldsymbol{l}_t)
\end{aligned}
\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        \widetilde{\boldsymbol{\alpha}}_{t+1} &amp;= \frac{1}{A_t} \boldsymbol{P}^\top (\widetilde{\boldsymbol{\alpha}}_t \odot \boldsymbol{l}_t) \\
        A_t &amp;= \sum_{k=1}^K \sum_{j=1}^K P_{jk} \widetilde{\alpha}_{t,j} l_{t,j}
            \equiv \sum_{j=1}^K \widetilde{\alpha}_{t,j} l_{t,j} \quad \text{(since $\boldsymbol{P}$ is row-stochastic)}.
    \end{aligned}
\end{split}\]</div>
<p>This leads to a nice interpretation: The normalized
messages are predictive likelihoods
<span class="math notranslate nohighlight">\(\widetilde{\alpha}_{t+1,k} = p(z_{t+1}=k ~\vert~\boldsymbol{x}_{1:t})\)</span>,
and the normalizing constants are
<span class="math notranslate nohighlight">\(A_t = p(\boldsymbol{x}_t ~\vert~\boldsymbol{x}_{1:t-1})\)</span>.</p>
</section>
<section id="em-for-hidden-markov-models">
<h2>EM for Hidden Markov Models<a class="headerlink" href="#em-for-hidden-markov-models" title="Permalink to this heading">#</a></h2>
<p>Now we can put it all together. To perform
EM in an HMM,</p>
<ul>
<li><p><strong>E step:</strong> Compute the posterior distribution
$<span class="math notranslate nohighlight">\(
\begin{aligned}
            q(\boldsymbol{z}_{1:T})
            &amp;= p(\boldsymbol{z}_{1:T} ~\vert~\boldsymbol{x}_{1:T}; \boldsymbol{\Theta}).
        \end{aligned}
\)</span>$</p>
<p>(Really, run the <strong>forward-backward algorithm</strong> to get posterior marginals and pairwise marginals.)</p>
</li>
<li><p><strong>M step:</strong> Maximize the ELBO wrt <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
                \mathcal{L}(\boldsymbol{\Theta})
                &amp;= \mathbb{E}_{q(\boldsymbol{z}_{1:T})}\left[\log p(\boldsymbol{x}_{1:T}, \boldsymbol{z}_{1:T}; \boldsymbol{\Theta}) \right]  + c \\
                &amp;= \nonumber \mathbb{E}_{q(\boldsymbol{z}_{1:T})}\left[\sum_{k=1}^K \mathbb{I}[z_1=k]\log \pi_{0,k} \right] +
                \mathbb{E}_{q(\boldsymbol{z}_{1:T})}\left[\sum_{t=1}^{T-1} \sum_{i=1}^K \sum_{j=1}^K \mathbb{I}[z_t=i, z_{t+1}=j]\log P_{i,j} \right] \\
                &amp;\qquad + \mathbb{E}_{q(\boldsymbol{z}_{1:T})}\left[\sum_{t=1}^T \sum_{k=1}^K \mathbb{I}[z_t=k]\log p(\boldsymbol{x}_t; \theta_k) \right]
    \end{aligned}
    \end{split}\]</div>
</li>
</ul>
<p>For exponential family observations, the M-step only requires expected
sufficient statistics.</p>
<div class="tip admonition">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>How can we sample the posterior?</p></li>
<li><p>How can we find the posterior mode?</p></li>
<li><p>How can we choose the number of states?</p></li>
<li><p>What if my transition matrix is sparse?</p></li>
</ul>
</div>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h2>
<p>HMMs add temporal dependencies to the latent states of mixture models. They’re a simple yet powerful model for sequential data.</p>
<p>The emission distribution can be extended in many ways. For example, in <a class="reference internal" href="../labs/06_arhmm.html"><span class="doc std std-doc">Lab 6</span></a>, you will implement both Gaussian and <strong>Autoregressive (AR) HMMs</strong>.</p>
<p>Like mixture models, we can derive efficient <strong>stochastic EM</strong> algorithms for HMMs, which keep rolling averages of sufficient statistics across mini-batches (e.g., individual trials from a collection of sequences).</p>
<p>It’s always good to implement models and algorithms yourself at least once, like you will in Lab 6. Going forward, you should check out our implementations in <a class="reference external" href="https://probml.github.io/dynamax/">Dynamax</a> and <a class="reference external" href="https://lindermanlab.github.io/ssm-docs/">SSM</a>!</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="12_mixtures_em.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Mixture Models and the EM Algorithm</p>
      </div>
    </a>
    <a class="right-next"
       href="99_references.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">References</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-mixture-models">Gaussian Mixture Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-em-algorithm">The EM algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Hidden Markov Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-and-learning-in-hmm">Inference and Learning in HMM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-predictive-distributions">Computing the predictive distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-posterior-marginal-distributions">Computing the posterior marginal distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-backward-messages">Computing the backward messages</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-do-the-backward-messages-represent">What do the backward messages represent?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-posterior-pairwise-marginals">Computing the posterior pairwise marginals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizing-the-messages-for-numerical-stability">Normalizing the messages for numerical stability</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#em-for-hidden-markov-models">EM for Hidden Markov Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Scott Linderman
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>