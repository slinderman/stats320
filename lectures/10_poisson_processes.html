
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Poisson Processes &#8212; Machine Learning Methods for Neural Data Analysis</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Decoding Models" href="11_decoding.html" />
    <link rel="prev" title="Generalized Linear Models" href="09_glm.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Methods for Neural Data Analysis</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Labs
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/00_pytorch_primer.html">
   Lab 0: PyTorch Primer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/01_spike_sorting.html">
   Lab 1: Spike Sorting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/02_calcium_imaging.html">
   Lab 2: Calcium Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/03_pose_tracking.html">
   Lab 3: Markerless Pose Tracking
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/04_glms.html">
   Lab 4: Generalized Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/05_decoding.html">
   Lab 5: Bayesian Decoding
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="02_probabilistic_modeling.html">
   Probabilistic Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_neurobio.html">
   Basic Neurobiology
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unit I: Signal Extraction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="04_simple_spike_sorting.html">
   Simple Spike Sorting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_deconv_spike_sorting.html">
   Spike Sorting by Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_calcium_imaging.html">
   Demixing Calcium Imaging Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_pose_tracking.html">
   Markerless Pose Tracking
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unit II: Encoding &amp; Decoding
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="08_summary_stats.html">
   Summary Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_glm.html">
   Generalized Linear Models
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Poisson Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_decoding.html">
   Decoding Models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="99_references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/lectures/10_poisson_processes.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#definition">
   Definition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-a-poisson-process">
   Sampling a Poisson process
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interval-distribution">
   Interval distribution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#poisson-process-likelihood">
   Poisson process likelihood
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-estimation">
   Maximum likelihood estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#limit-of-the-discrete-time-model">
   Limit of the discrete-time model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#renewal-processes">
   Renewal processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conditional-intensity-functions">
   Conditional intensity functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hawkes-processes">
   Hawkes processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multivariate-hawkes-processes">
   Multivariate Hawkes processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-estimation-for-hawkes-processes">
   Maximum likelihood estimation for Hawkes processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#poisson-superposition-and-thinning">
   Poisson superposition and thinning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hawkes-processes-as-cascades-of-poisson-processes">
   Hawkes processes as cascades of Poisson processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nonlinear-hawkes-processes">
   Nonlinear Hawkes processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Poisson Processes</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#definition">
   Definition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-a-poisson-process">
   Sampling a Poisson process
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interval-distribution">
   Interval distribution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#poisson-process-likelihood">
   Poisson process likelihood
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-estimation">
   Maximum likelihood estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#limit-of-the-discrete-time-model">
   Limit of the discrete-time model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#renewal-processes">
   Renewal processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conditional-intensity-functions">
   Conditional intensity functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hawkes-processes">
   Hawkes processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multivariate-hawkes-processes">
   Multivariate Hawkes processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-estimation-for-hawkes-processes">
   Maximum likelihood estimation for Hawkes processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#poisson-superposition-and-thinning">
   Poisson superposition and thinning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hawkes-processes-as-cascades-of-poisson-processes">
   Hawkes processes as cascades of Poisson processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nonlinear-hawkes-processes">
   Nonlinear Hawkes processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="poisson-processes">
<h1>Poisson Processes<a class="headerlink" href="#poisson-processes" title="Permalink to this headline">#</a></h1>
<p>The preceding chapter discussed Poisson GLMs for neural spike counts. Changing notation slightly, we had,</p>
<div class="math notranslate nohighlight">
\[
y_{i} \sim \mathrm{Po}(\lambda_i \Delta )
\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i \in \mathbb{N}_0\)</span> denotes the number of spikes in the <span class="math notranslate nohighlight">\(i\)</span>-th time bin, <span class="math notranslate nohighlight">\(\lambda_i \in \mathbb{R}_+\)</span> is the firing rate (in spikes/second), and <span class="math notranslate nohighlight">\(\Delta \in \mathbb{R}_+\)</span> is the bin width (in seconds).</p>
<p>What happens if we take the bin width to zero? Then all but a finite number of bins will have zero counts, and we can instead represent the data as an unordered set of <strong>spike times</strong> <span class="math notranslate nohighlight">\(\{t_n\}_{n=1}^N \subset [0, T)\)</span>. Instead of a vector of Poisson-distributed spike counts, we can model the spike times as a realization of a <strong>Poisson process</strong>.</p>
<section id="definition">
<h2>Definition<a class="headerlink" href="#definition" title="Permalink to this headline">#</a></h2>
<p>A Poisson process is a <strong>stochastic process</strong> that generates sets of points, like the set of spike times above. It is defined by an intensity function or <strong>firing rate</strong>, <span class="math notranslate nohighlight">\(\lambda(t): [0, T) \mapsto \mathbb{R}_+\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(N(\mathcal{A}) = |\{n: t_n \in \mathcal{A}\}|\)</span> denote the number of points in the set <span class="math notranslate nohighlight">\(\mathcal{A} \subseteq [0,T)\)</span>. For example, an interval <span class="math notranslate nohighlight">\(\mathcal{A} = [a_0, a_1)\)</span>. Let <span class="math notranslate nohighlight">\(N(t)\)</span> be shorthand for <span class="math notranslate nohighlight">\(N([0,t))\)</span> that denotes the <strong>counting function</strong>, which specifies the number of points up to time <span class="math notranslate nohighlight">\(t\)</span>. Assume <span class="math notranslate nohighlight">\(N(0) = 0\)</span>.</p>
<p>A Poisson process has two defining properties:</p>
<ol>
<li><p>The number of points in an interval is <strong>Poisson distributed</strong> with expectation given by the integrated intensity function,</p>
<div class="math notranslate nohighlight">
\[N(\mathcal{A}) \sim \mathrm{Po}\left( \int_{\mathcal{A}} \lambda(t) \, \mathrm{d}t\right).\]</div>
</li>
<li><p>The number of points in <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> is <strong>independent</strong> of the number of points in a disjoint interval <span class="math notranslate nohighlight">\(\mathcal{A}'\)</span>,</p>
<div class="math notranslate nohighlight">
\[N(\mathcal{A}) \perp \!\!\! \perp N(\mathcal{A}')\]</div>
<p>if <span class="math notranslate nohighlight">\(\mathcal{A} \cap \mathcal{A}' = \varnothing\)</span>.</p>
</li>
</ol>
<p>A <strong>homogeneous</strong> Poisson process has a constant intensity function, <span class="math notranslate nohighlight">\(\lambda(t) \equiv \lambda\)</span>. Otherwise, the process is called <strong>inhomogeneous</strong>.</p>
</section>
<section id="sampling-a-poisson-process">
<h2>Sampling a Poisson process<a class="headerlink" href="#sampling-a-poisson-process" title="Permalink to this headline">#</a></h2>
<p>The two properties above imply a method for sampling a Poisson process. First, sample the total number of points from its Poisson distribution,</p>
<div class="math notranslate nohighlight">
\[
N \sim \mathrm{Po} \left(\int_0^T \lambda(t) \, \mathrm{d}t \right).
\]</div>
<p>Then sample the locations of the points by independently sampling from the normalized intensity,</p>
<div class="math notranslate nohighlight">
\[
t_n \overset{\text{iid}}{\sim} \frac{\lambda(t)}{\int_0^T \lambda(t) \, \mathrm{d} t},
\]</div>
<p>to obtain the unordered set <span class="math notranslate nohighlight">\(\{t_n\}_{n=1}^N\)</span>.</p>
</section>
<section id="interval-distribution">
<h2>Interval distribution<a class="headerlink" href="#interval-distribution" title="Permalink to this headline">#</a></h2>
<p>For a homogeneous Poisson process, the expected number of points is <span class="math notranslate nohighlight">\(\mathbb{E}[N] = \lambda T\)</span>, and the locations are independently sampled according to a uniform distribution over <span class="math notranslate nohighlight">\([0, T)\)</span>.  The <strong>intervals</strong> between points, <span class="math notranslate nohighlight">\(\delta_n = t_{(n)} - t_{(n-1)}\)</span>, where <span class="math notranslate nohighlight">\(t_{(n)}\)</span> is the <span class="math notranslate nohighlight">\(n\)</span>-th spike time in the <em>ordered</em> set, are <strong>exponential random variables</strong>,</p>
<div class="math notranslate nohighlight">
\[\delta_{n} \overset{\text{iid}}{\sim} \mathrm{Exp}(\lambda).\]</div>
<p>This suggests another means of sampling a homogeneous Poisson process: sample intervals independently until the total elapsed time exceeds <span class="math notranslate nohighlight">\(T\)</span>.</p>
<p>As a sanity check, note that the expected value of the intervals is <span class="math notranslate nohighlight">\(\mathbb{E}[\delta_n] = \frac{1}{\lambda}\)</span>, so we should get about <span class="math notranslate nohighlight">\(\lambda T\)</span> points for large <span class="math notranslate nohighlight">\(T\)</span>.</p>
<p>The exponential distribution is <strong>memoryless</strong>: the distribution of time until the next point arrives is independent of how much time has elapsed since the last point. More formally,</p>
<div class="math notranslate nohighlight">
\[
\Pr(\delta_n &gt; a + b \mid \delta_n &gt; a) = \Pr(\delta_n &gt; b).
\]</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The memoryless property of Poisson processes should give you pause. Don’t neurons have a refractory period, which sets a lower bound on the interval between spikes? Incorporating these types of dependencies will require us to move beyond Poisson processes, as discussed below.</p>
</div>
</section>
<section id="poisson-process-likelihood">
<h2>Poisson process likelihood<a class="headerlink" href="#poisson-process-likelihood" title="Permalink to this headline">#</a></h2>
<p>From the two-step sampling procedure above, we can derive the likelihood of a set of points under a Poisson process,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
p(\{t_n\}_{n=1}^N; \lambda)
&amp;= \exp\left\{-\int_0^T \lambda(t) \, \mathrm{d}t \right\} \prod_{n=1}^N \lambda(t_n)
\end{aligned}
\]</div>
<div class="admonition-derivation admonition">
<p class="admonition-title">Derivation</p>
<p>The sampling procedure has two steps: sample the number of points, then sample their location. Thus,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
p(\{t_n\}_{n=1}^N; \lambda)
&amp;= p(N) \times \left[ \prod_{n=1}^N p(t_n) \right] \times N!
\end{aligned}
\]</div>
<p>Where does that lagging <span class="math notranslate nohighlight">\(N!\)</span> come from? The likelihood is defined over <strong>unordered sets of points</strong> <span class="math notranslate nohighlight">\(\{t_n\}_{n=1}^N\)</span>. The product over <span class="math notranslate nohighlight">\(n\)</span> implicitly assumes an ordering. Since we could obtain the same output from any of the <span class="math notranslate nohighlight">\(N!\)</span> permutations, the likelihood needs to be multiplied by a factor of <span class="math notranslate nohighlight">\(N!\)</span>.</p>
<p>Now expand the equation above and simplify,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
p(\{t_n\}_{n=1}^N; \lambda)
&amp;= \mathrm{Po}\left(N; \int_0^T \lambda(t) \, \mathrm{d}t \right) \times \left[ \prod_{n=1}^N \frac{\lambda(t)}{\int_0^T \lambda(t) \, \mathrm{d} t} \right]  \times N! \\
&amp;= \frac{1}{N!} \left(\int_0^T \lambda(t) \, \mathrm{d}t \right)^N \exp\left\{-\int_0^T \lambda(t) \, \mathrm{d}t \right\} \times \left[ \prod_{n=1}^N \frac{\lambda(t)}{\int_0^T \lambda(t) \, \mathrm{d} t} \right]  \times N! \\
&amp;= \exp\left\{-\int_0^T \lambda(t) \, \mathrm{d}t \right\} \prod_{n=1}^N \lambda(t_n)
\end{aligned}
\end{split}\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Derive the likelihood of the set of intervals <span class="math notranslate nohighlight">\(\{\delta_n\}_{n=1}^N\)</span> corresponding to the points <span class="math notranslate nohighlight">\(\{t_n\}_{n=1}^N\)</span>. You should get the same form as above.</p>
</div>
</section>
<section id="maximum-likelihood-estimation">
<h2>Maximum likelihood estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this headline">#</a></h2>
<p>In practice, we often want to estimate the intensity function from data. For example, we may assume the intensity function has a parametric form, <span class="math notranslate nohighlight">\(\lambda(t; \theta)\)</span>, with parameters <span class="math notranslate nohighlight">\(\theta\)</span>. We could estimate the parameters by maximum likelihood estimation, using the Poisson process likelihood above. Note that the likelihood is a concave function of <span class="math notranslate nohighlight">\(\lambda\)</span>, making it amenable to optimization.</p>
<p>The challenge is the integrated intensity in the exponent. For many models, there is no closed-form solution for the integral. For temporal point processes like the ones considered in this chapter, this is simply a one-dimensional integral. We can approximate it using numerical quadrature rules, as long as we can evaluate <span class="math notranslate nohighlight">\(\lambda(t; \theta)\)</span> for any <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>Some classes of models do afford closed-form integration. For example, consider a model of the intensity as a weighted sum of basis functions,</p>
<div class="math notranslate nohighlight">
\[
\lambda(t; \theta) = \sum_{b=1}^B w_b \phi_b(t)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta = (w_1, \ldots, w_B)^\top \in \mathbb{R}_+^B\)</span> are non-negative weights and <span class="math notranslate nohighlight">\(\phi_b: [0,T) \mapsto \mathbb{R}_+\)</span> are non-negative <strong>basis functions</strong>. Then,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\int \lambda(t; \theta) \, \mathrm{d} t 
&amp;= \sum_{b=1}^B w_b C_b.
\end{aligned}
\]</div>
<p>where <span class="math notranslate nohighlight">\(C_b = \int \phi_b(t) \, \mathrm{d} t\)</span>.</p>
<p>Since we get to choose the basis functions, we can simply choose functions that are easy to integrate, like continuous univariate probability densities. In the special case of <strong>rectangular</strong> basis functions of width <span class="math notranslate nohighlight">\(\Delta\)</span>,</p>
<div class="math notranslate nohighlight">
\[\phi_b(t) = \mathbb{I}\big[t \in [b\Delta, (b+1)\Delta) \big],\]</div>
<p>we obtain a piecewise-constant model for the intensity function. This brings us back to the discrete-time model for spike counts from the last chapter.</p>
</section>
<section id="limit-of-the-discrete-time-model">
<h2>Limit of the discrete-time model<a class="headerlink" href="#limit-of-the-discrete-time-model" title="Permalink to this headline">#</a></h2>
<p>We can arrive at a Poisson process by taking a limit of the discrete-time model for spike counts. Intuitively, think of discrete time model as a Poisson process with <strong>piecewise constant intensity</strong>,</p>
<div class="math notranslate nohighlight">
\[
\lambda(t) = \lambda_{i(t)},
\]</div>
<p>where <span class="math notranslate nohighlight">\(i(t) = \lfloor \frac{t}{\Delta} \rfloor\)</span> is the index of the time bin corresponding to continuous time <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(\lambda_{i(t)}\)</span> is the constant intensity in that bin. The discrete-time model ignored the precise timing of spikes within each bin and simply focused on the spike counts.</p>
<p>Sampling a Poisson process with piecewise constant intensity is straightforward: each bin is an independent, homogenous Poisson process with its own intensity. More precisely, sample</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
y_i &amp;\sim \mathrm{Po}(\lambda_{i(t)} \Delta) &amp; \text{for } i&amp;=1,\ldots, \lfloor \tfrac{T}{\Delta} \rfloor \\
t_{i,n} &amp;\overset{\text{iid}}{\sim} \mathrm{Unif}([i \Delta, (i+1)\Delta)) &amp; \text{for } n&amp;=1,\ldots,y_i
\end{aligned}
\end{split}\]</div>
<p>Then return the union of all spikes, <span class="math notranslate nohighlight">\(\cup_{i} \{t_{i,n}\}_{n=1}^{y_i}\)</span>. The total number of spikes is <span class="math notranslate nohighlight">\(N=\sum_i y_i\)</span>.</p>
<p>Now let’s derive the likelihood,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
p(\{t_n\}_{n=1}^N; \lambda)
&amp;= \prod_{i=1}^{\lfloor \frac{T}{\Delta} \rfloor} p(\{t_{i,n}\}_{n=1}^{y_i}) \\
&amp;= \prod_{i=1}^{\lfloor \frac{T}{\Delta} \rfloor} \mathrm{Po}(y_i; \lambda_{i(t)} \Delta) \left[ \prod_{n=1}^{y_i} \mathrm{Unif}(t_{i,n}; [i\Delta, (i+1)\Delta)) \right] y_i! \\
&amp;= \prod_{i=1}^{\lfloor \frac{T}{\Delta} \rfloor} \frac{1}{y_i!} (\lambda_{i(t)} \Delta)^{y_i} e^{-\lambda_{i(t)} \Delta} \left(\tfrac{1}{\Delta}\right)^{y_i} y_i! \\
&amp;= \prod_{i=1}^{\lfloor \frac{T}{\Delta} \rfloor} \lambda_{i(t)}^{y_i} e^{-\lambda_{i(t)} \Delta} \\
&amp;= \exp \bigg\{-\sum_{i=1}^{\lfloor \frac{T}{\Delta} \rfloor} \lambda_{i(t)} \Delta \bigg\} \prod_{i=1}^{\lfloor \frac{T}{\Delta} \rfloor} \lambda_{i(t)}^{y_i} \\
\end{aligned}
\end{split}\]</div>
<p>Now consider the limit as the bin width <span class="math notranslate nohighlight">\(\Delta\)</span> goes to zero. The sum in the exponent is a Riemann sum, and its limit is the integral of the intensity function. Moreover, as the bin width goes to zero, so does the probability of having more than one spike in a bin. In other words, <span class="math notranslate nohighlight">\(y_i\)</span> is either zero or one. Since <span class="math notranslate nohighlight">\(\lambda^0 = 1\)</span>, we can write the likelihood as a product of instantaneous intensities at the time of each spike,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
p(\{t_n\}_{n=1}^N; \lambda)
&amp;= \lim_{\Delta \to 0} \; \exp \bigg\{-\sum_{i=1}^{\lfloor \frac{T}{\Delta} \rfloor} \lambda_{i(t)} \Delta \bigg\} \prod_{i=1}^{\lfloor \frac{T}{\Delta} \rfloor} \lambda_{i(t)}^{y_i} \\
&amp;= \exp \bigg\{ -\int_0^T \lambda(t) \, \mathrm{d} t \bigg\} \prod_{n=1}^N \lambda(t_n)
\end{aligned}
\end{split}\]</div>
<div class="dropdown admonition">
<p class="admonition-title">Simplicity and orderliness</p>
<p>A point process is <strong>simple</strong> if the probability of two spikes at the same time is zero.</p>
<p>A point process is said to be <strong>orderly</strong> or <strong>regular</strong> if</p>
<div class="math notranslate nohighlight">
\[
\Pr(N([t,t+\Delta)) &gt; 1) = o(\Delta)
\]</div>
<p>where <span class="math notranslate nohighlight">\(o(\Delta)\)</span> is <em>little-oh notation</em> that means <span class="math notranslate nohighlight">\(\lim_{\Delta \to 0} o(\Delta)/\Delta = 0\)</span>. Orderliness implies simplicity.</p>
<p>As long as the intensity <span class="math notranslate nohighlight">\(\lambda\)</span> has no atoms, a Poisson process is orderly and simple. We used this property when we claimed that <span class="math notranslate nohighlight">\(y_i\)</span> is either zero or one in the limit as <span class="math notranslate nohighlight">\(\Delta\)</span> goes to zero. To prove it more formally, note that in the piecewise constant model,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\Pr(y_i = 0) &amp;= e^{-\lambda_{i(t)} \Delta} \\
\Pr(y_i = 1) &amp;= \lambda_{i(t)} \Delta e^{-\lambda_{i(t)} \Delta} \\
\Pr(y_i = 2) &amp;= \frac{(\lambda_{i(t)} \Delta)^2}{2} e^{-\lambda_{i(t)} \Delta}
\end{aligned}
\end{split}\]</div>
<p>For small <span class="math notranslate nohighlight">\(\Delta\)</span>, we can use a Taylor approximation <span class="math notranslate nohighlight">\(e^{\lambda_{i(t)} \Delta} = 1 + \lambda_{i(t)} \Delta + o(\Delta)\)</span> where <span class="math notranslate nohighlight">\(o(\Delta)\)</span> is <em>little-oh notation</em> that means <span class="math notranslate nohighlight">\(\lim_{\Delta \to 0} o(\Delta)/\Delta = 0\)</span>. It captures the second and higher order terms in the Taylor approximation.  In this limit,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\Pr(y_t = 0) &amp;= 1 -\lambda_{i(t)} \Delta + o(\Delta) \\
\Pr(y_t = 1) &amp;= \lambda_{i(t)} \Delta + o(\Delta) \\
\Pr(y_t \geq 2) &amp;= o(\Delta) 
\end{aligned}
\end{split}\]</div>
<p>The last line says that the probability of seeing more than one spike goes to zero faster than <span class="math notranslate nohighlight">\(\Delta\)</span> does, which justifies our claim that <span class="math notranslate nohighlight">\(y_i\)</span> is binary in the limit of small bin sizes.</p>
</div>
</section>
<section id="renewal-processes">
<h2>Renewal processes<a class="headerlink" href="#renewal-processes" title="Permalink to this headline">#</a></h2>
<p>Let’s return to the interval distribution above. As we saw, under a Poisson process the intervals are exponentially distributed. This isn’t a very realistic model for neural spike trains because neurons have refractory periods.</p>
<p><strong>Renewal processes</strong> are a more general class of models that explicitly model the interval distribution. Again, let <span class="math notranslate nohighlight">\(\{\delta_n\}_{n=1}^N\)</span> denote the intervals between the points <span class="math notranslate nohighlight">\(\{t_n\}_{n=1}^N\)</span>. In a renewal process,</p>
<div class="math notranslate nohighlight">
\[
\delta_n \overset{\text{iid}}{\sim} p(\delta),
\]</div>
<p>where <span class="math notranslate nohighlight">\(p(\delta)\)</span> is an interval distribution with support on <span class="math notranslate nohighlight">\(\mathbb{R}_+\)</span>. When <span class="math notranslate nohighlight">\(p(\delta)\)</span> is an exponential distribution we recover the Poisson process. To model neural spike trains, we could substitute a more general form like a gamma distribution or an <a class="reference external" href="https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution"><strong>inverse Gaussian distribution</strong></a>.</p>
<p>The inverse Gaussian distribution is appealing due to its connection to Brownian motion: it is the distribution of the <em>first passage time</em> at which a particle undergoing Brownian motion reaches a fixed level. A very simple <a class="reference external" href="https://en.wikipedia.org/wiki/Biological_neuron_model#Stochastic_models_of_membrane_voltage_and_spike_timing"><strong>stochastic integrate-and-fire model</strong></a> of a neuron treats the spike times as the first passage times when the membrane potential reaches a firing threshold. If the membrane potential is modeled as Brownian motion with positive drift, then the inter-spike intervals are inverse Gaussian distributed.</p>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>A defining property of a Poisson process is that the numbers of events in disjoint intervals are independent. Do renewal processes violate this assumption?</p>
</div>
</section>
<section id="conditional-intensity-functions">
<h2>Conditional intensity functions<a class="headerlink" href="#conditional-intensity-functions" title="Permalink to this headline">#</a></h2>
<p>So far we have treated the intensity as simply a function of time. Of course, the intensity could also be modeled as a function of external stimuli or covariates, <span class="math notranslate nohighlight">\(\mathbf{x}(t)\)</span>. For example, the stimulus could be incorporated into the basis function model described above.</p>
<p>To incorporate direct interactions between the points themselves, we need to generalize our model via a <strong>conditional intensity function</strong>,</p>
<div class="math notranslate nohighlight">
\[
\lambda(t \mid \mathcal{H}_t) 
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{H}_t\)</span> represents the <strong>history</strong> of points up to time <span class="math notranslate nohighlight">\(t\)</span>. In the language of stochastic processes, <span class="math notranslate nohighlight">\(\mathcal{H}_t\)</span> is a <a class="reference external" href="https://en.wikipedia.org/wiki/Filtration_(probability_theory)"><strong>filtration</strong></a>.</p>
<p>Given the history, the process behaves like a Poisson process. However, as soon as a new point occurs, the intensity function changes to account for it. Intuitively, this allows us to construct continuous time analogs of discrete time autoregressive models.</p>
</section>
<section id="hawkes-processes">
<h2>Hawkes processes<a class="headerlink" href="#hawkes-processes" title="Permalink to this headline">#</a></h2>
<p>Perhaps the canonical example of a point process with conditional dependencies is the Hawkes process <span id="id1">[<a class="reference internal" href="99_references.html#id76" title="Alan G Hawkes. Spectra of some self-exciting and mutually exciting point processes. Biometrika, 58(1):83–90, 1971.">Hawkes, 1971</a>]</span>. It is defined by its conditional intensity function,</p>
<div class="math notranslate nohighlight">
\[
\lambda(t \mid \mathcal{H}_t) = \lambda_0 + \sum_{t_n \in \mathcal{H}_t} w \, f(t - t_n),
\]</div>
<p>where <span class="math notranslate nohighlight">\(f : \mathbb{R}_+ \mapsto \mathbb{R}_+\)</span> is the <strong>impulse response function</strong> or, following the last chapter, the self-coupling filter. It specifies how past spikes affect the conditional intensity at time <span class="math notranslate nohighlight">\(t\)</span>. Without loss of generality, assume the impulse response is normalized so that <span class="math notranslate nohighlight">\(\int_0^\infty f(s) \, \mathrm{d}s = 1\)</span>. A common choice is is an exponential density with time-constant (inverse rate) <span class="math notranslate nohighlight">\(\tau\)</span>,</p>
<div class="math notranslate nohighlight">
\[
f(s) = \mathrm{Exp}(s; \tau^{-1}) = \tau^{-1} e^{-s / \tau}.
\]</div>
<p>This simple formulation of the model has two parameters, the baseline intensity <span class="math notranslate nohighlight">\(\lambda_0 \in \mathbb{R}_+\)</span> and the self-coupling weight <span class="math notranslate nohighlight">\(w \in \mathbb{R}_+\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(f\)</span> is a positive function, a Hawkes process is <strong>self-exciting</strong>: past spikes can only increase the future firing rate. Self-excitation can easily become unstable, depending on the self-coupling weight. If <span class="math notranslate nohighlight">\(w \geq 1\)</span>, the process becomes unstable because, intuitively, each spike induces more than one future spike in expectation. If <span class="math notranslate nohighlight">\(w &lt; 1\)</span>, the stationary firing rate is,</p>
<div class="math notranslate nohighlight">
\[
\lambda_{\infty} = \frac{\lambda_0}{1 - w}.
\]</div>
<p>When <span class="math notranslate nohighlight">\(w=0\)</span>, we recover the standard Poisson process with rate <span class="math notranslate nohighlight">\(\lambda_0\)</span>, but as <span class="math notranslate nohighlight">\(w\)</span> increases the self-excitation produces bursts of spikes and a larger stationary rate.</p>
</section>
<section id="multivariate-hawkes-processes">
<h2>Multivariate Hawkes processes<a class="headerlink" href="#multivariate-hawkes-processes" title="Permalink to this headline">#</a></h2>
<p>Now let’s generalize the Hawkes process to multiple interacting point processes. Let <span class="math notranslate nohighlight">\(M\)</span> denote the number of processes — e.g., the number of neurons in a multi-neuronal spike train recording — and let <span class="math notranslate nohighlight">\(\lambda_m(t \mid \mathcal{H}_t)\)</span> denote the conditional intensity function for the <span class="math notranslate nohighlight">\(m\)</span>-th process. Let <span class="math notranslate nohighlight">\(\mathcal{H}_{t,m}\)</span> denote the history of events on process <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(\mathcal{H}_t = \{\mathcal{H}_{t,m}\}_{m=1}^M\)</span> denote the combined history of all processes.</p>
<p>The multivariate Hawkes process has conditional intensities,</p>
<div class="math notranslate nohighlight">
\[
\lambda_m(t \mid \mathcal{H}_t) = \lambda_{0,m} + \sum_{m'=1}^M \sum_{t_n \in \mathcal{H}_{t,m'}} w_{m',m} \, f(t - t_n),
\]</div>
<p>The weight <span class="math notranslate nohighlight">\(w_{m',m} \geq 0\)</span> specifies the influence that spikes on process <span class="math notranslate nohighlight">\(m'\)</span> have on the future rate of process <span class="math notranslate nohighlight">\(m\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{W} \in \mathbb{R}_+^{M \times M}\)</span> denote the matrix of weights. In the multivariate case, stability is determined by the eigenvalues of the weight matrix. The multivariate process is stable if the <strong>spectral radius</strong> of <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> is less than one. Then, the stationary rates are,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\lambda}_\infty = (\mathbf{I} - \mathbf{W})^{-1} \boldsymbol{\lambda}_0
\]</div>
<div class="admonition-spectral-radius-and-the-perron-frobenius-theorem admonition">
<p class="admonition-title">Spectral radius and the Perron-Frobenius theorem</p>
<p>The spectral radius of a matrix is the maximum absolute value of its eigenvalues. Since <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> is a non-negative, real-valued matrix, the <a class="reference external" href="https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem"><strong>Perron-Frobenius theorem</strong></a> says that the maximum eigenvalue is real-valued and non-negative.</p>
</div>
</section>
<section id="maximum-likelihood-estimation-for-hawkes-processes">
<h2>Maximum likelihood estimation for Hawkes processes<a class="headerlink" href="#maximum-likelihood-estimation-for-hawkes-processes" title="Permalink to this headline">#</a></h2>
<p>The likelhood of a Hawkes process is the same as that of a Poisson process, but with the conditional intensity instead. Consider the univariate case (the multivariate Hawkes process follows the same form),</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
p(\{t_n\}_{n=1}^N; \theta) &amp;= 
\exp \bigg\{-\int_0^T \lambda(t \mid \mathcal{H}_t; \theta) \, \mathrm{d} t\bigg\} 
\prod_{n=1}^N \lambda(t_n \mid \mathcal{H}_{t_n}; \theta),
\end{aligned}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta = (\lambda_0, w)\)</span> are the two parameters.</p>
<p>For Hawkes processes, the integral can be simplified,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\int_0^T \lambda(t \mid \mathcal{H}_t; \theta) \, \mathrm{d} t
&amp;= \lambda_0 T + \sum_{n=1}^N w \int_{t_n}^T f(t - t_n) \, \mathrm{d}t.
\end{aligned}
\]</div>
<p>If <span class="math notranslate nohighlight">\(f\)</span> has bounded support or decays quickly to zero — e.g., when <span class="math notranslate nohighlight">\(f(s) = \mathrm{Exp}(s; \tau^{-1})\)</span> the density is close to zero after delays greater than a few time constants <span class="math notranslate nohighlight">\(\tau\)</span> — then we can approximate the integral as,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\int_0^T \lambda(t \mid \mathcal{H}_t; \theta) \, \mathrm{d} t
&amp;\approx \lambda_0 T + w N.
\end{aligned}
\]</div>
<p>Maximum likelihood estimation boils down to a convex optimization problem: minize the negative log likelihood, which is convex and easy to evaluate, subject to nonnegativity constraints. This problem is straightforward to solve with CVXPy, for example.</p>
</section>
<section id="poisson-superposition-and-thinning">
<h2>Poisson superposition and thinning<a class="headerlink" href="#poisson-superposition-and-thinning" title="Permalink to this headline">#</a></h2>
<p>Just like the sum of independent Poisson random variables, the <strong>superposition</strong> of two independent Poisson proesses is another Poisson process. If,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\{t_{0,n}\}_{n=1}^{N_0} &amp;\sim \mathrm{PP}(\lambda_0(t)) \\
\{t_{1,n}\}_{n=1}^{N_1} &amp;\sim \mathrm{PP}(\lambda_1(t)) 
\end{aligned}
\end{split}\]</div>
<p>are two independent Poisson processes, then there superposition is a Poisson process as well,</p>
<div class="math notranslate nohighlight">
\[
\{t_{0,n}\}_{n=1}^{N_0} \cup \{t_{1,n}\}_{n=1}^{N_1} \sim \mathrm{PP}(\lambda_0(t) + \lambda_1(t)).
\]</div>
<p>Likewise, if a Poisson process intensity is a sum of non-negative functions, we can use <strong>Poisson thinning</strong> to separate the points into indepedent Processes. Suppose</p>
<div class="math notranslate nohighlight">
\[
\{t_n\}_{n=1}^N \sim \mathrm{PP}(\lambda_0(t) + \lambda_1(t)).
\]</div>
<p>Introduce independent binary variables,</p>
<div class="math notranslate nohighlight">
\[
z_n \sim \mathrm{Bern}\left( \frac{\lambda_1(t_n)}{\lambda_0(t_n) + \lambda_1(t_n)} \right)
\]</div>
<p>to separate the events. Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\{t_n: z_n = 0\} &amp;\sim \mathrm{PP}(\lambda_0(t)) \\
\{t_n: z_n = 1\} &amp;\sim \mathrm{PP}(\lambda_1(t)) 
\end{aligned}
\end{split}\]</div>
</section>
<section id="hawkes-processes-as-cascades-of-poisson-processes">
<h2>Hawkes processes as cascades of Poisson processes<a class="headerlink" href="#hawkes-processes-as-cascades-of-poisson-processes" title="Permalink to this headline">#</a></h2>
<p>We can use Poisson superposition and thinning to reinterpret the Hawkes process as a cascade of Poisson processes. Note that the conditional intensity of a Hawkes process is the sum of non-negative intensity functions: a homogeneous background rate plus weighted impulse responses. An equivalent way to sample the Hawkes process is to sample points from independent Poisson processes for each term in the intensity function.</p>
<ol>
<li><p>First, sample points from the background process,</p>
<div class="math notranslate nohighlight">
\[\mathcal{T}_0 = \{t_{0}\}_{n=1}^{N_0} \sim \mathrm{PP}(\lambda_0)\]</div>
</li>
<li><p>Then, for each point <span class="math notranslate nohighlight">\(t_n \in \mathcal{T}_0\)</span>, sample new points from its a Poisson process with its impulse response,</p>
<div class="math notranslate nohighlight">
\[\mathcal{T}_n = \{t_{n'}\}_{n'=1}^{N_n} \sim \mathrm{PP}(w f(t - t_n))\]</div>
</li>
<li><p>Repeat this process recursively for each newly sampled point.</p></li>
<li><p>Return the union of all induced points <span class="math notranslate nohighlight">\(\cup \mathcal{T}_n\)</span>.</p></li>
</ol>
<p>From this perspective, we can view the Hawkes process as a <strong>cascade of Poisson processes</strong>. Each point can spawn a new generation of <strong>child</strong> points.</p>
<p>This perspective also sheds light on the weights. When the impulse response is normalized, we can think of the weight as the <em>expected number of children</em>. If the expected number of children is greater than 1, then the process above becomes unstable and may not halt.</p>
<p>Finally, we can use this perspective to formulate the Hawkes process as a latent variable model. Each point has a latent <strong>parent</strong> — either the background or a preceding poitn &amp;mdash. If we knew the parents, the Hawkes process would decompose into independent Poisson processes. We can leverage this formulation to do efficient Bayesian inference in Hawkes processes <span id="id2">[<a class="reference internal" href="99_references.html#id77" title="Scott Linderman and Ryan Adams. Discovering latent network structure in point process data. In International conference on machine learning, 1413–1421. PMLR, 2014.">Linderman and Adams, 2014</a>]</span>.</p>
</section>
<section id="nonlinear-hawkes-processes">
<h2>Nonlinear Hawkes processes<a class="headerlink" href="#nonlinear-hawkes-processes" title="Permalink to this headline">#</a></h2>
<p>A limitation of Hawkes processes is that they are <strong>purely excitatory</strong>. If the weights were negative, the intensity function could be negative as well, and that would be an invalid point process.</p>
<p>One way around this is via <strong>nonlinear Hawkes processes</strong>, where the intensity function is,</p>
<div class="math notranslate nohighlight">
\[
\lambda(t \mid \mathcal{H}_t) = g \left( w_0 + \sum_{t_n \in \mathcal{H}_t} w \, f(t - t_n) \right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(g: \mathbb{R} \mapsto \mathbb{R}_+\)</span> is a rectifying nonlinearity. This is essentially th continuous-time analog of the GLMs from the preceding chapter. Unfortunately, many of the nice properties of regular Hawkes processes are lost in this formulation:</p>
<ul class="simple">
<li><p>The integrated intensity does not have a simpled closed form solution.</p></li>
<li><p>The process can no longer be viewed as a superposition.</p></li>
</ul>
<p>Still, we can approximate the log likelihood with numerical quadrature methods, and in some cases we can use fancy augmentation schemes to perform Bayesian inference <span id="id3">[<a class="reference internal" href="99_references.html#id78" title="Feng Zhou, Zhidong Li, Xuhui Fan, Yang Wang, Arcot Sowmya, and Fang Chen. Scalable inference for nonparametric Hawkes process using Pó$ lya-gamma augmentation. arXiv preprint arXiv:1910.13052, 2019.">Zhou <em>et al.</em>, 2019</a>]</span>.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Point processes are stochastic processes that produce discrete sets of points.  They are fundamental tools for modeling neural spike trains, where the points are spike times.</p></li>
<li><p>The Poisson process is the fundamental point process from which more complex models are constructed.</p></li>
<li><p>There are many ways to sample a Poisson process. We showed two: the “two-stage” approach of sampling the number of points and then their times; and the “interval” approach of repeatedly sampling inter-spike intervals.</p></li>
<li><p>Both approaches led to the same expression for the Poisson process likelihood, which is a concave function of the intensity.</p></li>
<li><p>However, the same simplifying assumptions that make Poisson processes so mathematically elegant also limit their utility for modeling data with dependencies. Renewal processes and Hawkes processes extend the Poisson process, without completely sacrificing tractability.</p></li>
<li><p>Finally, we saw how Hawkes processes can be viewed as cascades of Poisson processes thanks to the Poisson superposition and thinning principles.</p></li>
</ul>
<p>To learn more about Poisson processes, the definitive reference is <span id="id4">Kingman [<a class="reference internal" href="99_references.html#id79" title="John Frank Charles Kingman. Poisson processes. Volume 3. Clarendon Press, 1992.">1992</a>]</span>.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="09_glm.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Generalized Linear Models</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="11_decoding.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Decoding Models</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Scott Linderman<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>