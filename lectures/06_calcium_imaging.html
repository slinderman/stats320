
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Demixing Calcium Imaging Data &#8212; Machine Learning Methods for Neural Data Analysis</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Markerless Pose Tracking" href="07_pose_tracking.html" />
    <link rel="prev" title="Spike Sorting by Deconvolution" href="05_deconv_spike_sorting.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Methods for Neural Data Analysis</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notes
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="02_probabilistic_modeling.html">
   Probabilistic Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_neurobio.html">
   Basic Neurobiology
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_simple_spike_sorting.html">
   Simple Spike Sorting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_deconv_spike_sorting.html">
   Spike Sorting by Deconvolution
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Demixing Calcium Imaging Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_pose_tracking.html">
   Markerless Pose Tracking
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="99_references.html">
   References
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Labs
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/00_pytorch_primer.html">
   Lab 0: PyTorch Primer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/01_spike_sorting.html">
   Lab 1: Spike Sorting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/02_calcium_imaging.html">
   Lab 2: Calcium Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/03_pose_tracking.html">
   Lab 3: Markerless pose tracking
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://colab.research.google.com/github/slinderman/stats320/blob/winter2023/lectures/06_calcium_imaging.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/lectures/06_calcium_imaging.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#constrained-non-negative-matrix-factorization-cnmf">
   Constrained Non-negative Matrix Factorization (CNMF)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recursive-formulation">
   Recursive formulation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prior-on-intensity-traces">
   Prior on intensity traces
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#completing-the-model">
   Completing the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-a-posteriori-map-estimation">
   Maximum
   <em>
    a posteriori
   </em>
   (MAP) estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-calcium-fluorescence-traces">
   Optimizing calcium fluorescence traces
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dual-formulation">
   Dual formulation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setting-the-threshold">
   Setting the threshold
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-the-background-fluorescence">
   Optimizing the background fluorescence
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#non-negative-spatial-footprints">
   Non-negative spatial footprints
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#alternative-approach">
   Alternative approach
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#better-background-models">
   Better background models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   Further reading
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Demixing Calcium Imaging Data</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#constrained-non-negative-matrix-factorization-cnmf">
   Constrained Non-negative Matrix Factorization (CNMF)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recursive-formulation">
   Recursive formulation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prior-on-intensity-traces">
   Prior on intensity traces
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#completing-the-model">
   Completing the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-a-posteriori-map-estimation">
   Maximum
   <em>
    a posteriori
   </em>
   (MAP) estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-calcium-fluorescence-traces">
   Optimizing calcium fluorescence traces
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dual-formulation">
   Dual formulation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setting-the-threshold">
   Setting the threshold
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-the-background-fluorescence">
   Optimizing the background fluorescence
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#non-negative-spatial-footprints">
   Non-negative spatial footprints
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#alternative-approach">
   Alternative approach
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#better-background-models">
   Better background models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   Further reading
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="demixing-calcium-imaging-data">
<h1>Demixing Calcium Imaging Data<a class="headerlink" href="#demixing-calcium-imaging-data" title="Permalink to this headline">#</a></h1>
<figure class="align-default" id="caiman">
<img alt="../_images/fig1.png" src="../_images/fig1.png" />
<figcaption>
<p><span class="caption-number">Fig. 7 </span><span class="caption-text">a: Pipeline for demixing and deconvolving raw calcium imaging movies.
b. An example frame from a 2-photon calcium imaging experiment with three
overlapping neurons.
c. Demixed calcium fluorescence traces for each of the neurons outlined in b.
Spikes above indicate the times of action potentials deconvolved from the red trace.
d. Schematic of the model. The (motion corrected) movie frames are modeled as a superposition
of fluorescence traces from each component (i.e. neuron) plus background (i.e. neuropil).
Each component consists of a spatial factor (i.e. “footprint”) and a temporal factor (i.e. “calcium trace”).
This figure was adapted from fig. 1 of <span id="id1">Giovannucci <em>et al.</em> [<a class="reference internal" href="99_references.html#id25" title="Andrea Giovannucci, Johannes Friedrich, Pat Gunn, Jérémie Kalfon, Brandon L Brown, Sue Ann Koay, Jiannis Taxidis, Farzaneh Najafi, Jeffrey L Gauthier, Pengcheng Zhou, Baljit S Khakh, David W Tank, Dmitri B Chklovskii, and Eftychios A Pnevmatikakis. CaImAn an open source tool for scalable calcium imaging data analysis. Elife, January 2019.">2019</a>]</span></span><a class="headerlink" href="#caiman" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Electrophysiological (<em>ephys</em>) recordings offer fine timescale measurements of neural spiking, but they have a few key limitations. Even with high density probes like Neuropixels, you only record the cells that happen to be near a recording site. Moreover, ephys methods don’t leverage the wide range of genetic tools available for some model organisms. For example, neuroscientists can control gene expression in specific cell types.</p>
<p>Optical physiology (<em>ophys</em>) techniques leverage these genetic tools to develop an alternative approach to measuring brain activity. The general idea is to alter the DNA of an organism to encode a fluorescent indicator of neuronal activity. These indicators may be targeted to a subset of cells to make them fluoresce when they spike. How can you make fluorescent cells? One way is to alter their DNA so that the neurons of interest produce green fluorescent protein (GFP), which emits bright green light when exposed to blue light; e.g. from a laser. A variety of other fluorescent proteins exist that emit light in other colors, allowing for “multi-spectral” methods that measure activity in multiple color channels.
But how can you make cells fluoresce <em>only</em> when they spike?
You need to activate the fluorescent protein only when an action potential occurs, and fortunately, there are many ways of accomplishing that feat <span id="id2">[<a class="reference internal" href="99_references.html#id24" title="Michael Z Lin and Mark J Schnitzer. Genetically encoded indicators of neuronal activity. Nat. Neurosci., 19(9):1142–1153, August 2016.">Lin and Schnitzer, 2016</a>]</span>.</p>
<p>The most mature method of optically measuring neuronal activity is with <strong>genetically encoded calcium indicators (GECIs)</strong>. GECIs bind to calcium ions, which are let into the cell by voltage-gated calcium channels when neurons spike. Once bound, GECIs become fluorescent; for example, GCaMP absorbs blue light and emits green light when bound to calcium. After an action potential, the calcium eventually unbinds and is pumped out of the cell, causing the fluorescence to decay exponentially after spikes. The result is a calcium fluorescence trace with fast peaks followed by exponential decays, as shown in <a class="reference internal" href="#caiman"><span class="std std-numref">Fig. 7</span></a>c.</p>
<p>Our goal is to extract the fluorescence traces associated with each neuron in the field of view.
The problem is challenging for many reasons. First, the raw video is often corrupted by non-rigid motion artifacts. Second, the neurons may be overlapping, so fluorescence in a single pixel may be attributed to one of many neurons. Third, there is substantial “background” fluorescence coming from out-of-focus tissue (i.e. neuropil). Fourth, the fluorescence is corrupted by noise.  On the other hand, we have substantial prior knowledge about fluorescence traces that can guide our inferences.  We’ll make use of this prior knowledge in the following model, which is based on the constrained non-negative matrix factorization (CNMF) approach of <span id="id3">Vogelstein <em>et al.</em> [<a class="reference internal" href="99_references.html#id28" title="Joshua T Vogelstein, Adam M Packer, Timothy A Machado, Tanya Sippy, Baktash Babadi, Rafael Yuste, and Liam Paninski. Fast nonnegative deconvolution for spike train inference from population calcium imaging. Journal of Neurophysiology, 104(6):3691–3704, 2010.">2010</a>]</span> and <span id="id4">Pnevmatikakis <em>et al.</em> [<a class="reference internal" href="99_references.html#id27" title="Eftychios A Pnevmatikakis, Daniel Soudry, Yuanjun Gao, Timothy A Machado, Josh Merel, David Pfau, Thomas Reardon, Yu Mu, Clay Lacefield, Weijian Yang, and others. Simultaneous denoising, deconvolution, and demixing of calcium imaging data. Neuron, 2016.">2016</a>]</span> and implemented in the CaImAn package <span id="id5">[<a class="reference internal" href="99_references.html#id25" title="Andrea Giovannucci, Johannes Friedrich, Pat Gunn, Jérémie Kalfon, Brandon L Brown, Sue Ann Koay, Jiannis Taxidis, Farzaneh Najafi, Jeffrey L Gauthier, Pengcheng Zhou, Baljit S Khakh, David W Tank, Dmitri B Chklovskii, and Eftychios A Pnevmatikakis. CaImAn an open source tool for scalable calcium imaging data analysis. Elife, January 2019.">Giovannucci <em>et al.</em>, 2019</a>]</span>. It is very similar to the methods implemented in Suite2P <span id="id6">[<a class="reference internal" href="99_references.html#id26" title="Marius Pachitariu, Carsen Stringer, Mario Dipoppa, Sylvia Schröder, L Federico Rossi, Henry Dalgleish, Matteo Carandini, and Kenneth D Harris. Suite2p: Beyond 10,000 neurons with standard two-photon microscopy. BioRxiv, pages 061507, 2017.">Pachitariu <em>et al.</em>, 2017</a>]</span>.</p>
<section id="constrained-non-negative-matrix-factorization-cnmf">
<h2>Constrained Non-negative Matrix Factorization (CNMF)<a class="headerlink" href="#constrained-non-negative-matrix-factorization-cnmf" title="Permalink to this headline">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{N \times T}\)</span> denote a calcium imaging video with <span class="math notranslate nohighlight">\(N\)</span> pixels (assume each frame is flattened into a vector) and <span class="math notranslate nohighlight">\(T\)</span> time frames (typically sampled at around 30 Hz). The entries of this matrix, <span class="math notranslate nohighlight">\(x_{n,t}\)</span>, denote the fluorescence intensity measured in pixel <span class="math notranslate nohighlight">\(n\)</span> at time frame <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>Like in the preceding chapter, we assume fluorescence is the sum of intensities from <span class="math notranslate nohighlight">\(K\)</span> neurons, each with its own <strong>spatial footprint</strong> <span class="math notranslate nohighlight">\(\mathbf{u}_k \in \mathbb{R}^N\)</span> and nonnegative <strong>amplitudes</strong> <span class="math notranslate nohighlight">\(\mathbf{a}_k \in \mathbb{R}_+^T\)</span>. Unlike the previous chapter, however, here we will explicitly model <strong>background activity</strong> with its own spatial footprint <span class="math notranslate nohighlight">\(\mathbf{u}_0\)</span> and time-varying intensity <span class="math notranslate nohighlight">\(\mathbf{c}_0 \in \mathbb{R}^T\)</span>.</p>
<p>The main thing that distinguishes our model for calcium imaging data from our model for spike sorting is that the temporal profile are highly constrained. When the neuron fires an action potential, the calcium concentration (and hence the fluorescence intensity) spikes. As calcium ions unbind from the indicator, the fluorescence decays exponentially. We can model the temporal profile as an exponential function,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\mathbf{v} &amp;= (1, e^{-1/\tau}, e^{-2/\tau}, \ldots )
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\tau \in \mathbb{R}_+\)</span> is the time-constant of intensity decay. The time constant is primarily driven by the calcium indicator (e.g. GCaMP6f vs GCaMP6s), which is usually the same for all neurons. It is easy to generalize the model to allow different time constants for each neuron, if desired.</p>
<p>Since <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> is fixed, calcium imaging models are often formulated in terms of the calcium traces <span class="math notranslate nohighlight">\(\mathbf{c}_k = (c_{k,1}, \ldots, c_{k,T})\)</span>, which are the convolution of the spike amplitudes and the temporal profile,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
c_{k,t} = [\mathbf{a}_k \circledast \mathbf{v}]_t = \sum_{d=0}^\infty a_{k-d} e^{-d/\tau}.
\end{align*}
\]</div>
<p>Putting it all together, the likelihood is,</p>
<div class="math notranslate nohighlight">
\[
p(\mathbf{X} \mid \mathbf{U}, \mathbf{C}) 
= \prod_{n=1}^N \prod_{t=1}^T \mathcal{N}\left(x_{n,t} \,\bigg|\, \sum_{k=1}^K u_{k,n} c_{k,t} + u_{0,n} c_{0,t}, \, \sigma^2 \right).
\]</div>
</section>
<section id="recursive-formulation">
<h2>Recursive formulation<a class="headerlink" href="#recursive-formulation" title="Permalink to this headline">#</a></h2>
<p>The reason we call this <strong>constrained</strong> NMF is that the calcium traces are modeled as nonnegative vectors with jumps followed by exponential decays. Thanks to the <strong>memoryless</strong> property of the exponential function, we can write the model recursively,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    c_{k,t} 
    &amp;= \sum_{d=0}^\infty a_{k,t-d} e^{-d/\tau} \\
    &amp;= a_{k,t} + \sum_{d=1}^\infty a_{k,t-d} e^{-d/\tau} \\
    &amp;= a_{k,t} + \sum_{d=0}^\infty a_{k,t-1-d} e^{-(d+1)/\tau} \\
    &amp;= a_{k,t} + e^{-1/\tau} c_{k,t-1}.
\end{align*}
\end{split}\]</div>
<p>From one frame to the next, the calcium intensity decays by a fraction <span class="math notranslate nohighlight">\(e^{-1/\tau}\)</span>, and then a spike of amplitude <span class="math notranslate nohighlight">\(a_{k,t}\)</span> is added.</p>
<div class="admonition-back-of-the-envelope-calculations-with-exponentials admonition">
<p class="admonition-title">Back-of-the-envelope calculations with exponentials</p>
<p>Since <span class="math notranslate nohighlight">\(\tau &gt; 0\)</span>, <span class="math notranslate nohighlight">\(e^{-1/\tau} &lt; 1\)</span>, which captures the fact that calcium traces <em>decay</em> in the absence of spikes. To get a quick estimate of the decay, note that for large <span class="math notranslate nohighlight">\(\tau\)</span>, the exponential function is well-approximated by its first ordre Taylor expansion around zero,</p>
<div class="math notranslate nohighlight">
\[
e^{-1/\tau} \approx 1-\frac{1}{\tau}
\]</div>
<p>For a 300ms time constant and 30Hz frame rate, the time constant (in units of frames) is <span class="math notranslate nohighlight">\(\tau\)</span> = 0.3 sec <span class="math notranslate nohighlight">\(\times\)</span> 30 frames/sec = 9 frames. Thus, between consecutive frames the calcium fluorescence decreases by a fraction of <span class="math notranslate nohighlight">\(1-\frac{1}{9} \approx 0.88\)</span> if there’s no spike.</p>
</div>
<!-- Finally, we model the background fluorescence as independent Gaussian noise~$c_{0,t} \sim \cN(0, \varsigma_0^2)$. -->
</section>
<section id="prior-on-intensity-traces">
<h2>Prior on intensity traces<a class="headerlink" href="#prior-on-intensity-traces" title="Permalink to this headline">#</a></h2>
<p>How can we translate our prior on amplitudes into a distribution on calcium traces? Note that <span class="math notranslate nohighlight">\(\mathbf{c}_k\)</span> is an <strong>invertible</strong> function of the spike amplitudes, <span class="math notranslate nohighlight">\(\mathbf{c}_k = f(\mathbf{a}_k)\)</span>. Using the change of measure formula, we can rewrite <span class="math notranslate nohighlight">\(p(\mathbf{c}_k)\)</span> in terms of the prior on amplitudes,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
p(\mathbf{c}_k) &amp;= \left| \tfrac{\mathrm{d} f^{-1}(\mathbf{c}_k)}{\mathrm{d} \mathbf{c}_k} \right| \, p(f^{-1}(\mathbf{c}_k))
\end{align*}
\]</div>
<p>In this model, the inverse function is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    \mathbf{a}_k &amp;= f^{-1}(\mathbf{c}_k) = \mathbf{G} \mathbf{c}_k, 
    \quad \text{where} \quad
    \mathbf{G} = 
    \begin{bmatrix}
    1             &amp;               &amp;        &amp;        \\
    -e^{-1/\tau} &amp; 1             &amp;        &amp;        \\
    0             &amp; -e^{-1/\tau} &amp; 1      &amp;        \\
                  &amp; 0             &amp; \ddots &amp; \ddots \\
    \end{bmatrix},
\end{align*}
\end{split}\]</div>
<p>and its Jacobian is</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\frac{\mathrm{d} f^{-1}(\mathbf{c}_k)}{\mathrm{d} \mathbf{c}_k} &amp;= \mathbf{G}.
\end{align*}
\]</div>
<p>Since <span class="math notranslate nohighlight">\(G\)</span> is lower triangular, its determinant is simply the product of its diagonal, which in this case is just 1. Intuitively, the determinant of the Jacobian tells us how the volume of a set in <span class="math notranslate nohighlight">\(\mathbb{R}^T\)</span> changes under the linear transformation <span class="math notranslate nohighlight">\(G\)</span>. The fact that the determinant of the Jacobian is one tells us that this transformation is volume preserving.</p>
<p>Now we can derive the probability of <span class="math notranslate nohighlight">\(\mathbf{c}_k\)</span> using the change of measure formula,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    p(\mathbf{c}_k) 
    &amp;= \left| \tfrac{\mathrm{d} f^{-1}(\mathbf{c}_k)}{\mathrm{d} \mathbf{c}_k} \right| \,  p(f^{-1}(\mathbf{c}_k)) \\
    &amp;= \left|\mathbf{G} \right| \prod_{t=1}^T \mathrm{Exp}([f^{-1}(\mathbf{c}_k)]_t; \lambda) \\
    &amp;= \prod_{t=1}^T \mathrm{Exp}(c_{k,t} - e^{-1/\tau} c_{k,t-1}; \lambda).
\end{align*}
\end{split}\]</div>
<p>This is just a fancy way of arriving at a very simple observation: the probability of the calcium trace is simply the probability of the corresponding spike amplitudes under the exponential prior.</p>
</section>
<section id="completing-the-model">
<h2>Completing the model<a class="headerlink" href="#completing-the-model" title="Permalink to this headline">#</a></h2>
<p>For now, let’s assume the spatial footprints are unit-norm, just like in the preceding chapter,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{u}_k \sim \mathrm{Unif}(\mathbb{S}_{N-1})
\]</div>
<p>Later, we will discuss a more realistic model in whcih the footprints are constrained to be non-negative.</p>
<p>The other new feature of our model for calcium imaging data is the background model. We will assume the background footprint <span class="math notranslate nohighlight">\(\mathbf{u}_0\)</span> is uniform over <span class="math notranslate nohighlight">\(\mathbb{S}_{N-1}\)</span> as well, but allow the background trace to be positive or negative via a Gaussian prior,</p>
<div class="math notranslate nohighlight">
\[
c_{0,t} \sim \mathcal{N}(0, \sigma_0^2).
\]</div>
</section>
<section id="maximum-a-posteriori-map-estimation">
<h2>Maximum <em>a posteriori</em> (MAP) estimation<a class="headerlink" href="#maximum-a-posteriori-map-estimation" title="Permalink to this headline">#</a></h2>
<p>We’re getting very good at MAP estimation now! The algorithm here will look very similar to that of the last chapter. We will cycle through one neuron at a time, updating its footprint and calcium trace while holding the others fixed. Again, the updates will depend on the residual,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{R} = \mathbf{X} - \sum_{j \neq k} \mathbf{u}_j \mathbf{c}_j^\top - \mathbf{u}_0 \mathbf{c}_0^\top
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{R} \in \mathbb{R}^{N \times T}\)</span> has columns <span class="math notranslate nohighlight">\(\mathbf{r}_{t}\)</span> and entries <span class="math notranslate nohighlight">\(r_{n,t}\)</span>.</p>
<p>We will add a twist though: we’ll reformulate the update of calcium traces as a convex optimization problem.</p>
</section>
<section id="optimizing-calcium-fluorescence-traces">
<h2>Optimizing calcium fluorescence traces<a class="headerlink" href="#optimizing-calcium-fluorescence-traces" title="Permalink to this headline">#</a></h2>
<p>As a function of the calcium fluorescence for neuron <span class="math notranslate nohighlight">\(k\)</span>, the log likelihood is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    \log p(\mathbf{X} \mid \mathbf{U}, \mathbf{C})
    &amp;= \sum_{t=1}^T \log \mathcal{N}(\mathbf{r}_{t} \mid \mathbf{u}_{k} c_{k,t}, \, \sigma^2 \mathbf{I}) \\
    &amp;= -\frac{1}{2\sigma^2} \sum_{t=1}^T (\mathbf{r}_{t} - \mathbf{u}_{k} c_{k,t})^\top (\mathbf{r}_{t} -  \mathbf{u}_{k} c_{k,t}) \\
    &amp;= -\frac{1}{2\sigma^2} \sum_{t=1}^T (\mathbf{r}_{t}^\top \mathbf{r}_t - \mathbf{r}_t^\top \mathbf{u}_{k} c_{k,t} + c_{k,t}^2 \mathbf{u}_k^\top \mathbf{u}_{k}) \\
    &amp;= -\frac{1}{2\sigma^2} \sum_{t=1}^T (- \mathbf{r}_t^\top \mathbf{u}_{k} c_{k,t} + c_{k,t}^2) + c' \\
\end{align*}
\end{split}\]</div>
<p>where we used the fact that <span class="math notranslate nohighlight">\(\mathbf{u}_k^\top \mathbf{u}_k = 1\)</span>. We can rewrite this in vector notation,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
    \log p(\mathbf{X} \mid \mathbf{U}, \mathbf{C})
    &amp;= -\frac{1}{2\sigma^2} ( \mathbf{c}_k^\top \mathbf{c}_k - (\mathbf{R}^\top \mathbf{u}_k)^\top \mathbf{c}_k) + c'.
\end{align*}
\]</div>
<p>Completing the square, the log likelihood (up to an additive constant) is,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
    \log p(\mathbf{X} \mid \mathbf{U}, \mathbf{C})
    &amp;= -\frac{1}{2 \sigma^2} \|\mathbf{c}_k - \mathbf{R}^\top \mathbf{u}_k\|_2^2 
\end{align*}
\]</div>
<p>In other words, it is the squared Euclidean norm of the difference between the calcium trace and the projected residual <span class="math notranslate nohighlight">\(\mathbf{R}^\top \mathbf{u}_k\)</span>.</p>
<p>Now consider the log prior as a function of <span class="math notranslate nohighlight">\(\mathbf{c}_k\)</span>. It can be expressed as,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\log p(\mathbf{C}) 
&amp;= \sum_{t=1}^T \log \mathrm{Exp}(c_{k,t} - e^{-1/\tau} c_{k,t-1}; \lambda) \\
&amp;= -\lambda \sum_{t=1}^T (c_{k,t} - e^{-1/\tau} c_{k,t-1}) \\
&amp;= -\lambda \|\mathbf{G} \mathbf{c}_k\|_1
\end{align*}
\end{split}\]</div>
<p>Note that everything we have done in this section is exactly analogous to the way we derived the objective for the spike amplitudes in the previous chapter. However, since we’re working with the calcium fluorescence traces — i.e., the convolution of the spike amplitudes and the temporal responses — instead of the spike amplitudes directly, we end up with a slightly different expression. We could, of course, convert back to spike amplitudes and use the same heuristic approach as before. Namely, we could use <code class="docutils literal notranslate"><span class="pre">find_peaks</span></code> on the cross-correlation of the residual and the exponential decay filter. However, that heuristic required us to set a minimum peak height, <span class="math notranslate nohighlight">\(\lambda \sigma^2\)</span>.  This formulation suggests an alternative approach.</p>
</section>
<section id="dual-formulation">
<h2>Dual formulation<a class="headerlink" href="#dual-formulation" title="Permalink to this headline">#</a></h2>
<p>Maximizing the objective above subject to the non-negativity constrain <span class="math notranslate nohighlight">\(\mathbf{c}_k \geq 0\)</span> is equivalent to solving the following optimization problem,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
    \mathbf{c}_k^\star = \text{arg} \, \min_{\mathbf{c}_k} \; \|\mathbf{G} \mathbf{c}_k\|_1 
    \quad \text{subject to } \quad 
    \|\mathbf{c}_k - \mathbf{R}^\top \mathbf{u}_k\|_2 &amp;\leq \theta, \; \mathbf{G} \mathbf{c}_k \geq 0,
\end{align*}
\]</div>
<p>for some threshold <span class="math notranslate nohighlight">\(\theta\)</span> that depends on <span class="math notranslate nohighlight">\(\lambda\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>This is a <strong>convex optimization</strong> problem. It has a <strong>linear objective</strong> (<span class="math notranslate nohighlight">\(\|\mathbf{G} \mathbf{c}_k\|_1\)</span>) with <strong>linear constraints</strong> (<span class="math notranslate nohighlight">\(\mathbf{G} \mathbf{c}_k \geq 0\)</span>) and <strong>quadratic constraints</strong> (<span class="math notranslate nohighlight">\(\|\mathbf{c}_k - \mathbf{R}^\top \mathbf{u}_k\|_2 \leq \theta\)</span>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since <span class="math notranslate nohighlight">\(\mathbf{G}\)</span> is a sparse banded matrix, this optimization problem is tractable even for large <span class="math notranslate nohighlight">\(T\)</span> using standard optimization libraries like <a class="reference external" href="https://www.cvxpy.org/"><strong>CVXpy</strong></a>.</p>
</div>
</section>
<section id="setting-the-threshold">
<h2>Setting the threshold<a class="headerlink" href="#setting-the-threshold" title="Permalink to this headline">#</a></h2>
<!-- % To see the equivalence, square both sides of the constraint (this does not its domain) and form the Lagrangian of \cref{eq:ca_dual}. The Lagrangian is $\eta (\|\mathbf{c}_k - \mathbf{R}^\top \mathbf{u}_k\|_2^2 - \theta^2)$, where $\eta$ is a Lagrange multiplier, plus $\|G \mathbf{c}_k\|_1$. Minimizing the Lagrangian is equivalent to maximizing its negation, which is proportional to  -->
<p>Whereas setting <span class="math notranslate nohighlight">\(\lambda\)</span> was a bit tricky, here we can make a pretty good guess as to what the threshold <span class="math notranslate nohighlight">\(\theta\)</span> should be. Under our model, the vector of differences <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}_k = (\epsilon_{k,1}, \ldots, \epsilon_{k,T})\)</span> with <span class="math notranslate nohighlight">\(\epsilon_{k,t} = c_{k,t} - \mathbf{r}_t^\top \mathbf{u}_k\)</span> is distributed as,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\epsilon}_k \sim \mathcal{N}(0, \sigma^2 \mathbf{I}),
\]</div>
<p>or equivalently,</p>
<div class="math notranslate nohighlight">
\[
\sigma^{-1} \boldsymbol{\epsilon}_k \sim \mathcal{N}(0, \mathbf{I}).
\]</div>
<p>The <span class="math notranslate nohighlight">\(\ell_2\)</span> norm of a vector of <span class="math notranslate nohighlight">\(T\)</span> iid standard normal random variables is itself a random variable, and it follows a <strong>chi distribution</strong> with <span class="math notranslate nohighlight">\(T\)</span> degrees of freedom,</p>
<div class="math notranslate nohighlight">
\[
\|\sigma^{-1} \boldsymbol{\epsilon}_k\|_2 \sim \chi_T.
\]</div>
<p>For large <span class="math notranslate nohighlight">\(T\)</span>, the chi distribution concentrates around its mode, <span class="math notranslate nohighlight">\(\sqrt{T-1}\)</span>, so we expect</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{c}_k - \mathbf{R}^\top \mathbf{u}_k\|_2 \approx \sigma \sqrt{T-1}.
\]</div>
<p>A conservative guess for the threshold is <span class="math notranslate nohighlight">\(\theta = (1 + \delta) \sigma \sqrt{T-1}\)</span> with, e.g., <span class="math notranslate nohighlight">\(\delta = \tfrac{1}{4}\)</span>.</p>
<p>But how do we estimate the noise variance, <span class="math notranslate nohighlight">\(\sigma^2\)</span>? One option is to fit it as part of our MAP estimation. Another is to note that Gaussian noise has a <strong>flat power spectrum</strong>. If we high pass filter the data to throw away spikes and other background fluctuations, the result should be white noise, and its variance should be a good estimate of <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<div class="admonition-the-chi-and-chi-2-distributions admonition">
<p class="admonition-title">The <span class="math notranslate nohighlight">\(\chi\)</span> and <span class="math notranslate nohighlight">\(\chi^2\)</span> distributions</p>
<p>The <span class="math notranslate nohighlight">\(\ell_2\)</span> norm of a vector of iid standard normal random variables <span class="math notranslate nohighlight">\(\mathbf{z} = (z_1, \ldots, z_\nu)\)</span> follows a <a class="reference external" href="https://en.wikipedia.org/wiki/Chi_distribution"><strong>chi (<span class="math notranslate nohighlight">\(\chi\)</span>) distribution</strong></a> with <span class="math notranslate nohighlight">\(\nu\)</span> degrees of freedom. The sum of squares, <span class="math notranslate nohighlight">\(\mathbf{z}^\top \mathbf{z} = z_1^2 + \ldots z_\nu^2\)</span>, follows its sibling, the <a class="reference external" href="https://en.wikipedia.org/wiki/Chi-squared_distribution"><strong>chi (<span class="math notranslate nohighlight">\(\chi^2\)</span>) squared distribution</strong></a>. The <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution is widely used throughout statistics, with applications to hypothesis testing, the analysis of variance, and as a prior distribution for variances in Bayesian models.</p>
<p>Its density is,</p>
<div class="math notranslate nohighlight">
\[
\chi^2(x; \nu) = \frac{1}{2^{\frac{\nu}{2}} \Gamma(\frac{\nu}{2})} x^{\frac{\nu}{2} - 1} e^{-\frac{x}{2}}.
\]</div>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Show that the <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution is also a special case of the gamma distribution.</p>
</div>
</div>
</section>
<section id="optimizing-the-background-fluorescence">
<h2>Optimizing the background fluorescence<a class="headerlink" href="#optimizing-the-background-fluorescence" title="Permalink to this headline">#</a></h2>
<p>As a function of the background fluorescence trace <span class="math notranslate nohighlight">\(\mathbf{c}_0\)</span>, the log joint probability is,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
    \log p(\mathbf{X}, \mathbf{U}, \mathbf{C})
    &amp;=
    -\frac{1}{2\sigma^{2}} \|\mathbf{c}_{0} - \mathbf{R}^\top \mathbf{u}_0\|_2^2 - \frac{1}{2\sigma_0^2} \mathbf{c}_0^\top \mathbf{c}_0 + c',
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{R} = \mathbf{x} - \sum_{k=1}^K \mathbf{u}_k \mathbf{c}_{k}^\top\)</span> denotes the residual.</p>
<p>Both terms in the log probability are quadratic in <span class="math notranslate nohighlight">\(\mathbf{c}_0\)</span>. Completing the square yields,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
    \log p(\mathbf{X}, \mathbf{U}, \mathbf{C}) 
    = -\frac{1}{2\varsigma_{0}^2} \|\mathbf{c}_0 - \mathbf{c}_0^\star\|_2^2  + c'
\end{align*}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    \varsigma_{0}^2 
    &amp;= \left( \frac{1}{\sigma_0^2} + \frac{1}{\sigma^2} \right)^{-1} = \frac{\sigma^2 \sigma_0^2}{\sigma^2 + \sigma_0^2} \\
    \mathbf{c}_0^\star &amp;= \varsigma_{0}^2 \left( \frac{\mathbf{R}^\top \mathbf{u}_0}{\sigma^2} \right)
    = \frac{\sigma_0^2}{\sigma^2 + \sigma_0^2} (\mathbf{R}^\top \mathbf{u}_0).
\end{align*}
\end{split}\]</div>
<p>The maximum is attained at <span class="math notranslate nohighlight">\(\mathbf{c}_0^\star\)</span>, the residual projected onto the background footprint but shrunk by a factor of <span class="math notranslate nohighlight">\(\sigma_0^2 / (\sigma^2 + \sigma_0^2) &lt; 1\)</span>. As the prior variance <span class="math notranslate nohighlight">\(\sigma_0^2\)</span> goes to infinity, the shrinkage factor goes to one and we get simply the projected residual. When the prior variance is small relative to the noise, the estimate is shrunk more toward the prior mean of zero.</p>
<!-- ## Optimizing the spatial footprints

We can optimize the footprints in at least two ways. One is to solve for $\mathbf{u}_k$ holding $\{\mathbf{u}_j\}_{j \neq k}$ fixed, just as in the previous chapter. In that case, we end up with the same solution as before,

$$
\begin{align*}
    \mathbf{u}_k^\star 
    &\propto \mathbf{R} \mathbf{c}_k
\end{align*}
Again, we used the fact that~$u_n^\top u_n = 1$. The optimum, subject to $u_n \in \cS_1$ (i.e.  --></section>
<section id="non-negative-spatial-footprints">
<h2>Non-negative spatial footprints<a class="headerlink" href="#non-negative-spatial-footprints" title="Permalink to this headline">#</a></h2>
<p>Unlike the spikes observed in voltage recordings, calcium fluorescence should only be positive.\footnote{Strictly speaking, the fluorescence could go below zero if calcium concentrations dip below baseline at some points in time. We’ll disregard these minor effects.} Our model so far allows the spatial footprints <span class="math notranslate nohighlight">\(u_n\)</span> to be positive or negative, as long as they are unit norm. A simple fix is to drop the normalization constraint and instead impose a non-negativity constraint in the form of an exponential prior on the footprints,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
    p(\{\mathbf{u}_k\}_{k=1}^K) = \prod_{k=1}^K \prod_{n=1}^N \mathrm{Exp}(u_{k,n} \mid \lambda_u),
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_u\)</span> is the rate (i.e. inverse scale) hyperparameter. Likewise, we place an exponential prior on the background footprint,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
    p(\mathbf{u}_0) &amp;= \prod_{n=1}^N \mathrm{Exp}(u_{0,n} \mid \lambda_{u_0}).
\end{align*}
\]</div>
<p>Since the background footprint will typically not be sparse, we set <span class="math notranslate nohighlight">\(\lambda_{u_0} \ll \lambda_u\)</span>, which increases the prior mean and corresponds to weaker regularization of the background footprint.</p>
<p>Under this model, the log joint probability as a function of <span class="math notranslate nohighlight">\(u_{k,n}\)</span> is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    \log p(\mathbf{X}, \mathbf{U}, \mathbf{C}) 
    &amp;= -\frac{1}{2 \sigma^2}  \sum_{t=1}^T \|r_{n,t} - u_{k,n} c_{k,t}\|_2^2 - \lambda_{u} u_{k,n} + c' \\
    &amp;= -\frac{1}{2 \sigma^2}  (\mathbf{r}_n - u_{k,n} \mathbf{c}_{k})^\top (\mathbf{r}_n - u_{k,n} \mathbf{c}_{k}) - \lambda_{u} u_{k,n} + c' \\
    &amp;= -\frac{\alpha}{2} u_{k,n}^2 + \beta u_{k,n} + c''
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{r}_n\)</span> is the <span class="math notranslate nohighlight">\(n\)</span>-th row of <span class="math notranslate nohighlight">\(\mathbf{R}\)</span> and,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\alpha &amp;= \frac{\mathbf{c}_k^\top \mathbf{c}_k}{\sigma^2} \\
\beta &amp;= \frac{\mathbf{r}_n^\top \mathbf{c}_k}{\sigma^2} - \lambda_u
\end{align*}
\end{split}\]</div>
 <!-- &= -\frac{1}{2\sigma^2} \sum_{t=1}^T \left[-2 u_{k,n} (c_{n,t} r_{k,n,t}) + c_{n,t}^2 u_{k,n}^2 \right] - \lambda_{u} u_{k,n} + c' \\
    &= -\frac{1}{2 \varsigma_{k,n}^2} (u_{k,n} - \mu_{k,n})^2,
where

\begin{align*}
    \varsigma_{n,p}^2 &\triangleq \frac{\sigma^2}{c_n^\top c_n} \\
    \mu_{n,p} &\triangleq \varsigma_{n,p}^2 \left( \frac{c_n^\top r_{n,p}}{\sigma^2} - \lambda_u \right) = \frac{c_n^\top r_{n,p} - \sigma^2 \lambda_u}{c_n^\top c_n}.
\end{align*} -->
<p>The maximum, subject to <span class="math notranslate nohighlight">\(u_{k,n} \geq 0\)</span>, is at <span class="math notranslate nohighlight">\(\max\{0, \beta/\alpha\}\)</span>, or,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
    u_{k,n} = \max \left\{0, \frac{\mathbf{c}_k^\top \mathbf{r}_{n} - \sigma^2 \lambda_u}{\mathbf{c}_k^\top \mathbf{c}_k} \right\}
\end{align*}
\]</div>
<p>The same steps lead to</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
    u_{0,n} = \max \left\{0, \frac{\mathbf{c}_0^\top \mathbf{r}_{n} - \sigma^2 \lambda_{u_0}}{\mathbf{c}_0^\top \mathbf{c}_0} \right\} 
\end{align*}
\]</div>
<p>for the background footprint.</p>
<p>Again, the hyperparameters <span class="math notranslate nohighlight">\(\lambda_u\)</span> and <span class="math notranslate nohighlight">\(\lambda_{u_0}\)</span> are a bit of a nuisance. It’s not obvious how to set the <em>a priori</em>. We can expect the spatial footprints to be spatially localized, but that leads to dependencies between pixels that are challenging to optimize. One simpler heuristic is to tune <span class="math notranslate nohighlight">\(\lambda_u\)</span> so that the number of nonzero pixels in the footprint is about the size of an average neuron.</p>
</section>
<section id="alternative-approach">
<h2>Alternative approach<a class="headerlink" href="#alternative-approach" title="Permalink to this headline">#</a></h2>
<p>Another way is to solve for the all weights associated with a single pixel.  This approach is attractive because the problem factors over pixels.</p>
<p>As a function of the weights associated with pixel <span class="math notranslate nohighlight">\(n\)</span>, the log probability is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    &amp;\log p(\mathbf{X}, \mathbf{U}, \mathbf{C}) \\
    &amp;\qquad -\frac{1}{2\sigma^2} \sum_{t=1}^T \left(x_{n,t} - \sum_{k=1}^K u_{k,n} c_{k,t} - u_{0,n} c_{0,t} \right)^2 - \lambda_u \sum_{k=1}^K u_{k,n} - \lambda_{u_0} u_{0,n}.
\end{align*}
\end{split}\]</div>
<p>To put this in a more natural form, let <span class="math notranslate nohighlight">\(\tilde{u}_{0,n} = \tfrac{\lambda_{u_0}}{\lambda_u} u_{0,n}\)</span> and <span class="math notranslate nohighlight">\(\tilde{\mathbf{u}}_n = (\tilde{u}_{0,n}, u_{1,n}, \ldots, u_{K,n})^\top\)</span>. Likewise, let <span class="math notranslate nohighlight">\(\tilde{c}_{0,t} = \tfrac{\lambda_{u}}{\lambda_{u_0}} c_{0,t}\)</span>, <span class="math notranslate nohighlight">\(\tilde{\mathbf{c}}_0 = (\tilde{c}_{0,1}, \ldots, \tilde{c}_{0,T})^\top\)</span>, and,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    \tilde{C} &amp;= 
    \begin{bmatrix}
    | &amp; | &amp;  &amp; | \\
    \tilde{\mathbf{c}}_0 &amp; \mathbf{c}_1 &amp; \cdots &amp; \mathbf{c}_K \\
    | &amp; | &amp;  &amp; | 
    \end{bmatrix}.
\end{align*}
\end{split}\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{align*}
    \log p(\mathbf{X}, \mathbf{U}, \mathbf{C}) 
    &amp;= -\frac{1}{2\sigma^2} \|\mathbf{x}_{n} - \tilde{\mathbf{C}} \tilde{\mathbf{u}}_{n} \|_2^2 -\lambda_u \| \tilde{\mathbf{u}}_{n}\|_1,
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x}_n\)</span> is the <span class="math notranslate nohighlight">\(n\)</span>-th row of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and</p>
<p>This is a convex optimization problem just like one above for the fluorescence traces. Like above, we can solve its dual problem with a constraint,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
    \tilde{\mathbf{u}}_n^\star = \text{arg} \, \min_{\tilde{\mathbf{u}}_n} \; \|\tilde{\mathbf{u}}_n\|_1 
    \quad \text{s.t.} \quad 
    \|\mathbf{x}_{n} - \tilde{\mathbf{C}} \tilde{\mathbf{u}}_{n}\|_2 &amp;\leq (1 + \delta) \sigma \sqrt{T-1}, \; \tilde{\mathbf{u}}_n \geq 0,
\end{align*}
\]</div>
<p>A simpler way is to note that this is a standard <span class="math notranslate nohighlight">\(\ell_1\)</span>-regularized linear regression problem, and it can be solved with a variety of methods including the LARS algorithm, even when the weights are constrained to be non-negative.</p>
</section>
<section id="better-background-models">
<h2>Better background models<a class="headerlink" href="#better-background-models" title="Permalink to this headline">#</a></h2>
<p>The fluorescence measured at the camera does not arise solely from cells in imaging plane, but also from photons emitted by out-of-focus tissue. This is particularly evident in widefield microscopy and one-photon microendoscopy where the entire sample is illuminated by a light source and the measured fluorescence comes from neurons throughout the sample. Other microscopy methods like confocal and two-photon microscopy reduce out-of-focus light, but the measured fluorescence always has some contribution from tissue outside the imaging plane. The result is a blurry, time-varying background signal that adds to the light collected from the in-focus cells.</p>
<p>So far, we’ve modeled the “background” fluorescence with a rank one model, <span class="math notranslate nohighlight">\(\mathbf{u}_0 \mathbf{c}_0^\top\)</span>. This isn’t a bad approximation, but it doesn’t account for background intensities that fluctuate separately in different parts of the video frame. There have been a few suggestions for how to address this limitation. One is to simply increase the rank of the background model, however, without constraints there’s nothing to prevent the background model from also capturing neural signals of interest.</p>
<p>A natural constraint on the background is that it is spatially smoooth, at least on the length-scales of single neurons. There are many ways to incorporate such a constraint; a simple and straightforward way is with basis functions. Let, <span class="math notranslate nohighlight">\(B \in \mathbb{N}\)</span> denote the number of background basis functions, and let <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_b \in \mathbb{R}_+^{N}\)</span> denote a fixed, non-negative basis function over the pixels. For example, we could set,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    \boldsymbol{\phi}_{b,n} 
    &amp;\propto \mathcal{N} \left( \begin{bmatrix} x_n \\ y_n \end{bmatrix} \, \bigg| \, \begin{bmatrix} \mu_{x,b} \\ \mu_{y,b} \end{bmatrix}, \eta^2 I \right),
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\((x_n, y_n)\)</span> denotes the location of pixel <span class="math notranslate nohighlight">\(n\)</span> in xy-coordinates of the image, <span class="math notranslate nohighlight">\((\mu_{x,b}, \mu_{y,b})\)</span> denote the center of the <span class="math notranslate nohighlight">\(b\)</span>-th basis function in xy-coordinates of the image, and <span class="math notranslate nohighlight">\(\eta\)</span> denotes the width of the basis function in pixels. These are called <strong>radial basis functions</strong>. We typically set their width to be 5-6 times larger than the size of a neuron and tile them so they cover the image. As above, we normalize the basis functions so that <span class="math notranslate nohighlight">\(\|\boldsymbol{\phi}_b\|_2 = 1\)</span>.</p>
<p>Then we model the fluorescence as a sum of neuron factors plus the a weighted combination of the basis functions, which capture background variation,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
    \mathbf{x}_t &amp;\sim \mathcal{N} \left( \sum_{k=1}^K \mathbf{u}_k c_{k,t} + \sum_{b=1}^B \boldsymbol{\phi}_{b} f_{b,t}, \, \sigma^2 \mathbf{I} \right),
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(f_{b,t}\)</span> is background fluorescence for basis function <span class="math notranslate nohighlight">\(b\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">#</a></h2>
<p>Calcium imaging, and optical physiology more generally, offers a powerful and complementary toolkit for measuring neural activity in genetically defined cell types.</p>
<p>Methods for demixing and deconvolving calcium fluorescence traces are similar to those for spike sorting — again, convolutional matrix factorization was the key idea.</p>
<p>With calcium traces, though, we can place more stringent constraints on the form of the temporal response. It’s well-approximated as an exponential decay. This allows us to frame the problem in a new way that leverages convex optimization techniques.</p>
</section>
<section id="further-reading">
<h2>Further reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><span id="id7">Lin and Schnitzer [<a class="reference internal" href="99_references.html#id24" title="Michael Z Lin and Mark J Schnitzer. Genetically encoded indicators of neuronal activity. Nat. Neurosci., 19(9):1142–1153, August 2016.">2016</a>]</span> is a nice review of genetically encoded fluorescent indicators of neural activity.</p></li>
<li><p>The model developed in this chapter is motivated by CNMF and CaImAn <span id="id8">[<a class="reference internal" href="99_references.html#id25" title="Andrea Giovannucci, Johannes Friedrich, Pat Gunn, Jérémie Kalfon, Brandon L Brown, Sue Ann Koay, Jiannis Taxidis, Farzaneh Najafi, Jeffrey L Gauthier, Pengcheng Zhou, Baljit S Khakh, David W Tank, Dmitri B Chklovskii, and Eftychios A Pnevmatikakis. CaImAn an open source tool for scalable calcium imaging data analysis. Elife, January 2019.">Giovannucci <em>et al.</em>, 2019</a>, <a class="reference internal" href="99_references.html#id27" title="Eftychios A Pnevmatikakis, Daniel Soudry, Yuanjun Gao, Timothy A Machado, Josh Merel, David Pfau, Thomas Reardon, Yu Mu, Clay Lacefield, Weijian Yang, and others. Simultaneous denoising, deconvolution, and demixing of calcium imaging data. Neuron, 2016.">Pnevmatikakis <em>et al.</em>, 2016</a>]</span>. It is similar to Suite2P <span id="id9">[<a class="reference internal" href="99_references.html#id26" title="Marius Pachitariu, Carsen Stringer, Mario Dipoppa, Sylvia Schröder, L Federico Rossi, Henry Dalgleish, Matteo Carandini, and Kenneth D Harris. Suite2p: Beyond 10,000 neurons with standard two-photon microscopy. BioRxiv, pages 061507, 2017.">Pachitariu <em>et al.</em>, 2017</a>]</span>.</p></li>
<li><p>Better background models for one-photon imaging with microendoscope data were developed in CNMFe <span id="id10">[<a class="reference internal" href="99_references.html#id29" title="Pengcheng Zhou, Shanna L Resendez, Jose Rodriguez-Romaguera, Jessica C Jimenez, Shay Q Neufeld, Andrea Giovannucci, Johannes Friedrich, Eftychios A Pnevmatikakis, Garret D Stuber, Rene Hen, and others. Efficient and accurate extraction of in vivo calcium signals from microendoscopic video data. elife, 7:e28728, 2018.">Zhou <em>et al.</em>, 2018</a>]</span>.</p></li>
<li><p>The methods developed here still suffer from the shrinkage problem, due to the <span class="math notranslate nohighlight">\(\ell_1\)</span> penalty on spike amplitudes. <span id="id11">Jewell and Witten [<a class="reference internal" href="99_references.html#id30" title="Sean Jewell and Daniela Witten. Exact spike train inference via $\ell _\0\$ optimization. Annals of Applied Statistics, 12(4):2457–2482, December 2018.">2018</a>]</span> developed an elegant and exact solution to calcium deconvolution with <span class="math notranslate nohighlight">\(\ell_0\)</span> regularization by framing it as a changepoint detection problem.</p></li>
</ul>
<!-- 
\section{Exact Deconvolution with a Point Process Prior}
So far, we've assumed an exponential prior distribution on the spike amplitudes~$a$ (i.e. the jumps in the calcium fluorescence $c$). The exponential prior equates to an $\ell_1$-regularization in the log joint probability. As we have seen, the $\ell_1$ regularization leads to sparsity in the posterior mode. However, it also can shrink our estimates of the spike amplitudes and lead to spurious spikes. One reason for these mistakes is that the exponential prior isn't really a good generative model for spikes. A sample from an exponential distribution is positive with probability one, whereas we want almost amplitudes to be zero!  

Consider the following model instead, which is a Bayesian reformulation of the $\ell_0$-regularization proposed by \citet{Jewell2018-tw}. Let $\cX = \{(t_k, c_{t_k})\}_{k=1}^K$ denote a set of ordered spike times $1 \leq t_1 < \ldots < t_k \leq T$ and corresponding fluorescence amplitudes $c_{t_k}$ at those times. Moreover, assume the times are integers (i.e. frame indices). Given the fluorescence at those spike times, we assume the fluorescence at intermediate frames decays exponentially,
\begin{align}
    c(\cX) &= \big( c_1(\cX), \ldots, c_T(\cX) \big)^\top, & 
    c_{t}(\cX) &= \begin{cases}
    c_{t_k} & \text{if } t \in \{t_k\} \\
    e^{-1/\tau} c_{t-1}(\cX) & \text{ o.w.}.
    \end{cases}
\end{align}
To start the recursion, assume $c_0(\cX) = 0$.
We place a simple prior on $\cX$. Assume,
\begin{align}
    p(\cX) &= p(t_1) \times \prod_{k=2}^{|\cX|} \Big[ p(t_k \mid t_{k-1}) \Big] \times p(t_{K+1} > T \mid t_K) \times \prod_{k=1}^{|\cX|} p(c_{t_k})
\end{align}
where $p(t_k \mid t_{k-1})$ is a conditional distribution on the interval between spikes and $p(t_{K+1} > T \mid t_K)$ is a probability that the "next spike" occurs after time $T$. Start with a simple geometric interval model,
\begin{align}
    p(t_1) &= \mathrm{Geom}(t_1 \mid \nu)
    % = \nu (1-\nu)^{t_1 - 1} 
    \\
    p(t_k \mid t_{k-1}) &= \mathrm{Geom}(t_k - t_{k-1} \mid \nu) 
    % = \nu (1-\nu)^{t_k - t_{k-1} -1} 
    \\
    p(t_{K+1} > T \mid t_{K}) &= \sum_{t=T+1}^\infty \mathrm{Geom}(t - t_{K} \mid \nu) 
    % = (1 - \nu)^{T - t_K}
    \\
    p(c_{t_k}) &= \mathrm{Unif}(c_{t_k} \mid [0, c_{\mathsf{max}})).
    % = \lambda e^{-\lambda c_{t_k}}.
\end{align}
where $\mathrm{Geom}(t \mid \nu) = \nu (1-\nu)^{t-1}$ for $t=1,2,\ldots$ is the pmf of the geometric distribution and its cdf is $\Pr(t \leq T \mid \nu) = 1 - (1-\nu)^{T}$. Assume $c_{\mathsf{max}}$ is sufficiently large that it upper bounds the fluorescence. When we substitute in the geometric pmf and cdf, the prior probability of $\cX$ simplifies to,
\begin{align}
    p(\cX) 
    % &= \nu^K (1-\nu)^{T-K}  c_{\mathsf{max}}^{-K},
    &\propto \left(\frac{\nu}{c_{\mathsf{max}} (1-\nu)} \right)^{|\cX|},
\end{align}
which only depends on the number of spikes. This is called a marked point process. It's a distribution on sets of random cardinality $K$, where each entry in the set contains a time and a "mark." Here, the mark is the fluorescence intensity. It's still a weird prior for calcium traces---it allows for both postive and \emph{negative} jumps---but it does produce traces that jump only at a small number of times.

% As before, there is a one-to-one, volume preserving mapping between $c$ and $\cX$. That means we can evaluate the probability of a fluorescence trace by finding the jumps and plugging them into $p(\cX)$.

We can write the likelihood of a trace $\mu$ (e.g. the projected residual for a neuron) in terms of $\cX$ instead. We have up to an additive constant,
\begin{align}
    \log p(\mu \mid \cX) &= -\frac{1}{2\sigma^2} \sum_{t=1}^T (\mu_t - c_t(\cX))^2\\
    &= -\frac{1}{2 \sigma^2} \sum_{k=0}^{|\cX|} \sum_{t=t_k}^{t_{k+1}-1} (\mu_t - c_{t_k} e^{-(t-t_k)/\tau})^2 
\end{align}
(For notational convenience, let $t_0=1$ and $t_{K+1} = T +1$.) Combining the log likelihood and the log prior and simplifying,
\begin{align}
    \cL(\cX) \triangleq \log p(\mu, \cX) &= -\frac{1}{2\sigma^2} \sum_{k=0}^{|\cX|} \sum_{t=t_k}^{t_{k+1}-1} (\mu_t - c_{t_k} e^{-(t-t_k)/\tau})^2 \  + \eta |\cX| + \mathrm{const}.
\end{align}
Intuitively, the log joint consists of the log likelihood of each segment of the data between one spike and the next, a small regularization on the amplitude of the fluorescence, and a penalty $\eta = \log \tfrac{\nu}{c_{\mathsf{max}}(1 - \nu)}$ on the number of spikes.  

We want to maximize this objective with respect to $\cX$ to obtain,
\begin{align}
    \label{eq:spike_optimization}
    \cX^\star(\mu) &\triangleq \argmax_{\cX} \; \cL(\cX).
\end{align}
We've written the optimum as a function of $\mu$ to emphasize that it depends on the full sequence of observations. This reminder will help later on.

This problem seems hard because $\cX$ is a set of unknown cardinality and each entry is a tuple of a time and a mark. To make headway, suppose we place the last spike at time index $t$. Let $\mu_{[1,t)}$ denote the target up to but not including time $t$, and let $\cX_{[1,t)}$ denote the set of spikes and marks that occur before time $t$. We can decompose the log joint into two terms: the likelihood of the the data up to but not including time $t$, and the likelihood of the remainder. The latter term is only a function of the last spike's mark,
\begin{align}
\cL(\cX_{[1,t)}, t, c_{t}) &= \log p\big(\mu, \cX_{[1,t)} \cup (t, c_{t}) \big) \\
&= \underbrace{\log p(\mu_{[1,t)}, \cX_{[1,t)})}_{\cL_{[1,t)}(\cX_{[1,t)})} \underbrace{-\frac{1}{2\sigma^2} \sum_{t'=t}^T (\mu_{t'} - c_{t} e^{-(t' - t)/\tau})^2}_{\cL_{[t,T]}(t, c_{t})} + \eta
\end{align}
The preceding spikes $\cX_{[1,t)}$ only appear in the first term, so maximizing with respect to previous spikes amounts to finding,
\begin{align}
    \cX^\star(\mu_{[1,t)}) &= \argmax_{\cX_{[1,t)}} \; \cL_{[1,t)}(\cX_{[1,t)}). % \log p(\mu_{[1,t)}, \cX_{[1,t)}).
\end{align}
\emph{This is the same optimization problem as \cref{eq:spike_optimization} but on a subset of the data!} Namely, $\cL(\cX) \equiv \cL_{[1,T+1)}(\cX_{[1,T+1)})$.


Similarly, the last spike's mark~$c_{t}$ only appears in this second term. Moreover, this is a quadratic function of $c_{t}$ with an analytical maximum (again assuming $c_{\mathsf{max}}$ sufficiently large),
\begin{align}
    c^\star(t) &= \argmax \;\cL_{[t,T]}(t, c_{t}) = \max\left\{0, \tfrac{h(t)}{J(t)}\right\} & 
    h(t) &= \frac{1}{\sigma^2} \sum_{t'=t}^T \mu_{t'} e^{-(t' - t)/\tau} \\
    & & 
    J(t) &= \frac{1}{\sigma^2} \sum_{t'=t}^T e^{-2(t' - t)/\tau}.
\end{align}
It's important to note that optimal value for $c_{t}$ is a function of $t$. For each assignment of the last spike time, we would have a different optimal fluorescence. We need to special case $c^\star(0)$ though; if the most recent spike is at time zero (i.e. there are no spikes!) we set $c^\star(0)=0$ so that the fluorescence trace is deterministically zero. 

All that is left is to plug in these optima to get an objective in terms of $t$ alone,
\begin{align}
    \cL_{T}(t) &= \cL_{[1,t)}(\cX^\star(\mu_{[1,t)})) + \cL_{[t,T]}(t, c^\star(t)) + \eta.
\end{align}
We subscript the objective by $T$ to remind ourselves that its domain runs from $t=0,\ldots, T$. We will search over this domain, the objective for each possible assignment of the last spike time. Then we set the most recent spike time to the time with the highest score,
\begin{align}
    t_T^* &= \argmax_{t \in \{0,\ldots,T\}} \cL_{T}(t)
\end{align}
and return
\begin{align}
    \cX^\star = \cX^\star_{[1,t_T^\star)} \cup (t_T^\star, c^\star(t_T^\star))
\end{align}
If the most recent time is zero, that means there are no spikes in the entire dataset and calcium fluorescence is zero for all time frames. 

Have we really solved the problem though? Only if we can find the optimal spikes for any leading subset of the data. That is, only if we can efficiently find the best most recent spike time before $t$ for all times $t=0,\ldots, T$. Fortunately, all the problems are nested, so we can solve them one at a time, starting from the beginning of the time series.  

% The base case starts with $\cX^\star(\mu_{<1}) = {(0,0)}$; i.e. we place a fake "spike" at time zero with amplitude zero. Since there are no observations before time 1, and $\cL(\cX^\star(\mu_{<1})) = 0$. 
The base case starts with $\cX^\star(\mu_{[1,1)}) = \varnothing$ and $\cL(\cX^\star(\mu_{[1,1)})) = 0$. 
In the first iteration, there are only two possibilities. Either $t=0$, indicating that there are no spikes in the entire time series (the most recent spike happened at time 0). In this case, we take the the entire fluorescence trace to be zero. Alternative, $t=1$, indicating that there was a spike in the first time step.
\begin{align}
    \cL(\cX_{<1}, 0, c_0=0) = \cL(\cX_{<1} + \cL_2(0, c_0=0) + \eta
\end{align}


Note that each subproblem involves solving for $c^*(t)$, which in turn depends on some \emph{sufficient statistics} $h(t)$ and $J(t)$. These statistics can be updated very efficiently as we progress forward in time. We simply add one more term to each sum. Moreover, while each subproblem in theory involves a search over all $t-1$ possible assignments of the most recent spike time, it turns out that many of these possible assignments can be pruned to make this algorithm closer to $O(T)$ running time in practice. See \citet{Jewell2018-tw} for more details.
 -->
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="05_deconv_spike_sorting.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Spike Sorting by Deconvolution</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="07_pose_tracking.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Markerless Pose Tracking</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Scott Linderman<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>