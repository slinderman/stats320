
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Markerless Pose Tracking &#8212; Machine Learning Methods for Neural Data Analysis</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Summary Statistics" href="08_summary_stats.html" />
    <link rel="prev" title="Demixing Calcium Imaging Data" href="06_calcium_imaging.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Methods for Neural Data Analysis</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Labs
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/00_pytorch_primer.html">
   Lab 0: PyTorch Primer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/01_spike_sorting.html">
   Lab 1: Spike Sorting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/02_calcium_imaging.html">
   Lab 2: Calcium Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/03_pose_tracking.html">
   Lab 3: Markerless Pose Tracking
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/04_glms.html">
   Lab 4: Generalized Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/05_decoding.html">
   Lab 5: Bayesian Decoding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../labs/06_arhmm.html">
   Lab 6: Autoregressive HMMs
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="02_probabilistic_modeling.html">
   Probabilistic Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_neurobio.html">
   Basic Neurobiology
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unit I: Signal Extraction
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="04_simple_spike_sorting.html">
   Simple Spike Sorting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_deconv_spike_sorting.html">
   Spike Sorting by Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_calcium_imaging.html">
   Demixing Calcium Imaging Data
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Markerless Pose Tracking
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unit II: Encoding &amp; Decoding
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="08_summary_stats.html">
   Summary Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_glm.html">
   Generalized Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_poisson_processes.html">
   Poisson Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_decoding.html">
   Decoding Models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unit III: Unsupervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="12_mixtures_em.html">
   Mixture Models and the EM Algorithm
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="99_references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://colab.research.google.com/github/slinderman/stats320/blob/winter2023/lectures/07_pose_tracking.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/lectures/07_pose_tracking.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supervised-learning">
   Supervised learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistic-regression">
   Logistic regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization">
   Optimization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#setup">
     Setup
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-gradient-with-respect-to-the-weights">
     The gradient with respect to the weights
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-negative-log-likelihood-is-convex">
     The negative log likelihood is convex
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     Gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#newton-s-method">
     Newton’s method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#iteratively-reweighted-least-squares-irls">
     Iteratively reweighted least squares (IRLS)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computational-complexity">
     Computational complexity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scaling-up">
     Scaling up
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pose-tracking-with-convolutional-neural-networks-cnns">
   Pose tracking with convolutional neural networks (CNNs)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolutional-neural-networks">
     Convolutional neural networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#working-with-minimal-labeled-data">
     Working with minimal labeled data.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transfer-learning">
     Transfer learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#structured-prediction">
   Structured prediction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-triangulation">
     3D Triangulation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#further-reading">
     Further reading
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Markerless Pose Tracking</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supervised-learning">
   Supervised learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistic-regression">
   Logistic regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization">
   Optimization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#setup">
     Setup
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-gradient-with-respect-to-the-weights">
     The gradient with respect to the weights
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-negative-log-likelihood-is-convex">
     The negative log likelihood is convex
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     Gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#newton-s-method">
     Newton’s method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#iteratively-reweighted-least-squares-irls">
     Iteratively reweighted least squares (IRLS)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computational-complexity">
     Computational complexity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scaling-up">
     Scaling up
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pose-tracking-with-convolutional-neural-networks-cnns">
   Pose tracking with convolutional neural networks (CNNs)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolutional-neural-networks">
     Convolutional neural networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#working-with-minimal-labeled-data">
     Working with minimal labeled data.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transfer-learning">
     Transfer learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#structured-prediction">
   Structured prediction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-triangulation">
     3D Triangulation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#further-reading">
     Further reading
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="markerless-pose-tracking">
<h1>Markerless Pose Tracking<a class="headerlink" href="#markerless-pose-tracking" title="Permalink to this headline">#</a></h1>
<p>We’ve talked a lot about extracting neural signals from ephys and ophys measurements, but what comes next? One of the fundamental goals of systems neuroscience is to understand the relationship between neural activity and behavioral outputs. To that end, we need to quantify  and model not only neural activity but animals’ behavior as well.</p>
<p>The study of natural behavior is called <strong>ethology</strong> <span id="id1">[<a class="reference internal" href="99_references.html#id31" title="N Tinbergen. On aims and methods of ethology. Z. Tierpsychol., 20(4):410–433, 1963.">Tinbergen, 1963</a>]</span>. With advances in deep learning, the field is undergoing a computational revival <span id="id2">[<a class="reference internal" href="99_references.html#id33" title="David J Anderson and Pietro Perona. Toward a science of computational ethology. Neuron, 84(1):18–31, October 2014.">Anderson and Perona, 2014</a>]</span>. The once tedious process of labeling video data to track body posture and annotate behaviors has been largely automated by machine learning methods, which are well-suited to the task <span id="id3">[<a class="reference internal" href="99_references.html#id38" title="Praneet C Bala, Benjamin R Eisenreich, Seng Bum Michael Yoo, Benjamin Y Hayden, Hyun Soo Park, and Jan Zimmermann. Automated markerless pose estimation in freely moving macaques with OpenMonkeyStudio. Nat. Commun., 11(1):4560, September 2020.">Bala <em>et al.</em>, 2020</a>, <a class="reference internal" href="99_references.html#id40" title="Kristin Branson, Alice A Robie, John Bender, Pietro Perona, and Michael H Dickinson. High-throughput ethomics in large groups of drosophila. Nat. Methods, 6(6):451–457, June 2009.">Branson <em>et al.</em>, 2009</a>, <a class="reference internal" href="99_references.html#id36" title="Jacob M Graving, Daniel Chae, Hemal Naik, Liang Li, Benjamin Koger, Blair R Costelloe, and Iain D Couzin. DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning. Elife, October 2019.">Graving <em>et al.</em>, 2019</a>, <a class="reference internal" href="99_references.html#id39" title="Ana S Machado, Dana M Darmohray, João Fayad, Hugo G Marques, and Megan R Carey. A quantitative framework for whole-body coordination reveals specific deficits in freely walking ataxic mice. Elife, October 2015.">Machado <em>et al.</em>, 2015</a>, <a class="reference internal" href="99_references.html#id34" title="Alexander Mathis, Pranav Mamidanna, Kevin M Cury, Taiga Abe, Venkatesh N Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. DeepLabCut: markerless pose estimation of user-defined body parts with deep learning. Nat. Neurosci., 21(9):1281–1289, August 2018.">Mathis <em>et al.</em>, 2018</a>, <a class="reference internal" href="99_references.html#id35" title="Talmo D Pereira, Nathaniel Tabris, Arie Matsliah, David M Turner, Junyu Li, Shruthi Ravindranath, Eleni S Papadoyannis, Edna Normand, David S Deutsch, Z Yan Wang, and others. SLEAP: A deep learning system for multi-animal pose tracking. Nature methods, 19(4):486–495, 2022.">Pereira <em>et al.</em>, 2022</a>, <a class="reference internal" href="99_references.html#id37" title="Anqi Wu, Estefany Kelly Buchanan, Matthew Whiteway, Michael Schartner, Guido Meijer, Jean-Paul Noel, Erica Rodriguez, Claire Everett, Amy Norovich, Evan Schaffer, and Others. Deep graph pose: a semi-supervised deep graphical model for improved animal pose tracking. Adv. Neural Inf. Process. Syst., 2020.">Wu <em>et al.</em>, 2020</a>]</span>. At the intersection of computational neuroscience and computational ethology is the (predictably named) emerging field of <strong>computational neuroethology</strong> <span id="id4">[<a class="reference internal" href="99_references.html#id32" title="Sandeep Robert Datta, David J Anderson, Kristin Branson, Pietro Perona, and Andrew Leifer. Computational neuroethology: a call to action. Neuron, 104(1):11–24, October 2019.">Datta <em>et al.</em>, 2019</a>]</span>.</p>
<p>This chapter develops the basic methods underlying most markerless pose tracking systems.  There are a few key ideas. First, we can cast pose tracking — i.e. the task of labeling keypoints of interest in video frames, like an animals paws, snout, and tail, and tracking them over time — as a <strong>supervised learning</strong> problem. Given a few labeled frames, we can train a classifier to predict the keypoint locations in new frames.  Second, such classifiers require surprisingly few training examples, particularly when they are given features from networks that have been pre-trained on similar tasks. For example, deep neural networks trained for image classification on very large datasets like ImageNet may not immediately solve pose tracking problems, but the features they’ve learned for classifying cats and dogs may still be useful for tracking paws and snouts.  Using a neural network trained on one task to jump-start a model for a similar task is called <strong>transfer learning</strong>.</p>
<p>We’ll start with a simple logistic regression model for pose tracking and show how it can be implemented with a convolutional neural network (CNN). Then we’ll show how the same ideas can be generalized to pre-trained CNNs for image classification, like very deep residual networks <span id="id5">[<a class="reference internal" href="99_references.html#id41" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 770–778. openaccess.thecvf.com, 2016.">He <em>et al.</em>, 2016</a>]</span>.</p>
<section id="supervised-learning">
<h2>Supervised learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">#</a></h2>
<p>In supervised learning problems, our data consists of a set of tuples <span class="math notranslate nohighlight">\(\{(\mathbf{x}_n, y_n)\}_{n=1}^N\)</span> where <span class="math notranslate nohighlight">\(\mathbf{x}_n\)</span> are the <strong>inputs</strong> and <span class="math notranslate nohighlight">\(y_n\)</span> are the <strong>outputs</strong>. Contrast this with the spike sorting and calcium deconvolution problems, which we framed as unsupervised matrix factorization problems.</p>
<p>A simple way to frame the pose tracking problem as a supervised learning problem is to chop each image frame into <strong>patches</strong> and then assign each patch a binary <strong>label</strong> to indicate whether or not the patch contains a specific key point (e.g. “left paw”). Then we can train a binary <strong>classifier</strong> to predict the labels given the image patches. Of course, we usually want to track multiple key points at once, and we could simply train separate classifiers for each one. Ideally, the trained classifiers will <strong>generalize</strong> to new image patches from new video frames, giving us predictions about which patches contain which keypoints. Then, in post-processing, we can determine the most likely configuration of key points in future frames, using the classifiers’ predictions.</p>
<figure class="align-default" id="paw-track">
<img alt="../_images/paw_classifier.png" src="../_images/paw_classifier.png" />
<figcaption>
<p><span class="caption-number">Fig. 8 </span><span class="caption-text">A simple way to do pose tracking is to carve each video frame into small patches and label them as positive or negative examples of a key point. Then train a classifier to predict the label for new patches.
This figure was adapted from fig. 1B of <span id="id6">Machado <em>et al.</em> [<a class="reference internal" href="99_references.html#id39" title="Ana S Machado, Dana M Darmohray, João Fayad, Hugo G Marques, and Megan R Carey. A quantitative framework for whole-body coordination reveals specific deficits in freely walking ataxic mice. Elife, October 2015.">2015</a>]</span></span><a class="headerlink" href="#paw-track" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="logistic-regression">
<h2>Logistic regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{x}_n \in \mathbb{R}^P\)</span> be the <span class="math notranslate nohighlight">\(n\)</span>-th image patch, flattened into a vector of <span class="math notranslate nohighlight">\(P\)</span> pixels. Let <span class="math notranslate nohighlight">\(y_n \in \{0,1\}\)</span> be a binary label specifying whether the key point of interest is present in that frame.</p>
<p>In <strong>logistic regression</strong>, we model the conditional distribution of the label given the image as,</p>
<div class="math notranslate nohighlight">
\[
p(y_n \mid \mathbf{x}_n) = \mathrm{Bern}\big( \sigma(\mathbf{w}^\top \mathbf{x}_n) \big)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{w} \in \mathbb{R}^P\)</span> are the <strong>weights</strong> for each pixel, and <span class="math notranslate nohighlight">\(\sigma: \mathbb{R} \mapsto (0, 1)\)</span> is the <strong>logistic function</strong></p>
<div class="admonition-the-bernoulli-distribution admonition">
<p class="admonition-title">The Bernoulli distribution</p>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Bernoulli_distribution"><strong>Bernoulli distribution</strong></a> is a distribution over binary variables <span class="math notranslate nohighlight">\(y \in \{0,1\}\)</span> with probability <span class="math notranslate nohighlight">\(p \in [0,1]\)</span>. Its pmf can be written as,</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Bern}(y; p) = p^{y} \, (1-p)^{(1-y)}
\]</div>
</div>
<div class="admonition-the-logistic-sigmoid-function admonition">
<p class="admonition-title">The logistic (sigmoid) function</p>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Sigmoid_function"><strong>logistic (aka sigmoid) function</strong></a> is a map from the reals to the interval <span class="math notranslate nohighlight">\((0,1)\)</span>. It is defined as,</p>
<div class="math notranslate nohighlight">
\[
\sigma(a) = \frac{1}{1 + e^{-a}} = \frac{e^a}{1 + e^a}
\]</div>
<p>It asymptotes at <span class="math notranslate nohighlight">\(\lim_{a \to -\infty} \sigma(a) = 0\)</span> and <span class="math notranslate nohighlight">\(\lim_{a \to \infty} \sigma(a) = 1\)</span>.  It is plotted below.</p>
<p>Interestingly, the logistic function is symmetric in that,</p>
<div class="math notranslate nohighlight">
\[
1 - \sigma(a) = \frac{1}{1+e^a} = \sigma(-a).
\]</div>
<p>Its derivative is,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\sigma'(a) = \frac{e^a}{(1+e^a)^2} = \left( \frac{e^a}{1+e^a}\right) \left( \frac{1}{1+e^a}\right) = \sigma(a) \sigma(-a).
\end{align*}
\]</div>
<p>The derivative is positive (i.e., the logistic function is monotonically increasing) and attains its maximum at <span class="math notranslate nohighlight">\(\sigma'(0) = \tfrac{1}{4}\)</span>.</p>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">aa</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">aa</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">aa</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s2">&quot;True&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$a$&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\sigma(a)$&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/07_pose_tracking_5_0.png" src="../_images/07_pose_tracking_5_0.png" />
</div>
</div>
</section>
<section id="optimization">
<h2>Optimization<a class="headerlink" href="#optimization" title="Permalink to this headline">#</a></h2>
<p>Our goal is to estimate the weights <span class="math notranslate nohighlight">\(\mathbf{w}_{\mathsf{MLE}}\)</span> that maximize the log likelihood of the training data, or equivalently minimize the negative log likelihood. Unlike most of the problems we’ve encountered thus far, we won’t have a closed form solution for the weights, even if we try to do coordinate ascent. Instead we’ll have to turn to more general optimization strategies like gradient descent and Newton’s method. This is a good opportunity to introduce a few of these tools and some of the key concepts.</p>
<section id="setup">
<h3>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{N \times P}\)</span> denote the matrix of inputs (with rows <span class="math notranslate nohighlight">\(\mathbf{x}_n^\top\)</span>) and <span class="math notranslate nohighlight">\(\mathbf{y} = (y_1, \ldots, y_N) \in \{0,1\}^N\)</span> denote the vector of output labels. The <strong>negative</strong> log likelihood is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathcal{L}(\mathbf{w}) &amp;= -\log p(\mathbf{y} \mid \mathbf{w}, \mathbf{X}) \\
&amp;= -\sum_{n=1}^N \log p(y_n \mid \mathbf{w}, \mathbf{x}_n) \\
&amp;= -\sum_{n=1}^N \log \mathrm{Bern}(y_n; \sigma(\mathbf{w}^\top \mathbf{x}_n)) \\
&amp;= -\sum_{n=1}^N \left(y_n \mathbf{w}^\top \mathbf{x}_n - \log(1 + e^{\mathbf{w}^\top \mathbf{x}_n}) \right)
\end{align*}
\end{split}\]</div>
</section>
<section id="the-gradient-with-respect-to-the-weights">
<h3>The gradient with respect to the weights<a class="headerlink" href="#the-gradient-with-respect-to-the-weights" title="Permalink to this headline">#</a></h3>
<p>The gradient with respect to <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla \mathcal{L}(\mathbf{w}) 
&amp;= -\sum_{n=1}^N \left( y_n \mathbf{x}_n - \frac{e^{\mathbf{w}^\top \mathbf{x}_n}}{1 + e^{\mathbf{w}^\top \mathbf{x}_n}} \mathbf{x}_n\right) \\
&amp;= -\sum_{n=1}^N \big(y_n - \sigma(\mathbf{w}^\top \mathbf{x}_n)\big) \, \mathbf{x}_n
\end{align*}
\end{split}\]</div>
<p>Unfortunately, this is a nonlinear function of <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> (due to the logistic function), and when we set to zero and try to solve for the weights, we find there is no closed-form solution.</p>
</section>
<section id="the-negative-log-likelihood-is-convex">
<h3>The negative log likelihood is convex<a class="headerlink" href="#the-negative-log-likelihood-is-convex" title="Permalink to this headline">#</a></h3>
<p>While there may not be a closed-form solution, the problem is not necessarily all that hard to solve. It turns out the negative log likelihood is a <strong>convex function</strong> of the weights — i.e., it looks like an upward facing bowl — so we can solve it with off-the-shelf optimization tools.</p>
<p>To show that the objective function is convex, it suffices to show that it is twice-differentiable and its <a class="reference external" href="https://en.wikipedia.org/wiki/Hessian_matrix"><strong>Hessian</strong></a> (the matrix of second-order partial derivatives) is positive semi-definite (has eigenvalues <span class="math notranslate nohighlight">\(\geq 0\)</span>).</p>
<p>The Hessian of the negative log likelihood is,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\nabla^2 \mathcal{L}(\mathbf{w})
&amp;= \sum_{n=1}^N \sigma'(\mathbf{w}^\top \mathbf{x}_n) \mathbf{x}_n \mathbf{x}_n^\top 
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma'(\mathbf{w}^\top \mathbf{x}_n)\)</span> is the derivative of the logistic function (see above) evaluated at <span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x}_n\)</span>.</p>
<p>Since this a sum of outer products (<span class="math notranslate nohighlight">\(\mathbf{x}_n \mathbf{x}_n^\top\)</span>) with positive coefficients (<span class="math notranslate nohighlight">\(\sigma(\mathbf{w}^\top \mathbf{x}_n)\)</span>), the Hessian is positive semi-definite.</p>
<div class="tip admonition">
<p class="admonition-title">Matrix derivatives</p>
<p>It takes some practice to become familiar with the rules of matrix calculus. I recommend the first chapters of <a class="reference external" href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a> <span id="id7">[<a class="reference internal" href="99_references.html#id42" title="Kaare Brandt Petersen, Michael Syskind Pedersen, and others. The matrix cookbook. Technical University of Denmark, 7(15):510, 2008. URL: https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf.">Petersen <em>et al.</em>, 2008</a>]</span> for an introduction.</p>
</div>
</section>
<section id="gradient-descent">
<h3>Gradient descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">#</a></h3>
<p>Since the the negative log likelihood is convex (equivalently, the log likelihood is concave), we have a host of tools at our disposal for maximum likelihood estimation. We don’t need CVXPy to solve this problem (like we did for the previous chapter). Here, we can simply perform <strong>gradient descent</strong>.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{w}_0\)</span> denote our initial setting of the weights. Gradient descent is an iterative algorithm that produces a sequence of weights <span class="math notranslate nohighlight">\(\mathbf{w}_0, \mathbf{w}_1, \ldots\)</span> that (under certain conditions) converges to a local optimum of the objective. Since the objective is convex, all local optima are global optima. The idea is straightforward, on each iteration we update the weights by taking a step in the direction of the gradient,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\mathbf{w}_{i+1} &amp;= \mathbf{w}_i - \alpha_i \nabla \mathcal{L}(\mathbf{w}_i)
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_i \in \mathbb{R}_+\)</span> is the <strong>learning rate</strong> (aka step size) on iteration <span class="math notranslate nohighlight">\(i\)</span>, and <span class="math notranslate nohighlight">\(\nabla \mathcal{L}(\mathbf{w}_i)\)</span> is the gradient of the objective evaluated at the current weights <span class="math notranslate nohighlight">\(\mathbf{w}_i\)</span>.</p>
<div class="dropdown admonition">
<p class="admonition-title">Convergence of gradient descent</p>
<p>When <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is convex and <span class="math notranslate nohighlight">\(\nabla \mathcal{L}\)</span> is <a class="reference external" href="https://en.wikipedia.org/wiki/Lipschitz_continuity"><strong>Lipschitz continuous</strong></a>, and when the learning rates satisfy certain conditions (e.g., they are set by a <a class="reference external" href="https://en.wikipedia.org/wiki/Backtracking_line_search"><strong>backtracking line search</strong></a>), then gradient descent will converge to a local optimum. When the objective is convex, all local optima are global optima, so then gradient descent is guaranteed to find a global solution.</p>
</div>
</section>
<section id="newton-s-method">
<h3>Newton’s method<a class="headerlink" href="#newton-s-method" title="Permalink to this headline">#</a></h3>
<p>Gradient descent uses first-order information (i.e., the gradient of the objective at the current weights) to determine the descent direction. We can obtain faster convergence rates using <strong>second-order</strong> information (i.e., the Hessian of the objective).</p>
<p>The idea is to minimize a quadratic approximation of the objective given by a Taylor approximation around the current weights,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\mathcal{L}(\mathbf{w}) 
&amp;\approx \mathcal{L}(\mathbf{w}_i) + (\mathbf{w} - \mathbf{w}_i)^\top \nabla \mathcal{L}(\mathbf{w}_i) 
+ \frac{1}{2} (\mathbf{w} - \mathbf{w}_i)^\top \nabla^2 \mathcal{L}(\mathbf{w}_i) (\mathbf{w} - \mathbf{w}_i).
\end{align*}
\]</div>
<p>The minimum of this quadratic approximation has a closed form solution,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}_{i+1} = \mathbf{w}_i + \nabla^2 \mathcal{L}(\mathbf{w}_i)^{-1} \nabla \mathcal{L}(\mathbf{w}_i).
\]</div>
<p>Here, the descent direction is given by the inverse-Hessian times the gradient, <span class="math notranslate nohighlight">\(\nabla^2 \mathcal{L}(\mathbf{w}_i)^{-1} \nabla \mathcal{L}(\mathbf{w}_i)\)</span>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note that Newton’s method assumes that the Hessian is invertible, which is almost surely the case for logistic regression with many data points. We can ensure invertibility by adding a multivariate normal prior on the weights, as we will introduce below.</p>
</div>
<p>Newton’s method can be unstable in practice. A simple fix is to use the same descent direction, but that to vary the step size <span class="math notranslate nohighlight">\(\alpha_i\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}_{i+1} = \mathbf{w}_i + \alpha_i \nabla^2 \mathcal{L}(\mathbf{w}_i)^{-1} \nabla \mathcal{L}(\mathbf{w}_i).
\]</div>
<p>For example, the step-size can be set to <span class="math notranslate nohighlight">\(\alpha_i &lt; 1\)</span> to implement <em>damped</em> Newton’s method, or it can be determined by a backtracking line search.</p>
</section>
<section id="iteratively-reweighted-least-squares-irls">
<h3>Iteratively reweighted least squares (IRLS)<a class="headerlink" href="#iteratively-reweighted-least-squares-irls" title="Permalink to this headline">#</a></h3>
<p>The weight updates simplify nicely when we substitute in the form of the gradient and Hessian for logistic regression. Note that they can be written in matrix form as,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla \mathcal{L}(\mathbf{w}) &amp;= -\mathbf{X}^\top \big(\mathbf{y} - \sigma(\mathbf{X} \mathbf{w}) \big) \\
\nabla^2 \mathcal{L}(\mathbf{w}) &amp;= \mathbf{X}^\top \mathbf{S} \mathbf{X}
\end{align*}
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\mathbf{S} = \mathrm{diag}\left([\sigma'(\mathbf{w}^\top \mathbf{x}_1), \ldots, \sigma'(\mathbf{w}^\top \mathbf{x}_N)] \right)
\]</div>
<p>is a diagonal <strong>scaling (aka weighting) matrix</strong>. Note that the scale factors are all positive since <span class="math notranslate nohighlight">\(\sigma'(a) &gt; 0\)</span>.</p>
<p>Substituting these forms in and rearranging yield,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbf{w}_{i+1} 
&amp;= \mathbf{w}_i + \big(\mathbf{X}^\top \mathbf{S} \mathbf{X}\big)^{-1} \mathbf{X}^\top \big( \mathbf{y} - \sigma(\mathbf{X} \mathbf{w}) \big) \\
&amp;= \big(\mathbf{X}^\top \mathbf{S} \mathbf{X}\big)^{-1} \mathbf{X}^\top \mathbf{S} \tilde{\mathbf{y}},
\end{align*} 
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\tilde{\mathbf{y}} = \mathbf{X} \mathbf{w}_i + \mathbf{S}^{-1} \big(\mathbf{y} - \sigma(\mathbf{X} \mathbf{w}_i) \big).
\]</div>
<p>In other words, the standard Newton method update can be viewed as the solution to a <a class="reference external" href="https://en.wikipedia.org/wiki/Weighted_least_squares"><strong>weighted least squares</strong></a> problem with weights <span class="math notranslate nohighlight">\(\mathbf{S}\)</span> and targets <span class="math notranslate nohighlight">\(\tilde{\mathbf{y}}\)</span> that depend on the current weights <span class="math notranslate nohighlight">\(\mathbf{w}_i\)</span>. Viewed this way, we see that Newton’s method for logistic regression is equivalent to an algorithm called <a class="reference external" href="https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares"><strong>iteratively reweighted least squares (IRLS)</strong></a>.</p>
</section>
<section id="computational-complexity">
<h3>Computational complexity<a class="headerlink" href="#computational-complexity" title="Permalink to this headline">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Show that the computational complexity of gradient descent is <span class="math notranslate nohighlight">\(\mathcal{O}(NP)\)</span> whereas the complexity of Newton’s method is <span class="math notranslate nohighlight">\(\mathcal{O}(NP^2 + P^3)\)</span>.</p>
</div>
</section>
<section id="scaling-up">
<h3>Scaling up<a class="headerlink" href="#scaling-up" title="Permalink to this headline">#</a></h3>
<p>Though it converges faster, Newton’s method quickly becomes intractable for large <span class="math notranslate nohighlight">\(N\)</span> and <span class="math notranslate nohighlight">\(P\)</span>. For large <span class="math notranslate nohighlight">\(N\)</span>, even gradient descent becomes costly. There are a few ways to speed up computation:</p>
<ul class="simple">
<li><p><strong>Quasi-Newton methods</strong> like <a class="reference external" href="https://en.wikipedia.org/wiki/BFGS_method"><strong>BFGS</strong></a> replace the exact Hessian with an approximation and side-step the explicit matrix inversion.</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent"><strong>Stochastic gradient descent (SGD)</strong></a> uses subsets of data (a.k.a. <em>minibatches</em>) to approximate the gradient. Under fairly general conditions, it converges to a local optimum.</p></li>
<li><p><strong>Momentum</strong> is often used in conjunction with SGD to keep the descent direction from changing too rapidly. This can address some of the limitations of regular gradient descent as well, e.g., where the updates overshoot in poorly conditioned problems. Related methods like Nesterov’s accelerated gradient (see <span id="id8">Sutskever <em>et al.</em> [<a class="reference internal" href="99_references.html#id43" title="Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, 1139–1147. PMLR, 2013.">2013</a>]</span>) can achieve second-order convergence rates using first-order information (under certain conditions).</p></li>
<li><p>Still, SGD (with momentum) requires <strong>setting a learning rate</strong>. Modern machine learning packages like <a class="reference external" href="https://pytorch.org/docs/stable/optim.html"><code class="docutils literal notranslate"><span class="pre">torch.optim</span></code></a> implement a number of optimizers that automatically tune the learning rates, like <strong>AdaGrad</strong> <span id="id9">[<a class="reference internal" href="99_references.html#id44" title="John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 2011.">Duchi <em>et al.</em>, 2011</a>]</span>, <strong>RMSProp</strong> <span id="id10">[<a class="reference internal" href="99_references.html#id46" title="Geoffrey Hinton, Nitish Srivasta, and Kevin Swerskey. Neural networks for machine learning lecture 6a. 2014. URL: https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf.">Hinton <em>et al.</em>, 2014</a>]</span>, and <strong>Adam</strong> <span id="id11">[<a class="reference internal" href="99_references.html#id45" title="Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.">Kingma and Ba, 2014</a>]</span>.</p></li>
</ul>
</section>
</section>
<section id="pose-tracking-with-convolutional-neural-networks-cnns">
<h2>Pose tracking with convolutional neural networks (CNNs)<a class="headerlink" href="#pose-tracking-with-convolutional-neural-networks-cnns" title="Permalink to this headline">#</a></h2>
<p>Remember we started by treating each data point as patch <span class="math notranslate nohighlight">\(\mathbf{x}_n\)</span> and a binary label <span class="math notranslate nohighlight">\(y_n\)</span> specifying whether a specific key point (e.g. “left paw”) is present. Of course, in practice we want to classify all the patches in an image in parallel, and we want to predict more than one type of key point.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{X}_n \in \mathbb{R}^{P_H \times P_W}\)</span> denote the <span class="math notranslate nohighlight">\(n\)</span>-th image and <span class="math notranslate nohighlight">\(\mathbf{Y}_{n,k} \in \{0,1\}^{P_H \times P_W}\)</span> denote the binary mask of where in the <span class="math notranslate nohighlight">\(k\)</span>-th key point is in the <span class="math notranslate nohighlight">\(n\)</span>-th image. Both are <span class="math notranslate nohighlight">\(P_H\)</span> pixels in height and <span class="math notranslate nohighlight">\(P_W\)</span> pixels wide. Assume the patches are <span class="math notranslate nohighlight">\(P_h \times P_w\)</span> in size, with <span class="math notranslate nohighlight">\(P_h &lt; P_H\)</span> and <span class="math notranslate nohighlight">\(P_w &lt; P_W\)</span>.  Finally, ket <span class="math notranslate nohighlight">\(\mathbf{W}_k \in \mathbb{R}^{P_h \times P_w}\)</span> denote the weights for the <span class="math notranslate nohighlight">\(k\)</span>-th key point.</p>
<p>We can think of each image as having <span class="math notranslate nohighlight">\(P_H \cdot P_W\)</span> patches and corresponding labels, one centered on each pixel. In our simple logistic regression model, each patch’s label is modeled as a conditionally independent Bernoulli random variable,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
p(\mathbf{Y} \mid \mathbf{X}, \mathbf{W}) 
&amp;= \prod_{n=1}^N \prod_{k=1}^K \prod_{i=1}^{P_H} \prod_{j=1}^{P_W} \mathrm{Bern}(y_{n,k,i,j}; \sigma(a_{n,k,i,j}))
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(a_{n,k,i,j} \in \mathbb{R}\)</span> is the <strong>activation</strong> for that specific image, key point, and pixel. The activations are given by a <strong>2D cross-correlation</strong>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
a_{n,k,i,j} 
&amp;= \sum_{d=1}^{P_h} \sum_{d'=1}^{P_w} w_{k,d,d'} x_{n,i+d-\frac{P_h}{2},j+d'-\frac{P_w}{2}} \\
&amp;= [\mathbf{X_n} \star \mathbf{W}_k]_{i,j}.
\end{align*}
\end{split}\]</div>
<p>In fact, the activations for an entire batch of <span class="math notranslate nohighlight">\(N\)</span> images and <span class="math notranslate nohighlight">\(K\)</span> key points (i.e., output channels) can be computed in a single call to <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html"><code class="docutils literal notranslate"><span class="pre">F.conv2d</span></code></a>, with the appropriate padding. In lab, we’ll make use of the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code></a> class, which encapsulates the weights of a 2D convolution layer and makes it easy to train such models.</p>
<section id="convolutional-neural-networks">
<h3>Convolutional neural networks<a class="headerlink" href="#convolutional-neural-networks" title="Permalink to this headline">#</a></h3>
<p>Framed this way, we can view the logistic regression model as a <strong>one-layer convolutional neural network (CNN)</strong>. This view also suggests an obvious direction for improvement. The activations of the logistic regression model are <strong>linear functions</strong> of the pixels. In practice, a good key point detector may need <strong>nonlinear features</strong> of the images. Moreover, the features necessary to predict one keypoint (e.g., the left paw) may be similar to those needed for another (e.g., the right).</p>
<p>Convolutional neural networks allow both nonlinear feature learning and feature sharing between outputs. The idea is straightforward: stack multiple convolutional layers on top of each other, feeding the output of one as the input to the next.</p>
<p><strong>Residual networks (ResNets)</strong> enable very deep CNNs to be stably trained by adding <strong>skip connections</strong> whereby the input is fed straight to the output of a layer, thereby allowing the convolution to capture the difference (i.e., residual) between the input and output.</p>
<p>These notes will not comprehensively cover CNNs and ResNets. Instead, please consult the many great online resources, like <span id="id12">Goodfellow <em>et al.</em> [<a class="reference internal" href="99_references.html#id47" title="Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.">2016</a>]</span> and the PyTorch <a class="reference external" href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html">tutorials</a>.</p>
</section>
<section id="working-with-minimal-labeled-data">
<h3>Working with minimal labeled data.<a class="headerlink" href="#working-with-minimal-labeled-data" title="Permalink to this headline">#</a></h3>
<p>While deep neural networks like CNNs and ResNets have the capacity to learn nonlinear mappings from images to key point predictions, they also have many free parameters to train.</p>
<p>For modern machine learning tasks and benchmarks, these models are trained on millions of labeled images, like the ImageNet dataset for image classification.</p>
<p>Unfortunately, we don’t (yet) have comparable datasets of labeled images of animals in the lab (however, see <span id="id13">Marshall <em>et al.</em> [<a class="reference internal" href="99_references.html#id48" title="Jesse D Marshall, Diego E Aldarondo, Timothy W Dunn, William L Wang, Gordon J Berman, and Bence P Ölveczky. Continuous whole-body 3d kinematic recordings across the rodent behavioral repertoire. Neuron, 109(3):420–437, 2021.">2021</a>]</span>) Moreover, environments change from one lab to the next, making it challenging to collect one dataset that covers all the bases.</p>
<p>Ideally, we would like to train a pose tracking model for a specific lab environment using minimal labeled data.</p>
<p>Modern methods for markerless animal pose tracking take two approaches:</p>
<ol class="simple">
<li><p>Use a <strong>lightweight architecture</strong> with fewer free parameters, like the modular UNet in SLEAP <span id="id14">[<a class="reference internal" href="99_references.html#id35" title="Talmo D Pereira, Nathaniel Tabris, Arie Matsliah, David M Turner, Junyu Li, Shruthi Ravindranath, Eleni S Papadoyannis, Edna Normand, David S Deutsch, Z Yan Wang, and others. SLEAP: A deep learning system for multi-animal pose tracking. Nature methods, 19(4):486–495, 2022.">Pereira <em>et al.</em>, 2022</a>]</span></p></li>
<li><p>Use <strong>transfer learning</strong> to adapt the weights of a pre-trained network to the problem of animal pose tracking <span id="id15">[<a class="reference internal" href="99_references.html#id34" title="Alexander Mathis, Pranav Mamidanna, Kevin M Cury, Taiga Abe, Venkatesh N Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. DeepLabCut: markerless pose estimation of user-defined body parts with deep learning. Nat. Neurosci., 21(9):1281–1289, August 2018.">Mathis <em>et al.</em>, 2018</a>]</span>.</p></li>
</ol>
</section>
<section id="transfer-learning">
<h3>Transfer learning<a class="headerlink" href="#transfer-learning" title="Permalink to this headline">#</a></h3>
<p><strong>Transfer learning</strong> is when you take a model trained for one task (e.g. a ResNet trained for image classification with ImageNet) and adapt it to another task (e.g. classifying key points in behavioral video). If the tasks are similar enough, then it may require new minimal training data to adapt the pre-trained network.</p>
<p>For example, DeepLabCut <span id="id16">[<a class="reference internal" href="99_references.html#id34" title="Alexander Mathis, Pranav Mamidanna, Kevin M Cury, Taiga Abe, Venkatesh N Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. DeepLabCut: markerless pose estimation of user-defined body parts with deep learning. Nat. Neurosci., 21(9):1281–1289, August 2018.">Mathis <em>et al.</em>, 2018</a>]</span> originally converted a ResNet-50 (a very deep CNN) trained on ImageNet by rerouting the output of a middle layer to make predictions about key point locations in animal videos.
We can think of the activations at the middle layer as a highly nonlinear <strong>feature bank</strong> that represents the original image in a form that may be more amenable to key point prediction with simple models.</p>
<p>Finally, using a small number (100s) of hand-labeled video frames, DLC can train the mapping from intermediate features to key point predictions. It can also <strong>fine-tune</strong> the weights of the ResNet to obtain very good accuracy in key point prediction.</p>
<p>There are, of course, many details and design questions to answer. Which layer of the ResNet do you reroute? How complex is the mapping from features to predictions? How big should you make the prediction targets? We will work through these concerns in Lab 3.</p>
</section>
</section>
<section id="structured-prediction">
<h2>Structured prediction<a class="headerlink" href="#structured-prediction" title="Permalink to this headline">#</a></h2>
<p>The output of these CNNs is a map of key point probabilities for each image, key point, and pixel. However, the key points are obviously not independent — knowing where the left elbow is tells you a lot about where the left shoulder is likely to be found. Ideally, we would like to make a unified prediction about the collection of key point locations.</p>
<p>One way to tackle this problem is with <strong>structured prediction</strong> <span id="id17">[<a class="reference internal" href="99_references.html#id49" title="Pedro F Felzenszwalb and Daniel P Huttenlocher. Pictorial structures for object recognition. International journal of computer vision, 61:55–79, 2005.">Felzenszwalb and Huttenlocher, 2005</a>, <a class="reference internal" href="99_references.html#id50" title="Eldar Insafutdinov, Leonid Pishchulin, Bjoern Andres, Mykhaylo Andriluka, and Bernt Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14, 34–50. Springer, 2016.">Insafutdinov <em>et al.</em>, 2016</a>]</span>. Let <span class="math notranslate nohighlight">\(\ell_{n,k} \in [0,P_h] \times [0,P_w]\)</span> be a random variable denoting the location of key point <span class="math notranslate nohighlight">\(k\)</span> in image <span class="math notranslate nohighlight">\(n\)</span> (in pixel coordinates). We can represent a joint distribution over the collection of key points <span class="math notranslate nohighlight">\(\boldsymbol{\ell}_n = (\ell_{n,1}, \ldots, \ell_{n,K})\)</span> as a <strong>factor graph</strong>,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
p(\boldsymbol{\ell}_n) &amp;\propto 
\prod_{k=1}^K e^{f_k(\ell_{n,k}; \mathbf{x}_n)} 
\prod_{i=1}^K \prod_{j=1}^K e^{g_{ij}(\ell_{n,i}, \ell_{n,j})},
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(f_k\)</span> are called <strong>unary potentials</strong> and <span class="math notranslate nohighlight">\(g_{ij}\)</span> are called <strong>pairwise potentials</strong>. The unary potentials are functions that specify how likely key point <span class="math notranslate nohighlight">\(k\)</span> is to be found at a location <span class="math notranslate nohighlight">\(\ell_{n,k}\)</span> given the observed image <span class="math notranslate nohighlight">\(\mathbf{x}_n\)</span>; the pairwise potentials specify how likely the joint configuration of <span class="math notranslate nohighlight">\((\ell_{n,i}, \ell_{n,j})\)</span> is. For example, the pairwise potentials could rule out configurations in which the left elbow is meters away from the left shoulder.</p>
<p>Note that this joint distribution is <strong>unnomrmalized</strong>. That isn’t a problem for maximuim a posteriori (MAP) estimation of the key point locations, but it does make learning the parameters of the potential functions a bit harder. We’ll come back to this later in the course.</p>
<section id="d-triangulation">
<h3>3D Triangulation<a class="headerlink" href="#d-triangulation" title="Permalink to this headline">#</a></h3>
<p>Finally, we have so far focused on finding key points in 2D images, but there is growing interest in tracking 3D pose from multiple camera views.</p>
<p>One way to solve this problem is by training a CNN that synthesizes multiple 2D views into a 3D representation <span id="id18">[<a class="reference internal" href="99_references.html#id51" title="Timothy W Dunn, Jesse D Marshall, Kyle S Severson, Diego E Aldarondo, David GC Hildebrand, Selmaan N Chettih, William L Wang, Amanda J Gellis, David E Carlson, Dmitriy Aronov, and others. Geometric deep learning enables 3d kinematic profiling across species and environments. Nature methods, 18(5):564–573, 2021.">Dunn <em>et al.</em>, 2021</a>]</span>. Another is to use <strong>triangulation</strong> to infer a 3D key point location from multiple 2D views <span id="id19">[<a class="reference internal" href="99_references.html#id52" title="Pierre Karashchuk, Katie L Rupp, Evyn S Dickinson, Sarah Walling-Bell, Elischa Sanders, Eiman Azim, Bingni W Brunton, and John C Tuthill. Anipose: a toolkit for robust markerless 3d pose estimation. Cell reports, 36(13):109730, 2021.">Karashchuk <em>et al.</em>, 2021</a>, <a class="reference internal" href="99_references.html#id53" title="Tanmay Nath, Alexander Mathis, An Chi Chen, Amir Patel, Matthias Bethge, and Mackenzie Weygandt Mathis. Using deeplabcut for 3d markerless pose estimation across species and behaviors. Nature protocols, 14(7):2152–2176, 2019.">Nath <em>et al.</em>, 2019</a>, <a class="reference internal" href="99_references.html#id54" title="Libby Zhang, Tim Dunn, Jesse Marshall, Bence Olveczky, and Scott Linderman. Animal pose estimation from video data with a hierarchical von Mises-Fisher-Gaussian model. In International Conference on Artificial Intelligence and Statistics, 2800–2808. PMLR, 2021.">Zhang <em>et al.</em>, 2021</a>]</span>.</p>
<figure class="align-default" id="triangulation">
<img alt="../_images/triangulation.png" src="../_images/triangulation.png" />
<figcaption>
<p><span class="caption-number">Fig. 9 </span><span class="caption-text">Projective geometry makes far away objects appear smaller. Image modified from <a class="reference external" href="https://en.wikipedia.org/wiki/Triangulation_(computer_vision)">Wikipedia</a>.</span><a class="headerlink" href="#triangulation" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>GIMBAL <span id="id20">[<a class="reference internal" href="99_references.html#id54" title="Libby Zhang, Tim Dunn, Jesse Marshall, Bence Olveczky, and Scott Linderman. Animal pose estimation from video data with a hierarchical von Mises-Fisher-Gaussian model. In International Conference on Artificial Intelligence and Statistics, 2800–2808. PMLR, 2021.">Zhang <em>et al.</em>, 2021</a>]</span> combines structured prediction and triangulation to infer 3D pose from multiple 2D key point estimates. Changing notation slightly, let <span class="math notranslate nohighlight">\(\mathbf{y}_{c,k} \in \mathbb{R}^2\)</span> denote the estimated location of key point <span class="math notranslate nohighlight">\(k\)</span> in the coordinates of camera <span class="math notranslate nohighlight">\(c\)</span>, and let <span class="math notranslate nohighlight">\(\mathbf{x}_k \in \mathbb{R}^3\)</span> denote the 3D location in “world coordinates.” <a class="reference external" href="https://en.wikipedia.org/wiki/Projective_geometry"><strong>Projective geometry</strong></a> tells how to map 3D locations to 2D images,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{y}_{c,k} \approx \frac{1}{w} \begin{bmatrix} u \\ v \end{bmatrix} 
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} u \\ v \\ w \end{bmatrix} = \mathbf{A}_c \mathbf{x}_k + \mathbf{b}_c
\end{split}\]</div>
<p>are the 3D coordinates after affine transformation (parameterized by <span class="math notranslate nohighlight">\(\mathbf{A}_c\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}_c\)</span>) into the frame of camera <span class="math notranslate nohighlight">\(c\)</span>. Note that the transformation is <strong>nonlinear</strong> due to the scaling by <span class="math notranslate nohighlight">\(w^{-1}\)</span>.</p>
<p>GIMBAL combines this nonlinear camera model with a structured prior distribution on 3D key point locations <span class="math notranslate nohighlight">\(\mathbf{x} = (\mathbf{x}_1, \ldots, \mathbf{x}_K)\)</span> obtained from a hierarchical von Mises-Fisher-Gaussian distribution.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">#</a></h2>
<p>This chapter introduced the basics of modern animal pose tracking methods using supervised learning, deep convolutional neural networks, transfer learning, and structured prediction. Many of these techniques will reappear later in the course as we talk about encoding and decoding neural spike trains. In lab, you will practice implementing these techniques and building a simple pose tracker.</p>
<section id="further-reading">
<h3>Further reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">#</a></h3>
<p>As always, there is much more to say on each of the topics covered in this chapter. There is growing work on <strong>semi-supervised</strong> pose tracking to leverage the wealth of unlabeled image frames <span id="id21">[<a class="reference internal" href="99_references.html#id56" title="Matthew R Whiteway, Dan Biderman, Yoni Friedman, Mario Dipoppa, E Kelly Buchanan, Anqi Wu, John Zhou, Niccolò Bonacchi, Nathaniel J Miska, Jean-Paul Noel, and others. Partitioning variability in animal behavioral videos using semi-supervised variational autoencoders. PLoS computational biology, 17(9):e1009439, 2021.">Whiteway <em>et al.</em>, 2021</a>, <a class="reference internal" href="99_references.html#id55" title="Anqi Wu, Estefany Kelly Buchanan, Matthew Whiteway, Michael Schartner, Guido Meijer, Jean-Paul Noel, Erica Rodriguez, Claire Everett, Amy Norovich, Evan Schaffer, and others. Deep Graph Pose: a semi-supervised deep graphical model for improved animal pose tracking. Advances in Neural Information Processing Systems, 33:6040–6052, 2020.">Wu <em>et al.</em>, 2020</a>]</span>. There is still work to be done on multi-animal pose tracking <span id="id22">[<a class="reference internal" href="99_references.html#id57" title="Jessy Lauer, Mu Zhou, Shaokai Ye, William Menegas, Steffen Schneider, Tanmay Nath, Mohammed Mostafizur Rahman, Valentina Di Santo, Daniel Soberanes, Guoping Feng, and others. Multi-animal pose estimation, identification and tracking with deeplabcut. Nature Methods, 19(4):496–504, 2022.">Lauer <em>et al.</em>, 2022</a>, <a class="reference internal" href="99_references.html#id35" title="Talmo D Pereira, Nathaniel Tabris, Arie Matsliah, David M Turner, Junyu Li, Shruthi Ravindranath, Eleni S Papadoyannis, Edna Normand, David S Deutsch, Z Yan Wang, and others. SLEAP: A deep learning system for multi-animal pose tracking. Nature methods, 19(4):486–495, 2022.">Pereira <em>et al.</em>, 2022</a>]</span>, and recent machine learning challenges like <a class="reference external" href="https://sites.google.com/view/mabe22/home">MABe</a> have sought to make progress on these problems. Finally, experimentalists are starting to use these kinds of techniques for <strong>real-time</strong> pose tracking to guide closed-loop optogenetic stimulation of neural circuits <span id="id23">[<a class="reference internal" href="99_references.html#id58" title="Jeffrey E Markowitz, Winthrop F Gillis, Maya Jay, Jeffrey Wood, Ryley W Harris, Robert Cieszkowski, Rebecca Scott, David Brann, Dorothy Koveal, Tomasz Kula, and others. Spontaneous behaviour is structured by reinforcement without explicit reward. Nature, pages 1–10, 2023.">Markowitz <em>et al.</em>, 2023</a>]</span>. While there is still room for methodological improvement, these methods have already transformed the landscape of systems neuroscience.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="06_calcium_imaging.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Demixing Calcium Imaging Data</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="08_summary_stats.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Summary Statistics</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Scott Linderman<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>